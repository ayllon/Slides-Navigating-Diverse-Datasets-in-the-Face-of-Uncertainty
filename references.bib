
@article{abedjan2015,
	title = {Profiling relational data: a survey},
	volume = {24},
	doi = {10.1007/s00778-015-0389-y},
	number = {4},
	journal = {The VLDB Journal},
	author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
	year = {2015},
	note = {publisher: Springer},
	keywords = {Basics, Interesting, Survey},
	pages = {557--581},
}

@book{ENISA2015,
	title = {Privacy by design in big data : an overview of privacy enhancing technologies in the era of big data analytics},
	publisher = {European Network and Information Security Agency},
	author = {Cybersecurity, European Union Agency for and Montjoye, Y and Bourka, A and D' Acquisto, G and Domingo-Ferrer, J and Kikiras, P and Torra, V},
	year = {2016},
	doi = {doi/10.2824/641480},
}

@techreport{dacquisto_privacy_2015,
	title = {Privacy by design in big data},
	url = {https://doi.org/10.2824/641480},
	abstract = {The extensive collection and processing of personal information in big data analytics has given rise to serious privacy concerns, related to wide scale electronic surveillance, profiling, and disclosure of private data. To reap the benefits of analytics without invading the individuals' private sphere, it is essential to draw the limits of big data processing and integrate data protection safeguards in the analytics value chain. ENISA, with the current report, supports this approach and the position that the challenges of technology (for big data) should be addressed by the opportunities of technology (for privacy).
We first explain the need to shift from "big data versus privacy" to "big data with privacy". In this respect, the concept of privacy by design is key to identify the privacy requirements early in the big data analytics value chain and in subsequently implementing the necessary technical and organizational measures.
After an analysis of the proposed privacy by design strategies in the different phases of the big data value chain, we review privacy enhancing technologies of special interest for the current and future big data landscape. In particular, we discuss anonymization, the "traditional" analytics technique, the emerging area of encrypted search and privacy preserving computations, granular access control mechanisms, policy enforcement and accountability, as well as data provenance issues. Moreover, new transparency and access tools in big data are explored, together with techniques for user empowerment and control.
Achieving "big data with privacy" is no easy task and a lot of research and implementation is still needed. Yet, it remains a possible task, as long as all the involved stakeholders take the necessary steps to integrate privacy and data protection safeguards in the heart of big data, by design and by default.},
	institution = {European Union Agency for Network and Information Security},
	author = {D'Acquisto, Giuseppe and Domingo-Ferrer, Josep and Kikiras, Panayiotis and Torra, Vicenç and de Montjoye, Yves-Alexandre and Bourka, Athena},
	year = {2015},
	doi = {10.2824/641480},
}

@article{papenbrock2015data,
	title = {Data profiling with metanome},
	volume = {8},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Papenbrock, Thorsten and Bergmann, Tanja and Finke, Moritz and Zwiener, Jakob and Naumann, Felix},
	year = {2015},
	note = {Publisher: VLDB Endowment},
	pages = {1860--1863},
}

@article{Pepe2014,
	title = {How do astronomers share data? {Reliability} and persistence of datasets linked in {AAS} publications and a qualitative study of data practices among {US} astronomers},
	volume = {9},
	url = {https://doi.org/10.1371/journal.pone.0104798},
	doi = {10.1371/journal.pone.0104798},
	abstract = {We analyze data sharing practices of astronomers over the past fifteen years. An analysis of URL links embedded in papers published by the American Astronomical Society reveals that the total number of links included in the literature rose dramatically from 1997 until 2005, when it leveled off at around 1500 per year. The analysis also shows that the availability of linked material decays with time: in 2011, 44\% of links published a decade earlier, in 2001, were broken. A rough analysis of link types reveals that links to data hosted on astronomers' personal websites become unreachable much faster than links to datasets on curated institutional sites. To gauge astronomers' current data sharing practices and preferences further, we performed in-depth interviews with 12 scientists and online surveys with 173 scientists, all at a large astrophysical research institute in the United States: the Harvard-Smithsonian Center for Astrophysics, in Cambridge, MA. Both the in-depth interviews and the online survey indicate that, in principle, there is no philosophical objection to data-sharing among astronomers at this institution. Key reasons that more data are not presently shared more efficiently in astronomy include: the difficulty of sharing large data sets; over reliance on non-robust, non-reproducible mechanisms for sharing data (e.g. emailing it); unfamiliarity with options that make data-sharing easier (faster) and/or more robust; and, lastly, a sense that other researchers would not want the data to be shared. We conclude with a short discussion of a new effort to implement an easy-to-use, robust, system for data sharing in astronomy, at theastrodata.org, and we analyze the uptake of that system to-date.},
	number = {8},
	journal = {PLOS ONE},
	author = {Pepe, Alberto and Goodman, Alyssa and Muench, August and Crosas, Merce and Erdmann, Christopher},
	month = aug,
	year = {2014},
	note = {Publisher: Public Library of Science},
	pages = {1--11},
}

@article{Ball2010,
	title = {Data {Mining} {And} {Machine} {Learning} {In} {Astronomy}},
	volume = {19},
	url = {https://doi.org/10.1142/S0218271810017160},
	doi = {10.1142/S0218271810017160},
	abstract = {We review the current state of data mining and machine learning in astronomy. Data Mining can have a somewhat mixed connotation from the point of view of a researcher in this field. If used correctly, it can be a powerful approach, holding the potential to fully exploit the exponentially increasing amount of available data, promising great scientific advance. However, if misused, it can be little more than the black box application of complex computing algorithms that may give little physical insight, and provide questionable results. Here, we give an overview of the entire data mining process, from data collection through to the interpretation of results. We cover common machine learning algorithms, such as artificial neural networks and support vector machines, applications from a broad range of astronomy, emphasizing those in which data mining techniques directly contributed to improving science, and important current and future directions, including probability density functions, parallel algorithms, Peta-Scale computing, and the time domain. We conclude that, so long as one carefully selects an appropriate algorithm and is guided by the astronomical problem at hand, data mining can be very much the powerful tool, and not the questionable black box.},
	number = {07},
	journal = {International Journal of Modern Physics D},
	author = {Ball, Nicholas M. and Brunner, Robert J.},
	year = {2010},
	pages = {1049--1106},
}

@book{pearl1988,
	title = {Probabilistic reasoning in intelligent systems: {Networks} of plausible inference},
	publisher = {Morgan Kaufmann},
	author = {Pearl, Judea},
	year = {1988},
}

@article{schreiber_pomegranate_2017,
	title = {Pomegranate:  {Fast} and {Flexible} {Probabilistic} {Modeling} in {Python}},
	volume = {18},
	url = {https://dl.acm.org/doi/abs/10.5555/3122009.3242021},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Schreiber, Jacob},
	year = {2017},
	pages = {5992--5997},
}

@article{meinshausen2006,
	title = {Quantile regression forests},
	volume = {7},
	url = {https://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf},
	abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditionalmean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables.
The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
	number = {Jun},
	journal = {Journal of Machine Learning Research},
	author = {Meinshausen, Nicolai},
	year = {2006},
	pages = {983--999},
}

@inproceedings{mason2000,
	title = {Boosting algorithms as gradient descent},
	volume = {12},
	url = {https://proceedings.neurips.cc/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter L and Frean, Marcus R},
	year = {1999},
}

@article{Lucas2015,
	title = {Modeling the interactions between discrete and continuous causal factors in {Bayesian} networks},
	volume = {30},
	doi = {10.1002/int.21698},
	number = {3},
	journal = {International Journal of Intelligent Systems},
	author = {Lucas, Peter JF and Hommersom, Arjen},
	year = {2015},
	pages = {209--235},
}

@article{laigle2016cosmos2015,
	title = {The {COSMOS2015} catalog: {Exploring} the 1 {\textless} z {\textless} 6 universe with half a million galaxies},
	volume = {224},
	doi = {10.3847/0067-0049/224/2/24},
	number = {2},
	journal = {The Astrophysical Journal Supplement Series},
	author = {Laigle, Clotilde and McCracken, Henry J and Ilbert, Olivier and Hsieh, Bau-Ching and Davidzon, Iary and Capak, Peter and Hasinger, Günther and Silverman, John D and Pichon, Christophe and Coupon, Jean and {others}},
	year = {2016},
	pages = {24},
}

@article{kraska_case_2018,
	title = {The {Case} for {Learned} {Index} {Structures}},
	volume = {abs/1712.0},
	url = {http://arxiv.org/abs/1712.01208},
	doi = {10.1145/3183713.3196909},
	abstract = {Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70\% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible},
	journal = {Proceedings of the 2018 International Conference on Management of Data},
	author = {Kraska, Tim and Beutel, Alex and Chi, Ed H and Dean, Jeffrey and Polyzotis, Neoklis},
	year = {2018},
	keywords = {Interesting, ★},
	pages = {489--504},
}

@article{de2013kilo,
	title = {The {Kilo}-{Degree} {Survey}},
	volume = {35},
	doi = {10.1007/s10686-012-9306-1},
	number = {1-2},
	journal = {Experimental Astronomy},
	author = {de Jong, Jelte TA and Kleijn, Gijs A Verdoes and Kuijken, Konrad H and Valentijn, Edwin A and {others}},
	year = {2013},
	pages = {25--44},
}

@article{chen2017,
	title = {Learning discrete {Bayesian} networks from continuous data},
	volume = {59},
	doi = {10.1613/jair.5371},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chen, Yi-Chun and Wheeler, Tim A and Kochenderfer, Mykel J},
	year = {2017},
	pages = {103--132},
}

@article{ZELKOWITZ1997,
	title = {Experimental validation in software engineering},
	volume = {39},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584997000256},
	doi = {10.1016/S0950-5849(97)00025-6},
	number = {11},
	journal = {Information and Software Technology},
	author = {Zelkowitz, Marvin V and Wallace, Dolores},
	year = {1997},
	keywords = {Data collection, Evaluation, Experimentation, Measurement},
	pages = {735--743},
}

@article{zaremba2013b,
	title = {B-test: {A} {Non}-parametric, {Low} {Variance} {Kernel} {Two}-sample {Test}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zaremba, Wojciech and Gretton, Arthur and Blaschko, Matthew},
	year = {2013},
}

@article{Webster2002,
	title = {Analyzing the {Past} to {Prepare} for the {Future}: {Writing} a {Literature} {Review}.},
	volume = {26},
	url = {https://www.jstor.org/stable/4132319},
	abstract = {The authors present information on how to write a literature review in the field of information systems (IS), noting that in this field there are only a few published review articles. They note the factors to be considered when writing a review, such as the prospective author and topic of the review, the structure of the review, the identification of the relevant literaure, and the tone of review. The reviewing and revision process is also discussed.},
	number = {2},
	journal = {MIS Quarterly},
	author = {Webster, Jane and Watson, Richard T},
	year = {2002},
	keywords = {LITERATURE reviews, PEER review of academic writing, Writing},
	pages = {xiii -- xxiii},
}

@inproceedings{Wu2008,
	address = {New York, NY, USA},
	series = {{SIGMOD} '08},
	title = {Discovering topical structures of databases},
	isbn = {978-1-60558-102-6},
	url = {https://doi.org/10.1145/1376616.1376717},
	doi = {10.1145/1376616.1376717},
	abstract = {The increasing complexity of enterprise databases and the prevalent lack of documentation incur significant cost in both understanding and integrating the databases. Existing solutions addressed mining for keys and foreign keys, but paid little attention to more high-level structures of databases. In this paper, we consider the problem of discovering topical structures of databases to support semantic browsing and large-scale data integration. We describe iDisc, a novel discovery system based on a multi-strategy learning framework. iDisc exploits varied evidence in database schema and instance values to construct multiple kinds of database representations. It employs a set of base clusterers to discover preliminary topical clusters of tables from database representations, and then aggregate them into final clusters via meta-clustering. To further improve the accuracy, we extend iDisc with novel multiple-level aggregation and clusterer boosting techniques. We introduce a new measure on table importance and propose an approach to discovering cluster representatives to facilitate semantic browsing. An important feature of our framework is that it is highly extensible, where additional database representations and base clusterers may be easily incorporated into the framework. We have extensively evaluated iDisc using large real-world databases and results show that it discovers topical structures with a high degree of accuracy.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Wu, Wensheng and Reinwald, Berthold and Sismanis, Yannis and Manjrekar, Rajesh},
	year = {2008},
	keywords = {clustering, database structure, discovery, topical structure},
	pages = {1019--1030},
}

@article{wittek_somoclu_2017,
	title = {Somoclu: {An} efficient parallel library for self-organizing maps},
	volume = {78},
	doi = {10.18637/jss.v078.i09},
	number = {9},
	journal = {Journal of Statistical Software},
	author = {Wittek, Peter and Gao, Shi Chao and Lim, Ik Soo and Zhao, Li},
	year = {2017},
}

@article{Wilcoxon1945,
	title = {Individual comparisons by ranking methods},
	volume = {1},
	issn = {00994987},
	url = {http://www.jstor.org/stable/3001968},
	doi = {10.2307/3001968},
	number = {6},
	journal = {Biometrics Bulletin},
	author = {Wilcoxon, Frank},
	year = {1945},
	pages = {80--83},
}

@article{Wieringa2006,
	title = {Requirements engineering paper classification and evaluation criteria: {A} proposal and a discussion},
	volume = {11},
	issn = {09473602},
	url = {https://dl.acm.org/citation.cfm?id=1107683},
	doi = {10.1007/s00766-005-0021-6},
	abstract = {The article presents the outcome of the discussions by members of the steering committee of the International Electrical and Electronic Engineering Requirements Engineering (RE) Conference, regarding paper classification and evaluation criteria for RE papers. Section two of the article sketches the rationale for the classification. Section three presents the classification, and section four concludes with a discussion of background ideas and related work.},
	number = {1},
	journal = {Requirements Engineering},
	author = {Wieringa, Roel and Maiden, Neil and Mead, Nancy and Rolland, Colette},
	year = {2006},
	note = {ISBN: 0947-3602},
	keywords = {Paper classification, Paper evaluation criteria, Requirements engineering research, Research methods},
	pages = {102--107},
}

@article{WU2015693,
	title = {A review on algorithms for maximum clique problems},
	volume = {242},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221714008030},
	doi = {10.1016/j.ejor.2014.09.064},
	abstract = {The maximum clique problem (MCP) is to determine in a graph a clique (i.e., a complete subgraph) of maximum cardinality. The MCP is notable for its capability of modeling other combinatorial problems and real-world applications. As one of the most studied NP-hard problems, many algorithms are available in the literature and new methods are continually being proposed. Given that the two existing surveys on the MCP date back to 1994 and 1999 respectively, one primary goal of this paper is to provide an updated and comprehensive review on both exact and heuristic MCP algorithms, with a special focus on recent developments. To be informative, we identify the general framework followed by these algorithms and pinpoint the key ingredients that make them successful. By classifying the main search strategies and putting forward the critical elements of the most relevant clique methods, this review intends to encourage future development of more powerful methods and motivate new applications of the clique approaches.},
	number = {3},
	journal = {European Journal of Operational Research},
	author = {Wu, Qinghua and Hao, Jin-Kao},
	year = {2015},
	keywords = {Applications, Basics, Exact algorithms, Heuristics, Maximum clique problems},
	pages = {693--709},
}

@incollection{Villmann1999,
	address = {Amsterdam},
	title = {Topology preservation in self-organizing maps},
	isbn = {978-0-444-50270-4},
	url = {http://www.sciencedirect.com/science/article/pii/B978044450270450022X},
	abstract = {Publisher Summary Self-organizing map (SOMs) are special types of neural maps which have found a wide distribution. Neural maps constitute an important neural network paradigm. In brains, neural maps occur in all sensory modalities as well as in motor areas. In technical contexts, neural maps are utilized in the fashion of neighborhood preserving vector quantizers. In both cases, these networks project data from some possibly high-dimensional input space onto a position in some output space. To achieve this projection, neural maps are self-organized by unsupervised learning schemes. It also discusses the problem of topology preservation in self-organizing maps. A mathematically exact definition is developed for this and it show ways of measuring the degree of topology preservation. Finally, advanced learning scheme is also introduced for generating general hypercube structures for self-organizing maps which then yield improved topology preservation for the map.},
	booktitle = {Kohonen maps},
	publisher = {Elsevier Science B.V.},
	author = {Villmann, Th.},
	editor = {Oja, Erkki and Kaski, Samuel},
	year = {1999},
	doi = {10.1016/B978-044450270-4/50022-X},
	pages = {279 -- 292},
}

@article{uno_efficient_2010,
	title = {An efficient algorithm for solving pseudo clique enumeration problem},
	volume = {56},
	doi = {10.1007/s00453-008-9238-3},
	number = {1},
	journal = {Algorithmica},
	author = {Uno, Takeaki},
	year = {2010},
	note = {Publisher: Springer},
	keywords = {Basics, Interesting},
	pages = {3--16},
}

@techreport{ultsch2005esom,
	type = {Report},
	title = {{ESOM}-{Maps}: tools for clustering, visualization, and classification with {Emergent} {SOM}},
	url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=916f0fdb30a54fc5d9e5e9c69324ae2a86912928},
	institution = {University of Marburg},
	author = {Ultsch, Alfred and Mörchen, Fabian},
	year = {2005},
}

@inproceedings{ultsch2007emergence,
	title = {Emergence in self organizing feature maps},
	doi = {10.2390/biecoll-wsom2007-114},
	booktitle = {International workshop on self-organizing maps: {Proceedings} (2007)},
	author = {Ultsch, Alfred},
	year = {2007},
}

@article{Tomescu1981,
	title = {Le nombre maximum de cliques et de recouvrements par cliques des hypergraphes chromatiques complets},
	volume = {37},
	issn = {0012-365X},
	url = {https://www.sciencedirect.com/science/article/pii/0012365X81902259},
	doi = {10.1016/0012-365X(81)90225-9},
	abstract = {Dans ce travial on étudie le comportement asymptotique du nombre maximum de cliques des (h+1)-hypergraphes chromatiques complets à n sommets pour n → ∞ et on prove que pour tout k entier, k⩾1, il y a un nombre no(k,h) tel que pour tout n⩾n0(k,h) le nombre maximum de cliques des hypergraphes uniformes H à n sommets, de nombre chromatique χ(H)=k, est atteint dans la classe des hypergraphes chromatiques complets. On déduit le comportement asymptotique du nombre maximum de recouvrements par cliques de (h+1)-hypergraphes chromatiques complets (respectivement des (h+1)- hypergraphes de nombre chromatique égal à k), à n sommets, pour n → ∞. Au cas des recouvrements irréductibles on déduit le nombre maximum de cliques d'un recouvrement irréductible d'un hypergraphe chromatique complet et le comportement asymptotique du nombre maximum de recouvrements irréductibles par cliques dans la classe des (h+1)-hypergraphes chromatiques complets à n sommets, pour n → ∞. On fait la conjecture que le nombre maximum de cliques d'un hypergraphe uniforme est atteint dans la classe des hypergraphes chromatiques complets.},
	number = {2},
	journal = {Discrete Mathematics},
	author = {Tomescu, Ioan},
	year = {1981},
	pages = {263--277},
}

@article{Stonebraker2011,
	title = {The architecture of {SciDB}},
	volume = {6809 LNCS},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-22351-8_1},
	doi = {10.1007/978-3-642-22351-8_1},
	abstract = {SciDB is an open-source analytical database oriented toward the data management needs of scientists. As such it mixes statistical and linear algebra operations with data management ones, using a natural nested multi-dimensional array data model. We have been working on the code for two years, most recently with the help of venture capital backing. Release 11.06 (June 2011) is downloadable from our website (SciDB.org). This paper presents the main design decisions of SciDB. It focuses on our decisions concerning a high-level, SQL-like query language, the issues facing our query optimizer and executor and efficient storage management for arrays. The paper also discusses implementation of features not usually present in DBMSs, including version control, uncertainty and provenance.},
	journal = {Lecture Notes in Computer Science},
	author = {Stonebraker, Michael and Brown, Paul and Poliakov, Alex and Raman, Suchi},
	year = {2011},
	pmid = {11536845},
	note = {Publisher: Springer Berlin Heidelberg
ISBN: 9783642223501},
	keywords = {Scientific Computing, cluster:Distributed, linear algebra, multi-dimensional array, statistics, ★},
	pages = {1--16},
}

@book{Shull2008,
	title = {Guide to advanced empirical software engineering},
	isbn = {978-1-84800-043-8},
	abstract = {Empirical studies have become an integral element of software engineering research and practice. This unique text/reference includes chapters from some of the top international empirical software engineering researchers and focuses on the practical knowledge necessary for conducting, reporting and using empirical methods in software engineering. Part 1, Research Methods and Techniques, examines the proper use of various strategies for collecting and analysing data, and the uses for which those strategies are most appropriate. Part 2, Practical Foundations, provides a discussion of several important global issues that need to be considered from the very beginning of research planning. Finally, Knowledge Creation offers insight on using a set of disparate studies to provide useful decision support. Topics and features: Offers information across a range of techniques, methods, and qualitative and quantitative issues, providing a toolkit for the reader that is applicable across the diversity of software development contexts Presents reference material with concrete software engineering examples Provides guidance on how to design, conduct, analyse, interpret and report empirical studies, taking into account the common difficulties and challenges encountered in the field Arms researchers with the information necessary to avoid fundamental risks Tackles appropriate techniques for addressing disparate studies ensuring the relevance of empirical software engineering, and showing its practical impact Describes methods that are less often used in the field, providing less conventional but still rigorous and useful ways of collecting data Supplies detailed information on topics (such as surveys) that often contain methodological errors This broad-ranging, practical guide will prove an invaluable and useful reference for practising software engineers and researchers. In addition, it will be suitable for graduate students studying empirical methods in software development. Dr. Forrest Shull is a senior scientist at the Fraunhofer Center for Experimental Software Engineering, Maryland, and the director of its Measurement and Knowledge Management Division. In addition, he serves as associate editor in chief of IEEE Software magazine, specializing in empirical studies. Dr. Janice Singer heads the Human Computer Interaction program at the National Research Council, Canada. She has been conducting empirical research in software engineering for the past 12 years. Dr. Dag Sjøberg is currently research director of the software engineering group of the Simula Research Laboratory, Norway, which is ranked No. 3 in the world (out of 1400 institutions) in an evaluation in 2007 in the area of software and systems engineering.},
	author = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I.K.},
	year = {2008},
	doi = {10.1007/978-1-84800-044-5},
	note = {ISSN: 1098-6596},
}

@phdthesis{soriano2015bayesian,
	title = {Bayesian methods for two-sample comparison},
	url = {https://dukespace.lib.duke.edu/dspace/handle/10161/9859},
	school = {Duke University},
	author = {Soriano, Jacopo},
	year = {2015},
	keywords = {Interesting},
}

@article{Shearer2000,
	title = {The {CRISP}-{DM} model: {The} {New} {Blueprint} for {Data} {Mining}},
	volume = {5},
	issn = {1092-6208},
	abstract = {This article describes CRISP-DM (CRoss-Industry Standard Process for Data Mining), a non-proprietary, documented, and freely available data mining model. Developed by indus- try leaders with input from more than 200 data mining users and data mining tool and service providers, CRISP-DM is an industry-, tool-, and application-neutral model. This model encourages best practices and offers organizations the struc- ture needed to realize better, faster results from data mining. CRISP-DM organizes the data mining process into six phases: business understanding, data understanding, data prepara- tion, modeling, evaluation, and deployment. These phases help organizations understand the data mining process and provide a road map to follow while planning and carrying out a data mining project. This article explores all six phases, including the tasks involved with each phase. Sidebar materi- al, which takes a look at specific data mining problem types and techniques for addressing them, is provided.},
	number = {4},
	journal = {Journal of Data Warehousing},
	author = {Shearer, Colin and Watson, Hugh J and Grecich, Daryl G and Moss, Larissa and Adelman, Sid and Hammer, Katherine and Herdlein, Stacey a},
	year = {2000},
	keywords = {CRISP-DM Model, miner mineration},
	pages = {13--22},
}

@article{song2021fast,
	title = {A fast and effective large-scale two-sample test based on kernels},
	doi = {10.48550/arXiv.2110.03118},
	author = {Song, Hoseung and Chen, Hao},
	year = {2021},
}

@article{rothe2016deep,
	title = {Deep expectation of real and apparent age from a single image without facial landmarks},
	volume = {126},
	doi = {10.1007/s11263-016-0940-3},
	number = {2},
	journal = {International Journal of Computer Vision},
	author = {Rothe, Rasmus and Timofte, Radu and Van Gool, Luc},
	year = {2016},
	pages = {144--157},
}

@article{rosenblatt2019better,
	title = {Better-than-chance classification for signal detection},
	volume = {22},
	url = {https://doi.org/10.1093/biostatistics/kxz035},
	doi = {10.1093/biostatistics/kxz035},
	number = {2},
	journal = {Biostatistics (Oxford, England)},
	author = {Rosenblatt, Jonathan D and Benjamini, Yuval and Gilron, Roee and Mukamel, Roy and Goeman, Jelle J},
	year = {2019},
	keywords = {Basics, Interesting},
	pages = {365--380},
}

@inproceedings{oro44719,
	title = {Fostering open science to research using a taxonomy and an {eLearning} portal},
	url = {http://oro.open.ac.uk/44719/},
	doi = {10.1145/2809563.2809571},
	abstract = {The term "Open Science" is recently widely used, but it is still unclear to many research stakeholders - funders, policy makers, researchers, administrators, librarians and repository managers - how Open Science can be achieved. FOSTER (Facilitate Open Science Training for European Research) is a European Commission funded project, which is developing an e-learning portal to support the training of a wide range of stakeholders in Open Science and related areas. In 2014 the FOSTER project co-funded 28 training activities in Open Science, which include more than 110 events, while in 2015 the project has supported 24 community training events in 18 countries. In this paper, we describe the FOSTER approach in structuring the Open Science domain for educational purposes, present the functionality of the FOSTER training portal and discuss its use and potential for training the key stakeholders using self-learning and blended-learning methods.},
	booktitle = {{iKnow}: 15th international conference on knowledge technologies and data driven business},
	author = {Pontika, Nancy and Knoth, Petr and Cancellieri, Matteo and Pearce, Samuel},
	year = {2015},
	keywords = {FOSTER, Open Science, e-learning, scientific communication, taxonomy},
}

@article{Piatetsky-Shapiro1991,
	title = {Knowledge {Discovery} in {Real} {Databases}: {A} {Report} on the {International} {Joint} {Conference} on {Artificial} {Intelligence} 89 {Workshop}},
	volume = {11},
	issn = {0738-4602},
	url = {http://dl.acm.org/citation.cfm?id=124898.124915},
	number = {5},
	journal = {AI Magazine},
	author = {Piatetsky-Shapiro, Gregory},
	month = jan,
	year = {1991},
	pages = {68--70},
}

@inproceedings{perez2008kullback,
	title = {Kullback-{Leibler} divergence estimation of continuous distributions},
	doi = {10.1109/ISIT.2008.4595271},
	booktitle = {2008 {IEEE} international symposium on information theory},
	author = {Pérez-Cruz, Fernando},
	year = {2008},
	pages = {1666--1670},
}

@article{scikit-learn,
	title = {Scikit-learn: {Machine} learning in {Python}},
	volume = {12},
	url = {https://dl.acm.org/doi/abs/10.5555/1953048.2078195},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830},
}

@article{Polla2006,
	title = {Bibliography of self-organizing map ({SOM}) papers: 2002–2005},
	url = {http://users.ics.aalto.fi/tho/online-papers/TKK-ICS-R23.pdf},
	journal = {TKK Reports in Information and Computer Science},
	author = {Polla, Matti and Honkela, Timo and Kohonen, Teuvo},
	month = jan,
	year = {2006},
}

@article{Oja2003,
	title = {Bibliography of self-organizing map ({SOM}) papers: 1998-2001 addendum},
	volume = {3},
	url = {http://cis.legacy.ics.tkk.fi/research/som-bibl/NCS_vol3_1.pdf},
	journal = {Neural Computing Surveys},
	author = {Oja, Merja and Kaski, Samuel and Kohonen, Teuvo},
	month = feb,
	year = {2003},
	pages = {1--156},
}

@article{kaski1998bibliography,
	title = {Bibliography of self-organizing map ({SOM}) papers: 1981–1997},
	volume = {1},
	url = {http://cis.legacy.ics.tkk.fi/research/som-bibl/vol1_4.pdf},
	number = {3\&4},
	journal = {Neural computing surveys},
	author = {Kaski, Samuel and Kangas, Jari and Kohonen, Teuvo},
	year = {1998},
	pages = {1--176},
}

@article{Oates2006,
	title = {Researching {Information} {Systems} and {Computing}},
	volume = {37},
	issn = {1520510X},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2836698&tool=pmcentrez&rendertype=abstract},
	doi = {10.1016/j.ijinfomgt.2006.07.009},
	abstract = {With everything readers need to know about how to execute their research project, this book is written specifically for information systems (IS) and computing students. It introduces key quantitative and qualitative research methods, makes sense of underlying philosophies, and will help readers navigate and assess existing published academic papers. Throughout readers are supported by pedagogical features such as learning objectives, explanations, discussion questions, evaluation guides and suggestions for further reading.},
	author = {Oates, Briony J},
	year = {2006},
	note = {ISBN: 3175723993},
	pages = {341},
}

@article{MacLure2005,
	title = {'{Clarity} bordering on stupidity': where’s the quality in systematic review?},
	volume = {20},
	issn = {0268-0939},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02680930500131801},
	doi = {10.1080/02680930500131801},
	abstract = {Reprinted from Journal of education policy, 2005, vol. 20, no. 4, pp. 393-416},
	number = {4},
	journal = {Journal of Education Policy},
	author = {MacLure, Maggie},
	year = {2005},
	pages = {393--416},
}

@inproceedings{lopez2016revisiting,
	title = {Revisiting classifier two-sample tests},
	url = {https://hal.science/hal-01862834/},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lopez-Paz, David and Oquab, Maxime},
	year = {2017},
	keywords = {Basics, Interesting},
}

@inproceedings{pmlr-v119-liu20m,
	series = {Proceedings of machine learning research},
	title = {Learning deep kernels for non-parametric two-sample tests},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/liu20m.html},
	abstract = {We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two-sample tests is available at github.com/fengliu90/DK-for-TST.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Feng and Xu, Wenkai and Lu, Jie and Zhang, Guangquan and Gretton, Arthur and Sutherland, Danica J.},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {6316--6326},
}

@inproceedings{Kruse2017,
	title = {Fast approximate discovery of inclusion dependencies},
	url = {https://dl.gi.de/handle/20.500.12116/629},
	booktitle = {Datenbanksysteme für business, technologie und web ({BTW} 2017)},
	publisher = {Gesellschaft für Informatik, Bonn},
	author = {Kruse, Sebastian and Papenbrock, Thorsten and Dullweber, Christian and Finke, Moritz and Hegner, Manuel and Zabel, Martin and Zöllner, Christian and Naumann, Felix},
	editor = {Mitschang, Bernhard and Nicklas, Daniela and Leymann, Frank and Schöning, Harald and Herschel, Melanie and Teubner, Jens and Härder, Theo and Kopp, Oliver and Wieland, Matthias},
	year = {2017},
	pages = {207--226},
}

@incollection{koeller2006heuristic,
	title = {Heuristic strategies for the discovery of inclusion dependencies and other patterns},
	isbn = {978-3-540-31427-1},
	booktitle = {Journal on {Data} {Semantics} {V}},
	publisher = {Springer},
	author = {Koeller, Andreas and Rundensteiner, Elke A},
	year = {2006},
	doi = {10.1007/11617808_7},
	keywords = {Basics, Interesting},
	pages = {185--210},
}

@inproceedings{koeller2003discovery,
	title = {Discovery of high-dimensional inclusion dependencies},
	doi = {10.1109/ICDE.2003.1260834},
	booktitle = {Proceedings 19th international conference on data engineering (cat. {No}. {03CH37405})},
	author = {Koeller, Andreas and Rundensteiner, Elke A},
	year = {2003},
	pages = {683--685},
}

@article{kohonen1982self,
	title = {Self-organized formation of topologically correct feature maps},
	volume = {43},
	doi = {10.1007/BF00337288},
	number = {1},
	journal = {Biological cybernetics},
	author = {Kohonen, Teuvo},
	year = {1982},
	pages = {59--69},
}

@article{KOHONEN201352,
	title = {Essentials of the self-organizing map},
	volume = {37},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608012002596},
	doi = {10.1016/j.neunet.2012.09.018},
	abstract = {The self-organizing map (SOM) is an automatic data-analysis method. It is widely applied to clustering problems and data exploration in industry, finance, natural sciences, and linguistics. The most extensive applications, exemplified in this paper, can be found in the management of massive textual databases and in bioinformatics. The SOM is related to the classical vector quantization (VQ), which is used extensively in digital signal processing and transmission. Like in VQ, the SOM represents a distribution of input data items using a finite set of models. In the SOM, however, these models are automatically associated with the nodes of a regular (usually two-dimensional) grid in an orderly fashion such that more similar models become automatically associated with nodes that are adjacent in the grid, whereas less similar models are situated farther away from each other in the grid. This organization, a kind of similarity diagram of the models, makes it possible to obtain an insight into the topographic relationships of data, especially of high-dimensional data items. If the data items belong to certain predetermined classes, the models (and the nodes) can be calibrated according to these classes. An unknown input item is then classified according to that node, the model of which is most similar with it in some metric used in the construction of the SOM. A new finding introduced in this paper is that an input item can even more accurately be represented by a linear mixture of a few best-matching models. This becomes possible by a least-squares fitting procedure where the coefficients in the linear mixture of models are constrained to nonnegative values.},
	journal = {Neural Networks},
	author = {Kohonen, Teuvo},
	year = {2013},
	keywords = {Brain map, Data analysis, SOM, Self-organizing map, Similarity, Vector quantization},
	pages = {52--65},
}

@phdthesis{koeller2002integration,
	title = {Integration of heterogeneous databases: {Discovery} of meta-information and maintenance of schema-restructuring views},
	url = {https://digital.wpi.edu/concern/etds/00000005c?locale=en},
	school = {Worcester Polytechnic Institute},
	author = {Koeller, Andreas},
	year = {2002},
	keywords = {Interesting},
}

@inproceedings{kirchler2020two,
	title = {Two-sample testing using deep learning},
	url = {https://proceedings.mlr.press/v108/kirchler20a.html},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Kirchler, Matthias and Khorasani, Shahryar and Kloft, Marius and Lippert, Christoph},
	year = {2020},
	pages = {1387--1398},
}

@article{Khoussainova2010,
	title = {{SnipSuggest}: {Context}-aware autocompletion for {SQL}},
	volume = {4},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/1880172.1880175},
	doi = {10.14778/1880172.1880175},
	abstract = {In this paper, we present SnipSuggest, a system that provides on-the-go, context-aware assistance in the SQL composition process. SnipSuggest aims to help the increasing population of non-expert database users, who need to perform complex analysis on their large-scale datasets, but have difficulty writing SQL queries. As a user types a query, SnipSuggest recommends possible additions to various clauses in the query using relevant snippets collected from a log of past queries. SnipSuggest's current capabilities include suggesting tables, views, and table-valued functions in the FROM clause, columns in the SELECT clause, predicates in the WHERE clause, columns in the GROUP BY clause, aggregates, and some support for sub-queries. SnipSuggest adjusts its recommendations according to the context: as the user writes more of the query, it is able to provide more accurate suggestions.We evaluate SnipSuggest over two query logs: one from an undergraduate database class and another from the Sloan Digital Sky Survey database. We show that SnipSuggest is able to recommend useful snippets with up to 93.7\% average precision, at interactive speed. We also show that SnipSuggest outperforms naïve approaches, such as recommending popular snippets.},
	number = {1},
	journal = {Proceedings of the VLDB Endowment},
	author = {Khoussainova, Nodira and Kwon, YongChul and Balazinska, Magdalena and Suciu, Dan},
	month = oct,
	year = {2010},
	pages = {22--33},
}

@article{Karpathiotakis2014,
	title = {Adaptive query processing on {RAW} data},
	volume = {7},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2732977.2732986},
	doi = {10.14778/2732977.2732986},
	abstract = {Database systems deliver impressive performance for large classes of workloads as the result of decades of research into optimizing database engines. High performance, however, is achieved at the cost of versatility. In particular, database systems only operate efficiently over loaded data, i.e., data converted from its original raw format into the system's internal data format. At the same time, data volume continues to increase exponentially and data varies increasingly, with an escalating number of new formats. The consequence is a growing impedance mismatch between the original structures holding the data in the raw files and the structures used by query engines for efficient processing. In an ideal scenario, the query engine would seamlessly adapt itself to the data and ensure efficient query processing regardless of the input data formats, optimizing itself to each instance of a file and of a query by leveraging information available at query time. Today's systems, however, force data to adapt to the query engine during data loading. This paper proposes adapting the query engine to the formats of raw data. It presents RAW, a prototype query engine which enables querying heterogeneous data sources transparently. RAW employs Just-In-Time access paths, which efficiently couple heterogeneous raw files to the query engine and reduce the overheads of traditional general-purpose scan operators. There are, however, inherent overheads with accessing raw data directly that cannot be eliminated, such as converting the raw values. Therefore, RAW also uses column shreds, ensuring that we pay these costs only for the subsets of raw data strictly needed by a query. We use RAW in a real-world scenario and achieve a two-order of magnitude speedup against the existing hand-written solution.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Karpathiotakis, Manos and Branco, Miguel and Alagiannis, Ioannis and Ailamaki, Anastasia},
	month = aug,
	year = {2014},
	keywords = {RRaw: Yes, cluster:Adaptive Indexing, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {1119--1130},
}

@article{kantola1992,
	title = {Discovering functional and inclusion dependencies in relational databases},
	volume = {7},
	doi = {10.1002/int.4550070703},
	number = {7},
	journal = {International Journal of Intelligent Systems},
	author = {Kantola, Martti and Mannila, Heikki and Räihä, Kari-Jouko and Siirtola, Harri},
	year = {1992},
	pages = {591--607},
}

@article{Jorgensen2007,
	title = {A {Systematic} {Review} of {Software} {Development} {Cost} {Estimation} {Studies}},
	volume = {33},
	issn = {0098-5589},
	doi = {10.1109/TSE.2007.256943},
	abstract = {This paper aims to provide a basis for the improvement of software-estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. A Web-based library of these cost estimation papers is provided to ease the identification of relevant estimation research results. The review results combined with other knowledge provide support for recommendations for future software cost estimation research, including: 1) increase the breadth of the search for relevant studies, 2) search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) conduct more studies on estimation methods commonly used by the software industry, and 4) increase the awareness of how properties of the data sets impact the results when evaluating estimation methods},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Jorgensen, Magne and Shepperd, Martin},
	year = {2007},
	pages = {33--53},
}

@article{Idreos2011,
	title = {Here are my {Data} {Files}. {Here} are my {Queries}. {Where} are my {Results}?},
	url = {https://infoscience.epfl.ch/record/161489},
	abstract = {Database management systems (DBMS) provide incredible flexibility and performance when it comes to query processing, scalability and accuracy. To fully exploit DBMS features, however, the user must define a schema, load the data, tune the system for the expected workload, and answer several questions. Should the database use a column-store, a row-store or some hybrid format? What indices should be created? All these questions make for a formidable and time-consuming hurdle, often deterring new applications or imposing high cost to existing ones. A characteristic example is that of scientific databases with huge data sets. The prohibitive initialization cost and complexity still forces scientists to rely on "ancient" tools for their data management tasks, delaying scientific understanding and progress. Users and applications collect their data in flat files, which have traditionally been considered to be "outside" a DBMS. A DBMS wants control: always bring all data "inside", replicate it and format it in its own "secret" way. The problem has been recognized and current efforts extend existing systems with abilities such as reading information from flat files and gracefully incorporating it into the processing engine. This paper proposes a new generation of systems where the only requirement from the user is a link to the raw data files. Queries can then immediately be fired without preparation steps in between. Internally and in an abstract way, the system takes care of selectively, adaptively and incrementally providing the proper environment given the queries at hand. Only part of the data is loaded at any given time and it is being stored and accessed in the format suitable for the current workload.},
	journal = {CIDR '11: Fifth Biennial Conference on Innovative Data Systems Research},
	author = {Idreos, Stratos and Alagiannis, Ioannis and Johnson, Ryan and Ailamaki, Anastasia},
	year = {2011},
	keywords = {cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage},
	pages = {57--68},
}

@article{Hodges1958,
	title = {The significance probability of the {Smirnov} two-sample test},
	volume = {3},
	number = {5},
	journal = {Arkiv för Matematik},
	author = {Hodges, John L},
	year = {1958},
	pages = {469--486},
}

@article{hallin2021multivariate,
	title = {Multivariate goodness-of-fit tests based on {Wasserstein} distance},
	volume = {15},
	doi = {10.1214/21-EJS1816},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Hallin, Marc and Mordant, Gilles and Segers, Johan},
	year = {2021},
	pages = {1328--1371},
}

@article{mondal2015high,
	title = {On high dimensional two-sample tests based on nearest neighbors},
	volume = {141},
	doi = {10.1016/j.jmva.2015.07.002},
	journal = {Journal of Multivariate Analysis},
	author = {Mondal, Pronoy K and Biswas, Munmun and Ghosh, Anil K},
	year = {2015},
	keywords = {Interesting},
	pages = {168--178},
}

@article{Schilling1986b,
	title = {Multivariate two-sample tests based on nearest neighbors},
	volume = {81},
	url = {https://www.jstor.org/stable/2241756},
	number = {395},
	journal = {Journal of the American Statistical Association},
	author = {Schilling, Mark F},
	year = {1986},
	keywords = {Interesting},
	pages = {799--806},
}

@article{Henze1988,
	title = {A multivariate two-sample test based on the number of nearest neighbor type coincidences},
	volume = {16},
	issn = {00905364},
	url = {http://www.jstor.org/stable/2241756},
	abstract = {For independent d-variate random samples X1, ⋯, Xn1 i.i.d. f(x), Y1, ⋯, Yn2 i.i.d. g(x), where the densities f and g are assumed to be continuous a.e., consider the number T of all k nearest neighbor comparisons in which observations and their neighbors belong to the same sample. We show that, if f = g a.e., the limiting (normal) distribution of T, as (n₁, n₂) →∞, n₁/(n₁ + n₂) →τ, 0 {\textless} τ{\textless} 1, does not depend on f. An omnibus procedure for testing the hypothesis H0: f = g a.e. is obtained by rejecting H0 for large values of T. The result applies to a general distance (generated by a norm on Rd) for determining nearest neighbors, and it generalizes to the multisample situation.},
	number = {2},
	journal = {The Annals of Statistics},
	author = {Henze, Norbert},
	year = {1988},
	keywords = {Interesting},
	pages = {772--783},
}

@misc{HDF,
	title = {{HDF} {Group}},
	url = {https://www.hdfgroup.org/},
	author = {HDF},
}

@article{Han2016,
	title = {{AccuracyTrader}: {Accuracy}-{Aware} {Approximate} {Processing} for {Low} {Tail} {Latency} and {High} {Result} {Accuracy} in {Cloud} {Online} {Services}},
	issn = {01903918},
	url = {https://arxiv.org/abs/1607.02734},
	doi = {10.1109/ICPP.2016.39},
	abstract = {Modern latency-critical online services such as search engines often process requests by consulting large input data spanning massive parallel components. Hence the tail latency of these components determines the service latency. To trade off result accuracy for tail latency reduction, ex- isting techniques use the components responding before a specified deadline to produce approximate results. However, they may skip a large proportion of components when load gets heavier, thus incurring large accuracy losses. This paper presents AccuracyTrader that produces approximate results with small accuracy losses while maintaining low tail la- tency. AccuracyTrader aggregates information of input data on each component to create a small synopsis, thus enabling all components producing initial results quickly using their synopses. AccuracyTrader also uses synopses to identify the parts of input data most related to arbitrary requests’ result accuracy, thus first using these parts to improve the produced results in order to minimize accuracy losses. We evaluated AccuracyTrader using workloads in real services. The results show: (i) AccuracyTrader reduces tail latency by over 40 times with accuracy losses of less than 7\% compared to existing exact processing techniques; (ii) when using the same latency, AccuracyTrader reduces accuracy losses by over 13 times comparing to existing approximate processing techniques.},
	number = {8},
	journal = {The 45th International Conference on Parallel Processing},
	author = {Han, Rui and Huang, Siguang and Tang, Fei and Chang, Fugui and Zhan, Jianfeng},
	month = aug,
	year = {2016},
	keywords = {RDistributed: Yes, RInteractive: Yes, Result accuracy, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {278--287},
}

@article{Guba1990,
	title = {The alternative paradigm dialog},
	issn = {03186431},
	abstract = {"The contributors and editor of this work are to be commended for their successful efforts in delineating many of the concerns current within education and in calling for frank debate on these issues by all interested parties. Furthermore, they have stimulated good scholarship by readily admitting to the current state of affairs being one of more questions than answers and more confusion than clarity. They thus remind us that the search for knowledge is one fraught with conflict in a public arena. "The appropriate audience for this volume is assessed to be the reader who derives satisfaction from critical thinking. It would be appropriate for graduate students in education, human services, social sciences, or theology, or any person committed to the endeavor and process of education. "The Paradigm Dialog is one of those rare books that simultaneously stretches the mind while projecting one into self-reflection. For the applied practitioner, whether teacher, counselor, or consultant, the possibility of gaining further insight into the underlying assumptions which constrain one's pedagogy or practice is highly possible upon a critical reading." -The Journal of Applied Rehabilitation Counseling Is scientific positivism, long the reigning paradigm for research in the social sciences, the "best way" to conduct social research? This is the central question examined in The Paradigm Dialog. Recently three key challengers have appeared-postpositivism, critical theory, and constructivism. All three offer researchers new methodological approaches, and all three present fundamental questions that must be addressed. Can research be conducted between paradigms? Are they equally useful in answering questions of applied research? What constitutes good, or ethical, research in each? In this volume, these and other significant questions are examined by a multidisciplinary group of leading figures in qualitative research. Not surprisingly, there is no agreement on the "best" paradigm question, but the dialog offered in this compelling volume deftly explores important issues in selecting the proper paradigm for tackling a variety of research questions. With a group of contributors that reads like a veritable who's who in qualitative research, The Paradigm Dialog is a must for anyone conducting research in the social sciences.},
	journal = {The paradigm dialog},
	author = {Guba, Egon G.},
	year = {1990},
}

@article{gretton2012kernel,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	url = {http://jmlr.org/papers/v13/gretton12a.html},
	number = {25},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	pages = {723--773},
}

@article{Gray1997,
	title = {Data cube: {A} relational aggregation operator generalizing group-by, cross-tab, and sub-totals},
	issn = {13845810},
	doi = {10.1023/A:1009726021843},
	abstract = {Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, cross-tabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.},
	journal = {Data Mining and Knowledge Discovery},
	author = {Gray, Jim and Chaudhuri, Surajit and Bosworth, Adam and Layman, Andrew and Reichart, Don and Venkatrao, Murali and Pellow, Frank and Pirahesh, Hamid},
	year = {1997},
	keywords = {Data cube, Data mining, Database, Query, Summarization},
}

@incollection{guba_competing_1994,
	title = {Competing {Paradigms} in {Qualitative} {Research}},
	abstract = {IN this chapter we analyze four paradigms that currently are competing, or have until recently com- peted, for acceptance as the paradigm of choice in informing and guiding inquiry, especially qualitative inquiry: positivism, postpositivism, critical theory and related ideological positions, and constructiv- ism. We acknowledge at once our own commitment to constructivism (which we earlier called "natural- istic inquiry"; Lincoln \& Guba, 1985); the reader may wish to take that fact into account in judging the appropriateness and usefulness of our analysis. Although the title of this volume, Handbook of Qualitative Research, implies that the term qualita- tive is an umbrella term superior to the term para- digm (and, indeed, that usage is not uncommon), it is our position that it is a term that ought to be reserved for a description of types of methods.From our perspective, both qualitative and quantitative methods may be used appropriately with any re- search paradigm. Questions of method are secon- dary to questions of paradigm, which we define as the basic belief system or worldview that guides the investigator, not only in choices of method but in ontologicallyandepistemologicallyfundamentalways. It is certainly the case that interest in alternative paradigms has been stimulated by a growing dissat- isfaction with the patent overemphasis on quantita- tive methods. But as efforts were made to build a case for a renewed interestin qualitative approaches, it became clear that the metaphysical assumptions undergirding the conventional paradigm (the "re- ceived view") must be seriously questioned. Thus the emphasis of this chapter is on paradigms, their assumptions, and the implications of those assump- tions for a variety of research issues, not on the relative utility of qualitative versus quantitative methods. Nevertheless, as discussions of para- digms/methods over the past decade have often be- gun with a consideration of problems associated with overquantification, we will also begin there, shifting only later to our predominant interest.},
	booktitle = {Handbook of qualitative research},
	author = {Guba, E. G. and Lincoln, Y. S},
	year = {1994},
	note = {CitationKey: Guba1994},
}

@article{friedman2004multivariate,
	title = {On multivariate goodness-of-fit and two-sample testing},
	volume = {1},
	url = {https://www.slac.stanford.edu/econf/C030908/papers/SLAC-R-703.pdf#page=321},
	journal = {Proceedings of the Conference on Statistical Problems in Particle Physics, Astrophysics, and Cosmology},
	author = {Friedman, Jerome},
	year = {2003},
	pages = {311--313},
}

@article{Wilson1927,
	title = {Probable inference, the law of succession, and statistical inference},
	volume = {22},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1927.10502953},
	doi = {10.1080/01621459.1927.10502953},
	number = {158},
	journal = {Journal of the American Statistical Association},
	author = {{Edwin B. Wilson}},
	year = {1927},
	pages = {209--212},
}

@book{Davis1997,
	title = {Writing the doctoral dissertation: {A} systematic approach},
	publisher = {Barron's Educational Series},
	author = {Davis, Gordon Bitter and Parker, Clyde Alvin},
	year = {1979},
}

@inproceedings{DeMarchi2003zigzag,
	title = {Zigzag: a new algorithm for mining large inclusion dependencies in databases},
	doi = {10.1109/ICDM.2003.1250899},
	booktitle = {Third {IEEE} international conference on data mining},
	author = {De Marchi, Fabien and Petit, J-M},
	year = {2003},
	pages = {27--34},
}

@inproceedings{DeMarchi2002,
	title = {Efficient algorithms for mining inclusion dependencies},
	doi = {10.1007/3-540-45876-X_30},
	booktitle = {International conference on extending database technology},
	author = {De Marchi, Fabien and Lopes, Stéphane and Petit, Jean-Marc},
	year = {2002},
	keywords = {Schema Matching},
	pages = {464--476},
}

@inproceedings{Budgen2008,
	title = {Using {Mapping} {Studies} in {Software} {Engineering}},
	volume = {2},
	url = {https://www.ppig.org/files/2008-PPIG-20th-budgen.pdf},
	abstract = {Background: A mapping study provides a systematic and objective procedure for identifying the nature and extent of the empirical study data that is available to answer a particular research question. Such studies can also form a useful preliminary step for PhD study. Aim: We set out to assess how effective such studies have been when used for software engineering topics, and to identify the specific challenges that they present. Method: We have conducted an informal review of a number of mapping studies in software engineering, describing their main characteristics and the forms of analysis employed. Results: We examine the experiences and outcomes from six mapping studies, of which four are published. From these we note a recurring theme about the problems of classification and a preponderance of ‘gaps’ in the set of empirical studies. Conclusions: We identify our challenges as improving classification guidelines, encouraging better reporting of primary studies, and argue for identifying some ’empirical grand challenges’ for software engineering as a focus for the community.},
	booktitle = {Proceedings of {Psychology} of {Programming} {Interest} {Group}},
	author = {Budgen, David and Turner, Mark and Brereton, Pearl and Kitchenham, Barbara},
	year = {2008},
	pages = {195--204},
}

@book{breiman_classification_1984,
	title = {Classification {And} {Regression} {Trees}},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, Richard and Stone, Charles},
	year = {1984},
	doi = {10.1201/9781315139470},
}

@inproceedings{brunato2007effectively,
	title = {On effectively finding maximal quasi-cliques in graphs},
	doi = {10.1007/978-3-540-92695-5_4},
	booktitle = {International conference on learning and intelligent optimization},
	author = {Brunato, Mauro and Hoos, Holger H and Battiti, Roberto},
	year = {2007},
	pages = {41--55},
}

@article{Baud2012,
	title = {The {LHCb} {Data} {Management} {System}},
	volume = {396},
	url = {http://stacks.iop.org/1742-6596/396/i=3/a=032023},
	doi = {10.1088/1742-6596/396/3/032023},
	abstract = {The LHCb Data Management System is based on the DIRAC Grid Community Solution. LHCbDirac provides extensions to the basic DMS such as a Bookkeeping System. Datasets are defined as sets of files corresponding to a given query in the Bookkeeping system. Datasets can be manipulated by CLI tools as well as by automatic transformations (removal, replication, processing). A dynamic handling of dataset replication is performed, based on disk space usage at the sites and dataset popularity. For custodial storage, an on-demand recall of files from tape is performed, driven by the requests of the jobs, including disk cache handling. We shall describe the tools that are available for Data Management, from handling of large datasets to basic tools for users as well as for monitoring the dynamic behavior of LHCb Storage capacity.},
	number = {3},
	journal = {Journal of Physics: Conference Series},
	author = {Baud, J P and Charpentier, Ph and Ciba, K and Graciani, R and Lanciotti, E and Màthè, Z and Remenska, D and Santana, R},
	year = {2012},
	pages = {32023},
}

@article{alcala2011keel,
	title = {Keel data-mining software tool: data set repository, integration of algorithms and experimental analysis framework.},
	volume = {17},
	url = {https://sci2s.ugr.es/sites/default/files/files/ScientificImpact/255-287%20pp%20MVLSC_169i.pdf},
	journal = {Journal of Multiple-Valued Logic \& Soft Computing},
	author = {Alcalá-Fernández, Jesús and Fernández, Alberto and Luengo, Julián and Derrac, Joaquín and García, Salvador and Sánchez, Luciano and Herrera, Francisco},
	year = {2011},
}

@article{Alagiannis2012Adaptive,
	title = {{NoDB} in action: adaptive query processing on raw data},
	volume = {5},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2367502.2367543},
	doi = {10.14778/2367502.2367543},
	abstract = {As data collections become larger and larger, users are faced with increasing bottlenecks in their data analysis. More data means more time to prepare the data, to load the data into the database and to execute the desired queries. Many applications already avoid using traditional database systems, e.g., scientific data analysis and social networks, due to their complexity and the increased data-to-query time, i.e. the time between getting the data and retrieving its first useful results. For many applications data collections keep growing fast, even on a daily basis, and this data deluge will only increase in the future, where it is expected to have much more data than what we can move or store, let alone analyze. In this demonstration, we will showcase a new philosophy for designing database systems called NoDB. NoDB aims at minimizing the data-to-query time, most prominently by removing the need to load data before launching queries. We will present our prototype implementation, PostgresRaw, built on top of PostgreSQL, which allows for efficient query execution over raw data files with zero initialization overhead. We will visually demonstrate how PostgresRaw incrementally and adaptively touches, parses, caches and indexes raw data files autonomously and exclusively as a side-effect of user queries.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Alagiannis, Ioannis and Borovica, Renata and Branco, Miguel and Idreos, Stratos and Ailamaki, Anastasia},
	month = aug,
	year = {2012},
	keywords = {★},
	pages = {1942--1945},
}

@inproceedings{Agarwal2013,
	address = {New York, NY, USA},
	title = {{BlinkDB}: queries with bounded errors and bounded response times on very large data},
	isbn = {978-1-4503-1994-2},
	url = {http://dl.acm.org/citation.cfm?doid=2465351.2465355},
	doi = {10.1145/2465351.2465355},
	abstract = {In this paper, we present BlinkDB, a massively parallel, sampling-based approximate query engine for running ad-hoc, interactive SQL queries on large volumes of data. The key insight that BlinkDB builds on is that one can often make reasonable decisions in the absence of perfect answers. For example, reliably detecting a malfunctioning server using a distributed collection of system logs does not require analyzing every request processed by the system. Based on this insight, BlinkDB allows one to trade-off query accuracy for response time, enabling interactive queries over massive data by running queries on data samples and presenting results annotated with meaningful error bars. To achieve this, BlinkDB uses two key ideas that differentiate it from previous work in this area: (1) an adaptive optimization framework that builds and maintains a set of multi-dimensional, multi-resolution samples from original data over time, and (2) a dynamic sample selection strategy that selects an appropriately sized sample based on a query's accuracy and/or response time requirements. We have built an open-source version of BlinkDB and validated its effectiveness using the well-known TPC-H benchmark as well as a real-world analytic workload derived from Conviva Inc. Our experiments on a 100 node cluster show that BlinkDB can answer a wide range of queries from a real-world query trace on up to 17 TBs of data in less than 2 seconds (over 100{\textbackslash}times faster than Hive), within an error of 2 - 10\%.},
	booktitle = {Proceedings of the 8th {ACM} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Agarwal, Sameer and Mozafari, Barzan and Panda, Aurojit and Milner, Henry and Madden, Samuel and Stoica, Ion},
	year = {2013},
	keywords = {RDistributed: Yes, RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution, ★},
	pages = {29},
}

@article{SDSS14,
	title = {The {Fourteenth} {Data} {Release} of the {Sloan} {Digital} {Sky} {Survey}},
	volume = {235},
	doi = {10.3847/1538-4365/aa9e8a},
	number = {2},
	journal = {The Astrophysical Journal Supplement Series},
	author = {Abolfathi, Bela and Aguado, DS and Aguilar, Gabriela and Prieto, Carlos Allende and Almeida, Andres and Ananna, Tonima Tasnim and Anders, Friedrich and Anderson, Scott F and Andrews, Brett H and Anguiano, Borja and {others}},
	year = {2018},
	pages = {42},
}

@misc{noauthor_sdss_2019,
	title = {{SDSS}},
	url = {https://www.sdss.org/},
	urldate = {2019-05-28},
	year = {2019},
	note = {CitationKey: SDSS},
}

@misc{noauthor_sdss_nodate,
	title = {{SDSS} {Query} / {CasJobs}},
	url = {http://skyserver.sdss.org/CasJobs/},
	urldate = {2019-05-28},
	note = {CitationKey: SDSSQueryCasJobs},
}

@inproceedings{SaneiMehri2018,
	title = {Enumerating top-k quasi-cliques},
	doi = {10.1109/BigData.2018.8622352},
	booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data}},
	author = {Sanei-Mehri, Seyed-Vahid and Das, Apurba and Tirthapura, Srikanta},
	year = {2018},
	keywords = {Interesting, to-read},
	pages = {1107--1112},
}

@inproceedings{silva2011som,
	title = {A {SOM} combined with {KNN} for classification task},
	doi = {10.1109/IJCNN.2011.6033525},
	booktitle = {The 2011 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Silva, Leandro A and Del-Moral-Hernandez, Emilio},
	year = {2011},
	pages = {2368--2373},
}

@inproceedings{salojarvi2005inferring,
	address = {Whistler, BC, Canada},
	title = {Inferring relevance from eye movements: {Feature} extraction},
	url = {http://research.ics.aalto.fi/events/inips2005/inips2005proceedings.pdf#page=45},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Salojärvi, Jarkko and Puolamäki, Kai and Simola, Jaana and Kovanen, Lauri and Kojo, Ilpo and Kaski, Samuel},
	year = {2005},
	pages = {45},
}

@techreport{randles1979introduction,
	type = {Report},
	title = {Introduction to the theory of nonparametric statistics},
	institution = {John Wiley},
	author = {Randles, Ronald H and Wolfe, Douglas A},
	year = {1979},
}

@inproceedings{Rostin2009,
	title = {A machine learning approach to foreign key discovery.},
	url = {https://hpi.de/fileadmin/user_upload/fachgebiete/naumann/publications/2009/WebDB09_crc.pdf},
	booktitle = {12th {International} {Workshop} on the {Web} and {Databases}},
	author = {Rostin, Alexandra and Albrecht, Oliver and Bauckmann, Jana and Naumann, Felix and Leser, Ulf},
	year = {2009},
	keywords = {Interesting, Schema Matching},
}

@inproceedings{ramdas2015decreasing,
	title = {On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions},
	volume = {29},
	doi = {10.1609/aaai.v29i1.9692},
	booktitle = {Proceedings of the {AAAI} conference on {Artificial} {Intelligence}},
	author = {Ramdas, Aaditya and Reddi, Sashank Jakkam and Póczos, Barnabás and Singh, Aarti and Wasserman, Larry},
	year = {2015},
	note = {Number: 1},
	keywords = {Basics, Interesting},
}

@article{Palpanas2015,
	title = {Data series management: {The} road to big sequence analytics},
	volume = {44},
	issn = {01635808 (ISSN)},
	url = {https://dl.acm.org/citation.cfm?id=2814719},
	doi = {10.1145/2814710.2814719},
	abstract = {Massive data series collections are becoming a reality for virtually every scientific and social domain, and have to be processed and analyzed, in order to extract useful knowledge. Current data series management solutions are ad hoc, requiring huge investments in time and effort, and duplication of effort across different teams. Systems like relational databases, Column Stores, and Array Databases are not a suitable solution either, since none of these systems offers native support for data series. Our vision is to design and develop a generalpurpose Data Series Management System, able to cope with big data series, that is, very large and fast-changing collections of data series, which can be heterogeneous (i.e., originate from disparate domains and thus exhibit very different characteristics), and which can have uncertainty in their values (e.g., due to inherent errors in the measurements). Just like databases abstracted the relational data management problem and offered a black box solution that is now omnipresent, the proposed system will allow analysts that are not experts in data series management, as well as common users, to tap in the goldmine of the massive and ever-growing data series collections they (already) have.},
	number = {2},
	journal = {ACM SIGMOD Record},
	author = {Palpanas, T},
	year = {2015},
	keywords = {Big Data, Data acquisition, Highway planning, Human resource management, Information management, Inherent errors, Massive data, Relational Database, Relational data, cluster:Indexes, layer:Database Layer, supercluster:Indexes, type:Evaluation Research},
	pages = {47--52},
}

@article{Ogasawara2011,
	title = {An algebraic approach for data-centric scientific workflows},
	volume = {4},
	url = {https://hal.inria.fr/hal-00640431},
	abstract = {Scientific workflows have emerged as a basic abstraction for structuring and executing scientific experiments in computational environments. In many situations, these workflows are computationally and data intensive, thus requiring execution in large-scale parallel computers. However, parallelization of scientific workflows remains low-level, ad-hoc and labor- intensive, which makes it hard to exploit optimization opportunities. To address this problem, we propose an algebraic approach (inspired by relational algebra) and a parallel execution model that enable automatic optimization of scientific workflows. We conducted a thorough validation of our approach using both a real oil exploitation application and synthetic data scenarios. The experiments were run in Chiron, a data-centric scientific workflow engine implemented to support our algebraic approach. Our experiments demonstrate performance improvements of up to 226\% compared to an ad-hoc workflow implementation.},
	number = {11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ogasawara, Eduardo and Dias, Jonas and Oliveira, Daniel and Porto, Fabio and Valduriez, Patrick and Mattoso, Marta},
	year = {2011},
	pages = {1328--1339},
}

@article{Graefe2012,
	title = {Concurrency control for adaptive indexing},
	volume = {5},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2180912.2180918},
	doi = {10.14778/2180912.2180918},
	abstract = {Adaptive indexing initializes and optimizes indexes incrementally, as a side effect of query processing. The goal is to achieve the benefits of indexes while hiding or minimizing the costs of index creation. However, index-optimizing side effects seem to turn read- only queries into update transactions that might, for example, create lock contention. This paper studies concurrency control in the context of adaptive indexing. We show that the design and implementation of adaptive indexing rigorously separates index structures from index contents; this relaxes the constraints and requirements during adaptive in- dexing compared to those of traditional index updates. Our design adapts to the fact that an adaptive index is refined continuously, and exploits any concurrency opportunities in a dynamic way. A detailed experimental analysis demonstrates that (a) adaptive indexing maintains its adaptive properties even when running con- current queries, (b) adaptive indexing can exploit the opportunity for parallelism due to concurrent queries, (c) the number of con- currency conflicts and any concurrency administration overheads follow an adaptive behavior, decreasing as the workload evolves and adapting to the workload needs.},
	number = {7},
	journal = {Proceedings of the VLDB Endowment},
	author = {Graefe, Goetz and Halim, Felix and Idreos, Stratos and Kuno, Harumi and Manegold, Stefan},
	year = {2012},
	note = {arXiv: 1203.6405v1},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {656--667},
}

@article{Potti2015,
	title = {{DAQ}: {A} {New} {Paradigm} for {Approximate} {Query} {Processing}},
	volume = {8},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2777598.2777599},
	doi = {10.14778/2777598.2777599},
	abstract = {Many modern applications deal with exponentially increasing data volumes and aid business-critical decisions in near real-time. Particularly in exploratory data analysis, the focus is on interactive querying and some degree of error in estimated results is tolerable. A common response to this challenge is approximate query processing, where the user is presented with a quick confidence interval estimate based on a sample of the data. In this work, we highlight some of the problems that are associated with this probabilistic approach when extended to more complex queries, both in semantic interpretation and the lack of a formal algebra. As an alternative, we propose deterministic approximate querying (DAQ) schemes, formalize a closed deterministic approximation algebra, and outline some design principles for DAQ schemes. We also illustrate the utility of this approach with an example deterministic online approximation scheme which uses a bitsliced index representation and computes the most significant bits of the result first. Our prototype scheme delivers speedups over exact aggregation and predicate evaluation, and outperforms sampling-based schemes for extreme value aggregations.},
	number = {9},
	journal = {Proceedings of the VLDB Endowment},
	author = {Potti, Navneet and Patel, Jignesh M},
	month = may,
	year = {2015},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {898--909},
}

@article{Khan:2017:DTI:3055330.3055333,
	title = {Data {Tweening}: {Incremental} {Visualization} of {Data} {Transforms}},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3055330.3055333},
	doi = {10.14778/3055330.3055333},
	abstract = {In the context of interactive query sessions, it is common to issue a succession of queries, transforming a dataset to the desired result. It is often difficult to comprehend a succession of transformations, especially for complex queries. Thus, to facilitate understanding of each data transformation and to provide continuous feedback, we introduce the concept of “data tweening”, i.e., interpolating between resultsets, presenting to the user a series of incremental visual representations of a resultset transformation. We present tweening methods that consider not just the changes in the result, but also the changes in the query. Through user studies, we show that data tweening allows users to efficiently comprehend data transforms, and also enables them to gain a better understanding of the underlying query operations.},
	number = {6},
	journal = {Proceedings of the VLDB Endowment},
	author = {Khan, Meraj and Xu, Larry and Nandi, Arnab and Hellerstein, Joseph M},
	month = feb,
	year = {2017},
	keywords = {RInteractive: Yes, UserStudy:yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {661--672},
}

@article{Ailamaki:2015:DHB:2824032.2824142,
	title = {Databases and {Hardware}: {The} {Beginning} and {Sequel} of a {Beautiful} {Friendship}},
	volume = {8},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2824032.2824142},
	doi = {10.14778/2824032.2824142},
	abstract = {Fast query and transaction processing is the goal of 40 years of database research and the reason of existence for many new database system architectures. In data management, system performance means acceptable response time and throughput on critical-path operations, ideally with scalability guarantees. Performance is improved with top-of-the line research on data processing algorithms; efficiency, however, is contingent on seamless collaboration between the database software and hardware and storage devices. In 1980, the goal was to minimize disk accesses; in 2000, memory replaced disks in terms of access costs. Nowadays performance is synonymous to scalability; scalability, in turn, translates into sustainable and predictable use of hardware resources in the face of embarrassing parallelism and deep storage hierarchies while minimizing energy needs - a challenging goal in multiple dimensions. We discuss work done in the past four decades to tighten the interaction between the database software and underlying hardware and show that, as application and microarchitecture roadmaps evolve, the effort of maintaining smooth collaboration blossoms into a multitude of interesting research avenues with direct technological impact.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ailamaki, Anastasia},
	month = aug,
	year = {2015},
	note = {Publisher: VLDB Endowment},
	pages = {2058--2061},
}

@article{Wu2015,
	title = {Efficient {Evaluation} of {Object}-{Centric} {Exploration} {Queries} for {Visualization}},
	volume = {8},
	issn = {21508097},
	url = {https://dl.acm.org/citation.cfm?doid=2824032.2824072},
	doi = {10.14778/2824032.2824072},
	abstract = {The most effective way to explore data is through visualizing the results of exploration queries. For example, an exploration query could be an aggregate of some measures over time intervals, and a pattern or abnormality can be discovered through a time series plot of the query results. In this paper, we examine a special kind of exploration query, namely object-centric exploration query. Common examples include claims made about athletes in sports databases, such as " it is newsworthy that LeBron James has scored 35 or more points in nine consecutive games. " We focus on one common type of visualization, i.e., 2d scat-ter plot with heatmap. Namely, we consider exploration queries whose results can be plotted on a two-dimensional space, possibly with colors indicating object densities in regions. While we model results as pairs of numbers, the types of the queries are limited only by the users' imagination. In the LeBron James example above, the two dimensions are minimum points scored per game and number of consecutive games, respectively. It is easy to find other equally interesting dimensions, such as minimum rebounds per game or number of playoff games. We formalize this problem and propose an efficient, interactive-speed algorithm that takes a user-provided exploration query (which can be a blackbox function) and produces an approximate visual-ization that preserves the two most important visual properties: the outliers and the overall distribution of all result points.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Wu, You and Harb, Boulos and Yang, Jun and Yu, Cong},
	year = {2015},
	keywords = {RInteractive: Yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1752--1763},
}

@article{Panev:2016:EDV:3007263.3007300,
	title = {Exploring {Databases} via {Reverse} {Engineering} {Ranking} {Queries} with {PALEO}},
	volume = {9},
	issn = {21508097},
	url = {http://dx.doi.org/10.14778/3007263.3007300},
	doi = {10.14778/3007263.3007300},
	abstract = {A novel approach to explore databases using ranked lists is demonstrated. Working with ranked lists, capturing the relative performance of entities, is a very intuitive and widely applicable concept. Users can post lists of entities for which explanatory SQL queries and full result lists are returned. By refining the input, the results, or the queries, user can interactively explore the database content. The demonstrated system is centered around our PALEO framework for reverse engineering OLAP-style database queries and novel work on mining interesting categorical attributes.},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Panev, Kiril and Michel, Sebastian and Milchevski, Evica and Pal, Koninika},
	month = sep,
	year = {2016},
	note = {Publisher: VLDB Endowment},
	keywords = {UserStudy:yes, cluster:Automatic Exploration, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1525--1528},
}

@article{Karpathiotakis2016,
	title = {Fast queries over heterogeneous data through engine customization},
	volume = {9},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2994509.2994516},
	doi = {10.14778/2994509.2994516},
	abstract = {Industry and academia are continuously becoming more data-driven and data-intensive, relying on the analysis of a wide variety of heterogeneous datasets to gain insights. The different data models and formats pose a significant challenge on performing analysis over a combination of diverse datasets. Serving all queries using a single, general-purpose query engine is slow. On the other hand, using a specialized engine for each heterogeneous dataset increases complexity: queries touching a combination of datasets require an integration layer over the different engines. This paper presents a system design that natively supports heterogeneous data formats and also minimizes query execution times. For multi-format support, the design uses an expressive query algebra which enables operations over various data models. For minimal execution times, it uses a code generation mechanism to mimic the system and storage most appropriate to answer a query fast. We validate our design by building Proteus, a query engine which natively supports queries over CSV, JSON, and relational binary data, and which specializes itself to each query, dataset, and workload via code generation. Proteus outperforms state-of-the-art open-source and commercial systems on both synthetic and real-world workloads without being tied to a single data model or format, all while exposing users to a single query interface.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Karpathiotakis, Manos and Alagiannis, Ioannis and Ailamaki, Anastasia},
	year = {2016},
	keywords = {RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {972--983},
}

@article{Nandi2013a,
	title = {Gestural query specification},
	volume = {7},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2732240.2732247},
	doi = {10.14778/2732240.2732247},
	abstract = {Direct, ad-hoc interaction with databases has typically been per- formed over console-oriented conversational interfaces using query languages such as SQL.With the rise in popularity of gestural user interfaces and computing devices that use gestures as their exclusive modes of interaction, database query interfaces require a fundamen- tal rethinking to work without keyboards. We present a novel query specification system that allows the user to query databases using a series of gestures. We present a novel gesture recognition system that uses both the interaction and the state of the database to classify gestural input into relational database queries. We conduct exhaus- tive systems performance tests and user studies to demonstrate that our system is not only performant and capable of interactive laten- cies, but it is also more usable, faster to use and more intuitive than existing systems. 1.},
	number = {4},
	journal = {Proceedings of the VLDB Endowment},
	author = {Nandi, Arnab and Jiang, Lilong and Mandel, Michael},
	year = {2013},
	keywords = {UserStudy:yes, cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Philosophical Paper},
	pages = {289--300},
}

@article{Wang2016,
	title = {Fast and adaptive indexing of multi-dimensional observational data},
	volume = {9},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3007328.3007334},
	doi = {10.14778/3007328.3007334},
	abstract = {Sensing devices generate tremendous amounts of data each day, which include large quantities of multi-dimensional mea- surements. These data are expected to be immediately avail- able for real-time analytics as they are streamed into storage. Such scenarios pose challenges to state-of-the-art indexing methods, as they must not only support efficient queries but also frequent updates. We propose here a novel indexing method that ingests multi-dimensional observational data in real time. This method primarily guarantees extremely high throughput for data ingestion, while it can be continu- ously refined in the background to improve query efficiency. Instead of representing collections of points using Minimal Bounding Boxes as in conventional indexes, we model sets of successive points as line segments in hyperspaces, by exploiting the intrinsic value continuity in observational data. This representation reduces the number of index entries and drastically reduces “over-coverage” by entries. Experimental results show that our approach handles real-world workloads gracefully, providing both low-overhead indexing and excellent query efficiency.},
	number = {14},
	journal = {Proceedings of the VLDB Endowment},
	author = {Wang, Sheng and Maier, David and Ooi, Beng Chin},
	year = {2016},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1683--1694},
}

@article{Jayachandran2014,
	title = {Combining {User} {Interaction}, {Speculative} {Query} {Execution} and {Sampling} in the {DICE} {System}},
	volume = {7},
	issn = {21508097},
	url = {http://dx.doi.org/10.14778/2733004.2733064},
	doi = {10.14778/2733004.2733064},
	abstract = {The interactive exploration of data cubes has become a popular application, especially over large datasets. In this paper, we present DICE, a combination of a novel frontend query interface and distributed aggregation backend that enables interactive cube exploration. DICE provides a convenient, practical alternative to the typical offline cube materialization strategy by allowing the user to explore facets of the data cube, trading off accuracy for interactive response-times, by sampling the data. We consider the time spent by the user perusing the results of their current query as an opportunity to execute and cache the most likely followup queries. The frontend presents a novel intuitive interface that allows for sampling-aware aggregations, and encourages interaction via our proposed faceted model. The design of our backend is tailored towards the low-latency user interaction at the frontend, and vice-versa. We discuss the synergistic design behind both the frontend user experience and the backend architecture of DICE; and, present a demonstration that allows the user to fluidly interact with billion-tuple datasets within sub-second interactive response times.},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Jayachandran, Prasanth and Tunga, Karthik and Kamat, Niranjan and Nandi, Arnab},
	month = aug,
	year = {2014},
	note = {Publisher: VLDB Endowment},
	keywords = {RDistributed: Yes, RInteractive: Yes, UserStudy:yes, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution, ★},
	pages = {1697--1700},
}

@article{Kahng2016,
	title = {Interactive {Browsing} and {Navigation} in {Relational} {Databases}},
	volume = {9},
	issn = {21508097},
	url = {http://arxiv.org/abs/1603.02371},
	doi = {10.14778/2994509.2994520},
	abstract = {Although researchers have devoted considerable attention to helping database users formulate queries, many users still find it challenging to specify queries that involve joining tables. To help users construct join queries for exploring relational databases, we propose ETable, a novel presentation data model that provides users with a presentation-level interactive view. This view compactly presents one-to-many and many-to-many relationships within a single enriched table by allowing a cell to contain a set of entity references. Users can directly interact with this enriched table to incrementally construct complex queries and navigate databases on a conceptual entity-relationship level. In a user study, participants performed a range of database querying tasks faster with ETable than with a commercial graphical query builder. Subjective feedback about ETable was also positive. All participants found that ETable was easier to learn and helpful for exploring databases.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kahng, Minsuk and Navathe, Shamkant B. and Stasko, John T. and Chau, Duen Horng},
	year = {2016},
	note = {arXiv: 1603.02371},
	keywords = {RInteractive: Yes, UserStudy:yes, cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1017--1028},
}

@article{Neamtu:2016:ITS:3021924.3021933,
	title = {Interactive time series exploration powered by the marriage of similarity distances},
	volume = {10},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3021924.3021933},
	doi = {10.14778/3021924.3021933},
	abstract = {Finding similar trends among time series data is critical for applications ranging from financial planning to policy making. The detection of these multifaceted relationships, especially time warped matching of time series of different lengths and alignments is prohibitively expensive to compute. To achieve real time responsiveness on large time series datasets, we propose a novel paradigm called Online Exploration of Time Series (ONEX) employing a powerful one-time preprocessing step that encodes critical similarity relationships to support subsequent rapid data exploration. Since the encoding of a huge number of pairwise similarity relationships for all variable lengths time series segments is not feasible, our work rests on the important insight that clustering with inexpensive point-to-point distances such as the Euclidean Distance can support subsequent time warped matching. Our ONEX framework overcomes the prohibitive computational costs associated with a more robust elastic distance namely the DTW by applying it over the surprisingly compact knowledge base instead of the raw data. Our comparative study reveals that ONEX is up to 19\% more accurate and several times faster than the state-of-the-art. Beyond being a highly accurate and fast domain independent solution, ONEX offers a truly interactive exploration experience supporting novel time series operations.},
	number = {3},
	journal = {Proceedings of the VLDB Endowment},
	author = {Neamtu, Rodica and Ahsan, Ramoza and Rundensteiner, Elke and Sarkozy, Gabor},
	month = nov,
	year = {2016},
	note = {Publisher: VLDB Endowment},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {169--180},
}

@article{Liu:2016:KLM:3007263.3007266,
	title = {Kodiak: leveraging materialized views for very low-latency analytics over high-dimensional web-scale data},
	volume = {9},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3007263.3007266},
	doi = {10.14778/3007263.3007266},
	abstract = {Turn's online advertising campaigns produce petabytes of data. This data is composed of trillions of events, e.g. impressions, clicks, etc., spanning multiple years. In addition to a timestamp, each event includes hundreds of fields describing the user's attributes, campaign's attributes, attributes of where the ad was served, etc. Advertisers need advanced analytics to monitor their running campaigns' performance, as well as to optimize future campaigns. This involves slicing and dicing the data over tens of dimensions over arbitrary time ranges. Many of these queries need to power the web portal to provide reports and dashboards. For an interactive response time, they have to have tens of milliseconds latency. At Turn's scale of operations, no existing system was able to deliver this performance in a cost effective manner. Kodiak, a distributed analytical data platform for web-scale high-dimensional data, was built to serve this need. It relies on pre-computations to materialize thousands of views to serve these advanced queries. These views are partitioned and replicated across Kodiak's storage nodes for scalability and reliability. They are system maintained as new events arrive. At query time, the system auto-selects the most suitable view to serve each query. Kodiak has been used in production for over a year. It hosts 2490 views for over three petabytes of raw data serving over 200K queries daily. It has median and 99\% query latencies of 8 ms and 252 ms respectively. Our experiments show that its query latency is 3 orders of magnitude faster than leading big data platforms on head-to-head comparisons using Turn's query workload. Moreover, Kodiak uses 4 orders of magnitude less resources to run the same workload.},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Liu, Shaosu and Song, Bin and Gangam, Sriharsha and Lo, Lawrence and Elmeleegy, Khaled},
	month = sep,
	year = {2016},
	note = {Publisher: VLDB Endowment},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Evaluation Research},
	pages = {1269--1280},
}

@article{Idreos2011a,
	title = {Merging what's cracked, cracking what's merged},
	volume = {4},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2002938.2002944},
	doi = {10.14778/2002938.2002944},
	abstract = {Adaptive indexing is characterized by the partial creation and refinement of the index as side effects of query execution. Dynamic or shifting workloads may benefit from preliminary index structures focused on the columns and specific key ranges actually queried — without incurring the cost of full index construction. The costs and benefits of adaptive indexing techniques should therefore be compared in terms of initialization costs, the overhead imposed upon queries, and the rate at which the index converges to a state that is fully-refined for a particular workload component. Based on an examination of database cracking and adaptive merging, which are two techniques for adaptive indexing, we seek a hybrid technique that has a low initialization cost and also converges rapidly. We find the strengths and weaknesses of database cracking and adaptive merging complementary. One has a relatively high initialization cost but converges rapidly. The other has a low initialization cost but converges relatively slowly. We analyze the sources of their respective strengths and explore the space of hybrid techniques. We have designed and implemented a family of hybrid algorithms in the context of a column-store database system. Our experiments compare their behavior against database cracking and adaptive merging, as well as against both traditional full index lookup and scan of unordered data. We show that the new hybrids significantly improve over past methods while at least two of the hybrids come very close to the “ideal performance” in terms of both overhead per query and convergence to a final state.},
	number = {9},
	journal = {Proceedings of the VLDB Endowment},
	author = {Idreos, Stratos and Manegold, Stefan and Kuno, Harumi and Graefe, Goetz},
	year = {2011},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {586--597},
}

@article{Reeves:2009:MMT:1687627.1687639,
	title = {Managing {Massive} {Time} {Series} {Streams} with {Multi}-{Scale} {Compressed} {Trickles}},
	volume = {2},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?id=1687639},
	doi = {10.14778/1687627.1687639},
	abstract = {We present Cypress, a novel framework to archive and query massive time series streams such as those generated by sen- sor networks, data centers, and scientific computing. Cy- press applies multi-scale analysis to decompose time series and to obtain sparse representations in various domains (e.g. frequency domain and time domain). Relying on the spar- sity, the time series streams can be archived with reduced storage space. We then show that many statistical queries such as trend, histogram and correlations can be answered directly from compressed data rather than from reconstructed raw data. Our evaluation with server utilization data collected from real data centers shows significant benefit of our framework.},
	number = {1},
	journal = {Proceedings of the VLDB Endowment},
	author = {Reeves, Galen and Liu, Jie and Nath, Suman and Zhao, Feng},
	month = aug,
	year = {2009},
	note = {Publisher: VLDB Endowment},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {97--108},
}

@article{Mottin:2017:NTE:3137765.3137824,
	title = {New {Trends} on {Exploratory} {Methods} for {Data} {Analytics}},
	volume = {10},
	issn = {21508097},
	url = {https://doi.org/10.14778/3137765.3137824},
	doi = {10.14778/3137765.3137824},
	abstract = {Data usually comes in a plethora of formats and dimensions, rendering the exploration and information extraction processes cumbersome. Thus, being able to cast exploratory queries in the data with the intent of having an immediate glimpse on some of the data properties is becoming crucial. An exploratory query should be simple enough to avoid complicate declarative languages (such as SQL) and mechanisms, and at the same time retain the flexibility and expressiveness of such languages. Recently, we have witnessed a rediscovery of the so called example-based methods, in which the user, or the analyst circumvent query languages by using examples as input. An example is a representative of the intended results, or in other words, an item from the result set. Example-based methods exploit inherent characteristics of the data to infer the results that the user has in mind, but may not able to (easily) express. They can be useful both in cases where a user is looking for information in an unfamiliar dataset, or simply when she is exploring the data without knowing what to find in there. In this tutorial, we present an excursus over the main methods for exploratory analysis, with a particular focus on examplebased methods. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Mottin, Davide and Lissandrini, Matteo and Velegrakis, Yannis and Palpanas, Themis},
	month = aug,
	year = {2017},
	note = {Publisher: VLDB Endowment},
	pages = {1977--1980},
}

@article{Kraska2018,
	title = {Northstar: {An} {Interactive} {Data} {Science} {System}},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3229863.3240493},
	doi = {10.14778/3229863.3240493},
	abstract = {In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the "guts." Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the "protection" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kraska, Tim},
	month = aug,
	year = {2018},
	note = {Publisher: VLDB Endowment},
	keywords = {★},
	pages = {2150--2164},
}

@article{Schuhknecht:2015:SDS:2777598.2777602,
	title = {On the {Surprising} {Difficulty} of {Simple} {Things}: {The} {Case} of {Radix} {Partitioning}},
	volume = {8},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2777598.2777602},
	doi = {10.14778/2777598.2777602},
	abstract = {Partitioning a dataset into ranges is a task that is common in various applications such as sorting [1,6,7,8,9] and hashing [3] which are in turn building blocks for almost any type of query processing. Especially radix-based partitioning is very popular due to its simplicity and high performance over comparison-based versions [6].},
	number = {9},
	journal = {Proceedings of the VLDB Endowment},
	author = {Schuhknecht, Felix Martin and Khanchandani, Pankaj and Dittrich, Jens},
	month = may,
	year = {2015},
	note = {Publisher: VLDB Endowment},
	pages = {934--937},
}

@article{KimAlbert2015,
	title = {Rapid {Sampling} for {Visualizations} with {Ordering} {Guarantees}},
	volume = {8},
	issn = {21508097},
	doi = {10.14778/2735479.2735485},
	abstract = {Visualizations are frequently used as a means to understand trends and gather insights from datasets, but often take a long time to generate. In this paper, we focus on the problem of rapidly generating approximate visualizations while preserving crucial visual proper- ties of interest to analysts. Our primary focus will be on sampling algorithms that preserve the visual property of ordering; our techniques will also apply to some other visual properties. For instance, our algorithms can be used to generate an approximate visualization of a bar chart very rapidly, where the comparisons between any two bars are correct. We formally show that our sampling algorithms are generally applicable and provably optimal in theory, in that they do not take more samples than necessary to generate the visualizations with ordering guarantees. They also work well in practice, correctly ordering output groups while taking orders of magnitude fewer samples and much less time than conventional sampling schemes.},
	number = {5},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kim, Albert and Blais, Eric and Parameswaran, Aditya and Indyk, Piotr and Madden, Sam and Rubinfeld, Ronitt},
	year = {2015},
	note = {arXiv: 1412.3040},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {521--532},
}

@article{Li:2015:QEI:2831360.2831369,
	title = {Query from examples: {An} iterative, data-driven approach to query construction},
	volume = {8},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?id=2831369},
	doi = {10.14778/2831360.2831369},
	abstract = {In this paper, we propose a new approach, called Query from Examples (QFE), to help non-expert database users construct SQL queries. Our approach, which is designed for users who might be unfamiliar with SQL, only requires that the user is able to determine whether a given output table is the result of his or her intended query on a given input database. To kick-start the construction of a target query Q, the user first provides a pair of inputs: a sample database D and an output table R which is the result of Q on D. As there will be many candidate queries that transform D to R, QFE winnows this collection by presenting the user with new database-result pairs that distinguish these candidates. Unlike previous approaches that use synthetic data for such pairs, QFE strives to make these distinguishing pairs as close to the original (D, R) pair as possible. By doing so, it seeks to minimize the effort needed by a user to determine if a new database-result pair is consistent with his or her desired query. We demonstrate the effectiveness and efficiency of our approach using real datasets from SQLShare, a cloud-based platform designed to help scientists utilize RDBMS technology for data analysis.},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Hao and Chan, Chee-Yong and Maier, David},
	month = sep,
	year = {2015},
	note = {Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, UserStudy:yes, cluster:Automatic Exploration, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {2158--2169},
}

@article{Galakatos2017,
	title = {Revisiting {Reuse} for {Approximate} {Query} {Processing}},
	volume = {10},
	issn = {21508097},
	doi = {10.14778/3115404.3115418},
	abstract = {Visual data exploration tools allow users to quickly gather insights from new datasets. As dataset sizes continue to in-crease, though, new techniques will be necessary to maintain the interactivity guarantees that these tools require. Approximate query processing (AQP) attempts to tackle this problem and allows systems to return query results at "human speed. " However, existing AQP techniques start to break down when confronted with ad hoc queries that target the tails of the distribution. We therefore present an AQP formulation that can provide low-error approximate results at interactive speeds, even for queries over rare subpopulations. In particular, our formulation treats query results as random variables in order to leverage the ample opportunities for result reuse inherent in interactive data exploration. As part of our approach, we apply a variety of optimization techniques that are based on probability theory, including new query rewrite rules and index structures. We implemented these techniques in a prototype system and show that they can achieve interactivity where alternative approaches cannot.},
	number = {1},
	journal = {Proceedings of the VLDB Endowment},
	author = {Galakatos, Alex and Crotty, Andrew and Zgraggen, Emanuel and Binnig, Carsten and Kraska, Tim},
	month = jun,
	year = {2017},
	note = {Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1142--1153},
}

@article{Tauheed:2012:SPL:2350229.2350267,
	title = {{SCOUT}: {Prefetching} for {Latent} {Structure} {Following} {Queries}},
	volume = {5},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2350229.2350267},
	doi = {10.14778/2350229.2350267},
	abstract = {Today's scientists are quickly moving from in vitro to in silico experimentation: they no longer analyze natural phenomena in a petri dish, but instead they build models and simulate them. Managing and analyzing the massive amounts of data involved in simulations is a major task. Yet, they lack the tools to efficiently work with data of this size. One problem many scientists share is the analysis of the massive spatial models they build. For several types of analysis they need to interactively follow the structures in the spatial model, e.g., the arterial tree, neuron fibers, etc., and issue range queries along the way. Each query takes long to execute, and the total time for executing a sequence of queries significantly delays data analysis. Prefetching the spatial data reduces the response time considerably, but known approaches do not prefetch with high accuracy. We develop SCOUT, a structure-aware method for prefetching data along interactive spatial query sequences. SCOUT uses an approximate graph model of the structures involved in past queries and attempts to identify what particular structure the user follows. Our experiments with neuro-science data show that SCOUT prefetches with an accuracy from 71\% to 92\%, which translates to a speedup of 4x-15x. SCOUT also improves the prefetching accuracy on datasets from other scientific domains, such as medicine and biology.},
	number = {11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Tauheed, Farhan and Heinis, Thomas and Schürmann, Felix and Markram, Henry and Ailamaki, Anastasia},
	month = jul,
	year = {2012},
	note = {Publisher: VLDB Endowment},
	keywords = {RInteractive: Yes, cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1531--1542},
}

@article{Kalinin2015,
	title = {Searchlight: {Enabling} {Integrated} {Search} and {Exploration} over {Large} {Multidimensional} {Data}},
	issn = {21508097},
	doi = {10.14778/2794367.2794378},
	abstract = {We present a new system, called Searchlight, that uniquely integrates constraint solving and data management tech-niques. It allows Constraint Programming (CP) machinery to run efficiently inside a DBMS without the need to extract, transform and move the data. This marriage concurrently offers the rich expressiveness and efficiency of constraint-based search and optimization provided by modern CP solvers, and the ability of DBMSs to store and query data at scale, resulting in an enriched functionality that can effectively support both data-and search-intensive applications. As such, Searchlight is the first system to support generic search, exploration and mining over large multi-dimensional data collections, going beyond point algorithms designed for point search and mining tasks. Searchlight makes the following scientific contributions: • Constraint solvers as first-class citizens Instead of treating solver logic as a black-box, Searchlight provides native support, incorporating the necessary APIs for its specification and transparent execution as part of query plans, as well as novel algorithms for its optimized exe-cution and parallelization. • Speculative solving Existing solvers assume that the entire data set is main-memory resident. Searchlight uses an innovative two stage Solve-Validate approach that allows it to operate speculatively yet safely on main-memory synopses, quickly producing candidate search re-sults that can later be efficiently validated on real data. • Computation and I/O load balancing As CP solver logic can be computationally expensive, executing it on large search and data spaces requires novel CPU-I/O bal-ancing approaches when performing search distribution. We built a prototype implementation of Searchlight on Google's Or-Tools, an open-source suite of operations research tools, and the array DBMS SciDB. Extensive experimental results show that Searchlight often performs orders of magnitude faster than the next best approach (SciDB-only or CP-solver-only) in terms of end response time and time to first result.},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kalinin, Alexander and Zdonik, Stan},
	year = {2015},
	keywords = {Astronomy, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {1094--1105},
}

@article{Parameswaran:2013:SVD:2732240.2732250,
	title = {{SeeDB}: {Visualizing} {Database} {Queries} {Efficiently}},
	volume = {7},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2732240.2732250},
	doi = {10.14778/2732240.2732250},
	abstract = {Data scientists rely on visualizations to interpret the data returned by queries, but finding the right visualization remains amanual task that is oſten laborious.We propose a DBMS that partially automates the task of finding the right visualizations for a query. In a nutshell, given an input query Q, the new DBMS optimizer will explore not only the space of physical plans forQ, but also the space of possible visualizations for the results of Q.Te output will comprise a recommendation of potentially “interesting” or “useful” visualizations, where each visualization is coupled with a suitable query execution plan. We discuss the technical challenges in building this system and outline an agenda for future research.},
	number = {4},
	journal = {Proceedings of the VLDB Endowment},
	author = {Parameswaran, Aditya and Polyzotis, Neoklis and Garcia-Molina, Hector},
	month = dec,
	year = {2013},
	note = {Publisher: VLDB Endowment},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {325--328},
}

@article{Sun2016,
	title = {Skipping-oriented {Partitioning} for {Columnar} {Layouts}},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3025111.3025123},
	doi = {10.14778/3025111.3025123},
	abstract = {As data volumes continue to grow, modern database systems increasingly rely on data skipping mechanisms to improve perfor-mance by avoiding access to irrelevant data. Recent work [39] proposed a fine-grained partitioning scheme that was shown to improve the opportunities for data skipping in row-oriented systems. Modern analytics and big data systems increasingly adopt columnar storage schemes, and in such systems, a row-based approach misses important opportunities for further improving data skipping. The flexibility of column-oriented organizations, however, comes with the additional cost of tuple reconstruction. In this paper, we develop Generalized Skipping-Oriented Partitioning (GSOP), a novel hybrid data skipping framework that takes into account these row-based and column-based tradeoffs. In contrast to previous column-oriented physical design work, GSOP considers the tradeoffs between horizontal data skipping and vertical partitioning jointly. Our experiments using two public benchmarks and a real-world work-load show that GSOP can significantly reduce the amount of data scanned and improve end-to-end query response times over the state-of-the-art techniques.},
	number = {4},
	journal = {Proceedings of the VLDB Endowment},
	author = {Sun, Liwen and Franklin, Michael J and Wang, Jiannan and Wu, Eugene},
	month = nov,
	year = {2016},
	note = {Publisher: VLDB Endowment},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {421--432},
}

@article{Olma2017,
	title = {Slalom : {Coasting} {Through} {Raw} {Data} via {Adaptive} {Partitioning} and {Indexing}},
	volume = {10},
	issn = {21508097},
	url = {https://dl.acm.org/citation.cfm?id=3115415},
	abstract = {The constant flux of data and queries alike has been pushing the boundaries of data analysis systems. The increasing size of raw data files has made data loading an expensive operation that delays the data-to-insight time. Hence, recent in-situ query processing systems operate directly over raw data, alleviating the loading cost. At the same time, analytical workloads have increasing number of queries. Typically, each query focuses on a constantly shifting – yet small – range. Minimizing the workload latency, now, requires the benefits of indexing in in-situ query processing. In this paper, we present Slalom, an in-situ query engine that accommodates workload shifts by monitoring user access patterns. Slalom makes on-the-fly partitioning and indexing decisions, based on information collected by lightweight monitoring. Slalom has two key components: (i) an online partitioning and indexing scheme, and (ii) a partitioning and indexing tuner tailored for in-situ query engines. When compared to the state of the art, Slalom offers per-formance benefits by taking into account user query patterns to (a) logically partition raw data files and (b) build for each partition lightweight partition-specific indexes. Due to its lightweight and adaptive nature, Slalom achieves efficient accesses to raw data with minimal memory consumption. Our experimentation with both micro-benchmarks and real-life workloads shows that Slalom out-performs state-of-the-art in-situ engines (3 − 10×), and achieves comparable query response times with fully indexed DBMS, offer-ing much lower (∼ 3×) cumulative query execution times for query workloads with increasing size and unpredictable access patterns.},
	number = {10},
	journal = {Proceedings of the VLDB Endowment},
	author = {Olma, Matthaios and Karpathiotakis, Manos and Alagiannis, Ioannis and Athanassoulis, Manos and Ailamaki, Anastasia},
	year = {2017},
	keywords = {RRaw: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {1106--1117},
}

@article{Jiang2015,
	title = {{SnapToQuery}: {Providing} {Interactive} {Feedback} during {Exploratory} {Query} {Specification}},
	volume = {8},
	issn = {21508097},
	doi = {10.14778/2809974.2809986},
	abstract = {A critical challenge in the data exploration process is discovering and issuing the " right " query, especially when the space of possible queries is large. This problem of exploratory query specification is exacerbated by the use of interactive user interfaces driven by mouse, touch, or next-generation, three-dimensional, motion capture-based devices; which, are often imprecise due to jitter and sensitivity issues. In this paper, we propose SnapToQuery, a novel technique that guides users through the query space by providing interactive feedback during the query specification process by " snapping " to the user's likely intended queries. These in-tended queries can be derived from prior query logs, or from the data itself, using methods described in this pa-per. In order to provide interactive response times over large datasets, we propose two data reduction techniques when snapping to these queries. Performance experiments demonstrate that our algorithms help maintain an interac-tive experience while allowing for accurate guidance. User studies over three kinds of devices (mouse, touch, and mo-tion capture) show that SnapToQuery can help users specify queries quicker and more accurately; resulting in a query specification time speedup of 1.4× for mouse and touch-based devices and 2.2× for motion capture-based devices.},
	number = {11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Jiang, Lilong and Nandi, Arnab},
	year = {2015},
	keywords = {RInteractive: Yes, UserStudy:yes, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1250--1261},
}

@article{WangLu2015,
	title = {Spatial online sampling and aggregation},
	volume = {9},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2850583.2850584},
	doi = {10.14778/2850583.2850584},
	abstract = {The massive adoption of smart phones and other mobile devices has generated humongous amount of spatial and spatio-temporal data. The importance of spatial analytics and aggregation is ever-increasing. An important challenge is to support interactive explo-ration over such data. However, spatial analytics and aggregation using all data points that satisfy a query condition is expensive, especially over large data sets, and could not meet the needs of interactive exploration. To that end, we present novel indexing structures that support spatial online sampling and aggregation on large spatial and spatio-temporal data sets. In spatial online sam-pling, random samples from the set of spatial (or spatio-temporal) points that satisfy a query condition are generated incrementally in an online fashion. With more and more samples, various spatial analytics and aggregations can be performed in an online, interactive fashion, with estimators that have better accuracy over time. Our design works well for both memory-based and disk-resident data sets, and scales well towards different query and sample sizes. More importantly, our structures are dynamic, hence, they are able to deal with insertions and deletions efficiently. Extensive experiments on large real data sets demonstrate the improvements achieved by our indexing structures compared to other baseline methods.},
	number = {3},
	journal = {Proceedings of the VLDB Endowment},
	author = {Wang, Lu and Christensen, Robert and Li, Feifei and Yi, Ke},
	year = {2015},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {84--95},
}

@article{Krishnan:2015:SVC:2824032.2824037,
	title = {Stale {View} {Cleaning}: {Getting} {Fresh} {Answers} from {Stale} {Materialized} {Views}},
	volume = {8},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2824032.2824037},
	doi = {10.14778/2824032.2824037},
	abstract = {Materialized views (MVs), stored pre-computed results, are widely used to facilitate fast queries on large datasets. When new records arrive at a high rate, it is infeasible to continuously update (maintain) MVs and a common solution is to defer maintenance by batching updates together. Between batches the MVs become increasingly stale with incorrect, missing, and superfluous rows leading to increasingly inaccurate query results. We propose Stale View Cleaning (SVC) which addresses this problem from a data cleaning perspective. In SVC, we efficiently clean a sample of rows from a stale MV, and use the clean sample to estimate aggregate query results. While approximate, the estimated query results reflect the most recent data. As sampling can be sensitive to long-tailed distributions, we further explore an outlier indexing technique to give increased accuracy when the data distributions are skewed. SVC complements existing deferred maintenance approaches by giving accurate and bounded query answers between maintenance. We evaluate our method on a generated dataset from the TPC-D benchmark and a real video distribution application. Experiments confirm our theoretical results: (1) cleaning an MV sample is more efficient than full view maintenance, (2) the estimated results are more accurate than using the stale MV, and (3) SVC is applicable for a wide variety of MVs.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Krishnan, Sanjay and Wang, Jiannan and Franklin, Michael J and Goldberg, Ken and Kraska, Tim},
	month = aug,
	year = {2015},
	note = {Publisher: VLDB Endowment},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {1370--1381},
}

@article{Halim2012,
	title = {Stochastic {Database} {Cracking}: {Towards} {Robust} {Adaptive} {Indexing} in {Main}-{Memory} {Column}-{Stores}},
	issn = {2150-8097},
	url = {https://dl.acm.org/citation.cfm?doid=2168651.2168652},
	doi = {10.14778/2168651.2168652},
	abstract = {Modern business applications and scientific databases call for in- herently dynamic data storage environments. Such environments are characterized by two challenging features: (a) they have lit- tle idle system time to devote on physical design; and (b) there is little, if any, a priori workload knowledge, while the query and data workload keeps changing dynamically. In such environments, traditional approaches to index building and maintenance cannot apply. Database cracking has been proposed as a solution that allows on-the-fly physical data reorganization, as a collateral effect of query processing. Cracking aims to continuously and automatically adapt indexes to theworkload at hand, without human intervention. Indexes are built incrementally, adaptively, and on demand. Nevertheless, as we show, existing adaptive indexing methods fail to deliver workload-robustness; they perform much better with random workloads than with others. This frailty derives from the inelasticity with which these approaches interpret each query as a hint on how data should be stored. Current cracking schemes blindly reorganize the data within each query’s range, even if that results into successive expensive operations with minimal indexing benefit. In this paper, we introduce stochastic cracking, a significantly more resilient approach to adaptive indexing. Stochastic cracking also uses each query as a hint on how to reorganize data, but not blindly so; it gains resilience and avoids performance bottlenecks by deliberately applying certain arbitrary choices in its decision- making. Thereby, we bring adaptive indexing forward to a ma- ture formulation that confers the workload-robustness previous ap- proaches lacked. Our extensive experimental study verifies that stochastic cracking maintains the desired properties of original da- tabase cracking while at the same time it performs well with diverse realistic workloads.},
	journal = {Proceedings of the VLDB Endowment},
	author = {Halim, Felix and Idreos, Stratos and Karras, Panagiotis and Yap, Roland},
	year = {2012},
	note = {arXiv: 1203.0055},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {502--513},
}

@article{Schuhknecht2013,
	title = {The uncracked pieces in database cracking},
	volume = {7},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2732228.2732229},
	doi = {10.14778/2732228.2732229},
	abstract = {Database cracking has been an area of active research in recent years. The core idea of database cracking is to create indexes adaptively and incrementally as a side-product of query process- ing. Several works have proposed different cracking techniques for different aspects including updates, tuple-reconstruction, convergence, concurrency-control, and robustness. However, there is a lack of any comparative study of these different methods by an independent group. In this paper, we conduct an experimental study on database cracking. Our goal is to critically review several aspects, identify the potential, and propose promising directions in database cracking. With this study, we hope to expand the scope of database cracking and possibly leverage cracking in database engines other than MonetDB. We repeat several prior database cracking works including the core cracking algorithms as well as three other works on convergence (hybrid cracking), tuple-reconstruction (sideways cracking), and robustness (stochastic cracking) respectively. We evaluate these works and show possible directions to do even better. We further test cracking under a variety of experimental settings, including high selectivity queries, low selectivity queries, and multiple query access patterns. Finally, we compare cracking against dif- ferent sorting algorithms as well as against different main-memory optimised indexes, including the recently proposed Adaptive Radix Tree (ART). Our results show that: (i) the previously proposed cracking algorithms are repeatable, (ii) there is still enough room to significantly improve the previously proposed cracking algorithms, (iii) cracking depends heavily on query selectivity, (iv) cracking needs to catch up with modern indexing trends, and (v) different indexing algorithms have different indexing signatures.},
	number = {2},
	journal = {Proceedings of the VLDB Endowment},
	author = {Schuhknecht, Felix Martin and Jindal, Alekh and Dittrich, Jens},
	year = {2013},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Validation Research},
	pages = {97--108},
}

@article{Yu:2016:TBO:3025111.3025120,
	title = {Two {Birds}, {One} {Stone}: {A} {Fast}, {Yet} {Lightweight}, {Indexing} {Scheme} for {Modern} {Database} {Systems}},
	volume = {10},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3025111.3025120},
	doi = {10.14778/3025111.3025120},
	abstract = {Classic database indexes (e.g., B+-Tree), though speed up queries, suffer from two main drawbacks: (1) An index usually yields 5\% to 15\% additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on modern storage devices. (2) Maintaining an index incurs high latency because the DBMS has to locate and update those index pages affected by the underlying table changes. This paper proposes Hippo a fast, yet scalable, database indexing approach. It significantly shrinks the index storage and mitigates maintenance overhead without compromising much on the query execution performance. Hippo stores disk page ranges instead of tuple pointers in the indexed table to reduce the storage space occupied by the index. It maintains simplified histograms that represent the data distribution and adopts a page grouping technique that groups contiguous pages into page ranges based on the similarity of their index key attribute distributions. When a query is issued, Hippo leverages the page ranges and histogram-based page summaries to recognize those pages such that their tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages. Experiments based on real and synthetic datasets show that Hippo occupies up to two orders of magnitude less storage space than that of the B+-Tree while still achieving comparable query execution performance to that of the B+-Tree for 0.1\% -- 1\% selectivity factors. Also, the experiments show that Hippo outperforms BRIN (Block Range Index) in executing queries with various selectivity factors. Furthermore, Hippo achieves up to three orders of magnitude less maintenance overhead and up to an order of magnitude higher throughput (for hybrid query/update workloads) than its counterparts.},
	number = {4},
	journal = {Proceedings of the VLDB Endowment},
	author = {Yu, Jia and Sarwat, Mohamed},
	month = nov,
	year = {2016},
	note = {Publisher: VLDB Endowment},
	pages = {385--396},
}

@article{Chen:2010:CHP:1920841.1921020,
	title = {Cheetah: a high performance, custom data warehouse on top of {MapReduce}},
	volume = {3},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=1920841.1921020},
	doi = {10.14778/1920841.1921020},
	abstract = {Large-scale data analysis has become increasingly impor- tant for many enterprises. Recently, a new distributed com- puting paradigm, called MapReduce, and its open source implementation Hadoop, has been widely adopted due to its impressive scalability and flexibility to handle structured as well as unstructured data. In this paper, we describe our data warehouse system, called Cheetah, built on top of MapReduce. Cheetah is designed specifically for our online advertising application to allow various simplifications and custom optimizations. First, we take a fresh look at the data warehouse schema design. In particular, we define a virtual view on top of the common star or snowflake data warehouse schema. This virtual view abstraction not only allows us to design a SQL-like but much more succinct query language, but also makes it easier to support many advanced query processing features. Next, we describe a stack of optimization techniques ranging from data compression and access method to multi-query optimization and exploiting materialized views. In fact, each node with commodity hardware in our cluster is able to process raw data at 1GBytes/s. Lastly, we show how to seamlessly integrate Cheetah into any ad- hoc MapReduce jobs. This allows MapReduce developers to fully leverage the power of both MapReduce and data warehouse technologies.},
	number = {1-2},
	journal = {Proceedings of the VLDB Endowment},
	author = {Chen, Songting},
	month = sep,
	year = {2010},
	note = {Publisher: VLDB Endowment
ISBN: 2150-8097},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1459--1468},
}

@article{Klonatos2014,
	title = {Building efficient query engines in a high-level language},
	volume = {7},
	issn = {21508097},
	url = {https://dl.acm.org/citation.cfm?doid=2732951.2732959},
	doi = {10.14778/2732951.2732959},
	abstract = {In this paper we advocate that it is time for a radical rethinking of database systems design. Developers should be able to leverage high-level programming languages without having to pay a price in efficiency. To realize our vision of abstraction without regret, we present LegoBase, a query engine written in the high-level programming language Scala. The key technique to regain efficiency is to apply generative programming: the Scala code that constitutes the query engine, despite its high-level appearance, is actually a program generator that emits specialized, low-level C code. We show how the combination of high-level and generative program- ming allows to easily implement a wide spectrum of optimizations that are difficult to achieve with existing low-level query compilers, and how it can continuously optimize the query engine. We evaluate our approach with the TPC-H benchmark and show that: (a) with all optimizations enabled, our architecture signifi- cantly outperforms a commercial in-memory database system as well as an existing query compiler, (b) these performance improve- ments require programming just a few hundred lines of high-level code instead of complicated low-level code that is required by exist- ing query compilers and, finally, that (c) the compilation overhead is low compared to the overall execution time, thus making our ap- proach usable in practice for efficiently compiling query engines.},
	number = {10},
	journal = {Proceedings of the VLDB Endowment},
	author = {Klonatos, Yannis and Koch, Christoph and Rompf, Tiark and Chafi, Hassan},
	year = {2014},
	note = {arXiv: 1612.05566v1},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {853--864},
}

@inproceedings{Kersten2011,
	title = {The {Researcher}’s {Guide} to the {Data} {Deluge}: {Querying} a {Scientific} {Database} in {Just} a {Few} {Seconds}},
	doi = {10.1145/1409360.1409380},
	abstract = {There is a clear need for interactive exploration of extremely large databases, especially in the area of scientific data management where ingestion of multiple Terabytes on a daily basis is foreseen. Unfortunately, current data management technology is not well-suited for such overwhelming demands. In light of these challenges, we should rethink some of the strict requirements database systems adopted in the past. We envision that next generation database systems should interpret queries by their intent , rather than as a contract carved in stone for complete and correct answers. The result set should aid the user in un-derstanding the database’s content and provide guidance to con-tinue the data exploration journey. A scientist can stepwise explore deeper and deeper into the database, and stop when the result content and quality reaches his satisfaction point. At the same time, response times should be close to instant such that they allow a scientist tointeract with the system and explore the data in a con-textualized way. Several research directions are carved out to realize this vision. They range from engineering a novel database kernel where speed rather than completeness is the first class citizen, up to refusing to process a costly query in the first place, but providing advice on how to reformulate it instead, or even providing alternatives the system believes might be relevant for the exploration patterns ob-served.},
	booktitle = {Proceedings of the {VLDB} {Endowment}},
	author = {Kersten, M and Idreos, S and Manegold, S and Liarou, E},
	year = {2011},
	note = {ISSN: 21508097},
	keywords = {layer:Database Layer, type:Philosophical Paper, ★},
	pages = {1474},
}

@article{Gray2002,
	title = {Data {Mining} the {SDSS} {SkyServer} {Database}},
	url = {http://arxiv.org/abs/cs.DB/0202014},
	doi = {10.48550/arXiv.cs/0202014},
	journal = {Distributed Data and Structures 4: Records of the 4th International Meeting},
	author = {Gray, Jim and Szalay, Alexander S and Thakar, Ani and Kunszt, Peter Z and Stoughton, Christopher and Slutz, Donald R and VandenBerg, Jan},
	year = {2002},
	keywords = {Interesting, Use Cases},
	pages = {189--210},
}

@article{Gheraibia2019,
	title = {Safety + {AI}: {A} novel approach to update safety models using artificial intelligence},
	volume = {7},
	doi = {10.1109/ACCESS.2019.2941566},
	journal = {IEEE Access},
	author = {Gheraibia, Youcef and Kabir, Sohag and Aslansefat, Koorosh and Sorokos, Ioannis and Papadopoulos, Yiannis},
	year = {2019},
	pages = {135855--135869},
}

@misc{zenodo,
	title = {Zenodo},
	url = {https://www.zenodo.org/},
	language = {en},
	publisher = {CERN},
	author = {{European Organization For Nuclear Research} and {OpenAIRE}},
	year = {2013},
	doi = {10.25495/7GXK-RD71},
	keywords = {Dataset, FOS: Physical sciences, Publication},
}

@inproceedings{Dursch2019,
	address = {New York, NY, USA},
	series = {{CIKM} ’19},
	title = {Inclusion dependency discovery: {An} experimental evaluation of thirteen algorithms},
	isbn = {978-1-4503-6976-3},
	url = {https://doi.org/10.1145/3357384.3357916},
	doi = {10.1145/3357384.3357916},
	booktitle = {Proceedings of the 28th {ACM} international {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Dürsch, Falco and Stebner, Axel and Windheuser, Fabian and Fischer, Maxi and Friedrich, Tim and Strelow, Nils and Bleifuß, Tobias and Harmouch, Hazar and Jiang, Lan and Papenbrock, Thorsten and Naumann, Felix},
	year = {2019},
	keywords = {Basics, Interesting, data profiling, evaluation, inclusion dependency discovery},
	pages = {219--228},
}

@article{Zhang2010,
	title = {On multi-column foreign key discovery},
	volume = {3},
	issn = {2150-8097},
	doi = {10.14778/1920841.1920944},
	number = {1–2},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhang, Meihui and Hadjieleftheriou, Marios and Ooi, Beng Chin and Procopiuc, Cecilia M. and Srivastava, Divesh},
	month = sep,
	year = {2010},
	keywords = {Interesting, to-read},
	pages = {805--814},
}

@article{bell2009beyond,
	title = {Beyond the data deluge},
	volume = {323},
	doi = {10.1126/science.1170411},
	number = {5919},
	journal = {Science (New York, N.Y.)},
	author = {Bell, Gordon and Hey, Tony and Szalay, Alex},
	year = {2009},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1297--1298},
}

@phdthesis{alvarez-ayllon_navigating_2023,
	title = {Navigating {Diverse} {Datasets} in the {Face} of {Uncertainty}},
	school = {Universidad de Cádiz},
	author = {Alvarez-Ayllon, Alejandro},
	year = {2023},
}

@misc{PresQ,
	title = {{PresQ}: {Discovery} of {Multidimensional} {Equally}- {Distributed} {Dependencies} {Via} {Quasi}-{Cliques} on {Hypergraph} ({Source})},
	url = {https://doi.org/10.5281/zenodo.6865856},
	publisher = {Zenodo},
	author = {Alvarez-Ayllon, Alejandro},
	month = jul,
	year = {2022},
	doi = {10.5281/zenodo.6865856},
}

@misc{SOMA,
	title = {{SOMA}: {Self}-{Organizing} {Map} {Analysis} ({Source})},
	url = {https://doi.org/10.5281/zenodo.7452720},
	publisher = {Zenodo},
	author = {Alvarez-Ayllon, Alejandro},
	month = dec,
	year = {2022},
	doi = {10.5281/zenodo.7452720},
}

@unpublished{Alvarez2022SOM,
	title = {Two-sample test based on {Self}-{Organizing} {Maps}},
	url = {https://arxiv.org/abs/2212.08960},
	abstract = {Machine-learning classifiers can be leveraged as a two-sample statistical test. Suppose each sample is assigned a different label and that a classifier can obtain a better-than-chance result discriminating them. In this case, we can infer that both samples originate from different populations. However, many types of models, such as neural networks, behave as a black-box for the user: they can reject that both samples originate from the same population, but they do not offer insight into how both samples differ. Self-Organizing Maps are a dimensionality reduction initially devised as a data visualization tool that displays emergent properties, being also useful for classification tasks. Since they can be used as classifiers, they can be used also as a two-sample statistical test. But since their original purpose is visualization, they can also offer insights.},
	author = {Alvarez-Ayllon, Alejandro and Palomo-Duarte, Manuel and Dodero, Juan-Manuel},
	year = {2022},
	note = {arXiv: 2212.08960 [cs.LG]},
}

@misc{noauthor_programming_nodate,
	title = {Programming {Languages} — {C} ++ 20},
	publisher = {ISO},
}

@article{GaoHypergraph2022,
	title = {Hypergraph learning: {Methods} and practices},
	volume = {44},
	doi = {10.1109/TPAMI.2020.3039374},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gao, Yue and Zhang, Zizhao and Lin, Haojie and Zhao, Xibin and Du, Shaoyi and Zou, Changqing},
	year = {2022},
	pages = {2548--2566},
}

@incollection{Bretto2013,
	address = {Heidelberg},
	title = {Applications of hypergraph theory: {A} brief overview},
	isbn = {978-3-319-00080-0},
	url = {https://doi.org/10.1007/978-3-319-00080-0_7},
	abstract = {Like in most fruitful mathematical theories, the theory of hypergraphs has many applications. Hypergraphs model many practical problems in many different sciences. it makes very little time (20 years) that the theory of hypergraphs is used to model situations in the applied sciences. We find this theory in psychology, genetics, \$\${\textbackslash}ldots \$\$…but also in various human activities. Hypergraphs have shown their power as a tool to understand problems in a wide variety of scientific field. Moreover it well known now that hypergraph theory is a very useful tool to resolve optimization problems such as scheduling problems, location problems and so on. This chapter shows some possible uses of hypergraphs in Applied Sciences.},
	booktitle = {Hypergraph theory: {An} introduction},
	publisher = {Springer International Publishing},
	author = {Bretto, Alain},
	year = {2013},
	doi = {10.1007/978-3-319-00080-0_7},
	pages = {111--116},
}

@misc{conroy_three_2020,
	title = {Three reasons to share your research failures},
	url = {https://www.nature.com/nature-index/news-blog/three-reasons-to-share-your-research-science-failures},
	journal = {Nature Index},
	author = {Conroy, Gemma},
	month = sep,
	year = {2020},
}

@article{Mozafari2017Snappy,
	title = {{SnappyData} : {A} {Unified} {Cluster} for {Streaming} , {Transactions} , and {Interactive} {Analytics}},
	issn = {16130073},
	url = {https://dl.acm.org/citation.cfm?id=2933295},
	doi = {10.1145/2933267.2933295},
	abstract = {Many modern applications are a mixture of streaming, transactional and analytical workloads. However, traditional data platforms are each designed for supporting a specific type of workload. The lack of a single platform to support all these workloads has forced users to combine disparate products in custom ways. The common practice of stitching heterogeneous environments has caused enormous production woes by increasing complexity and the total cost of ownership. To support this class of applications, we present SnappyData as the first unified engine capable of delivering analytics, transactions, and stream processing in a single integrated cluster. We build this hybrid engine by carefully marrying a big data computational engine (Apache Spark) with a scale-out transactional store (Apache GemFire). We study and address the challenges involved in building such a hybrid distributed system with two conflicting compo-nents designed on drastically different philosophies: one being a lineage-based computational model designed for high-throughput analytics, the other a consensus-and replication-based model designed for low-latency operations.},
	journal = {Cidr},
	author = {Mozafari, Barzan and Ramnarayan, Jags and Menon, Sudhir},
	year = {2017},
	note = {arXiv: 1603.07016v1
ISBN: 9781450321389},
	keywords = {cluster:Middleware, layer:Middleware, supercluster:Middleware, type:Proposal of Solution},
	pages = {1--8},
}

@phdthesis{10498/24011,
	title = {Análisis de eventos para evaluar competencias en experiencias de aprendizaje basadas en serious games},
	url = {http://hdl.handle.net/10498/24011},
	abstract = {Nowadays, the attention of higher education learning has focused on acquiring the skills by students, instead of the traditional getting of knowledge. A relevant aspect in the skill assessment is the analysis of the process conducted during the learning experience. Whereas performance outcomes can be easily measured and observed, the followed processes to achieve those outcomes are often not as easily measurable or observable with manual approaches. Serious games-based learning experiences have taken relevance due to the grown of video games industry in recent years. Serious games or educational video games are video games with learning purposes. Playing in these environments, players develop their skills applying several interactions to overcome the challenges posed in the game. These interactions can be stored in event logs as large data sets that can provide the assessment process with objective information about the applied skills. Unfortunately, serious games-based learning experiences have particular complexities when it comes to assessment. First, generic factors (i.e. game context: platforms, adventure, etc.) and specific ones (i.e. educational context: higher education, corporate training, etc.) involve complexities in the assessment process. Second, most assessment methods identified so far in serious games-based learning experiences rely on manual approaches, suffering from scalability issues, scarce detailed assessments and a lack of automatic mechanisms. All these limitations affect the analysis of large data sets as the generated events by the players interactions. Due to the sequential nature of events generated by the game interactions and the detected assessment limitations, sequence analysis techniques can be considered as an alternative to assess the applied skills in serious games-based learning experiences. Within sequence analysis, process mining techniques allow automatically extracting knowledge from the event logs. Process mining has been successfully applied in multiple educational environments, coining its own term for these kinds of applications: Educational Process Mining (EPM). In this thesis, a method based on process mining techniques to analyse the interactions produced by the students in a serious games-based learning experience is proposed. First, a preliminary skill assessment method is introduced. Then, several studies are conducted to explore the benefits of applying process mining techniques in learning experiences. Finally, considering obtained results, a refined proposal for the skill assessment method is proposed, being implemented and validated in a serious games-based learning experience in higher education. The findings show that process mining techniques provide a suitable method to alleviate limitations of manual assessment in game-based learning experiences. Process mining supports a scalable and detailed analysis of performed interactions. Applying the process mining model discovery was a successful approach to analyse the behaviour of students in sequential processes. Finally, process mining techniques provided an automatic support to assess the developed skills during the game experience.},
	author = {Caballero Hernández, Juan Antonio},
	month = nov,
	year = {2020},
	keywords = {Applied games, Edutainment, Event-Based Data Analysis, Game Based Learning, Interactive learning environments, Learning Analytics, Model discovery, Process Mining, Serious games, Skill assessment, Systematic Literature Review, e-Learning},
}

@inproceedings{Jagadish2007,
	address = {New York, NY, USA},
	title = {Making {Database} {Systems} {Usable}},
	isbn = {978-1-59593-686-8},
	url = {http://doi.acm.org/10.1145/1247480.1247483},
	doi = {10.1145/1247480.1247483},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Jagadish, H V and Chapman, Adriane and Elkiss, Aaron and Jayapandian, Magesh and Li, Yunyao and Nandi, Arnab and Yu, Cong},
	year = {2007},
	note = {tex.ids= 10.1145/1247480.1247483
numPages: 12
place: Beijing, China
seriesTitle: SIGMOD '07},
	keywords = {Interesting, User interfaces, database, usability, user interface},
	pages = {13--24},
}

@article{AlvarezAyllonPresQ2022,
	title = {{PresQ}: {Discovery} of {Multidimensional} {Equally}-{Distributed} {Dependencies} via {Quasi}-{Cliques} on {Hypergraphs}},
	copyright = {All rights reserved},
	issn = {2168-6750},
	url = {https://ieeexplore.ieee.org/document/9861253},
	doi = {10.1109/TETC.2022.3198252},
	number = {Special Section on Emerging Trends and Advances in Graph-based Methods and Applications},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	author = {Alvarez-Ayllon, Alejandro and Palomo-Duarte, Manuel and Dodero, Juan Manuel},
	year = {2022},
	pages = {1--16},
}

@article{Alvarez2019,
	title = {Interactive {Data} {Exploration} of {Distributed} {Raw} {Files}: {A} {Systematic} {Mapping} {Study}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8540356/},
	doi = {10.1109/ACCESS.2018.2882244},
	abstract = {When exploring big amounts of data without a clear target, providing an interactive experience becomes really difficult, since this tentative inspection usually defeats any early decision on data structures or indexing strategies. This is also true on the physics domain, specifically on high energy physics, where the huge volume of data generated by the detectors is normally explored via C++ code using batch processing, which introduces a considerable latency. An interactive tool, when integrated into the existing data management systems, can add great value to the usability of these platforms. Objectives Here we intend to review the current state of the art of interactive data exploration, aiming at satisfying three requirements: access to raw data files, stored in a distributed environment, and with a reasonably low latency. Method This work follows the guidelines for Systematic Mapping Studies, which is well-suited for gathering and classifying available studies. Results We summarize the results after classifying the 242 papers that passed our inclusion criteria. While there are many proposed solutions that tackle the problem in different manners, there is little evidence available about their implementation in practice. Almost all of the solutions found by this study cover a subset of our requirements, with only one partially satisfying the three. Conclusions Solutions for data exploration abound. It is an active research area and, considering the continuous growth of data volume and variety, is only to become harder. There is a niche for research on a solution that covers our requirements and the required building blocks are there.},
	number = {1},
	journal = {IEEE Access},
	author = {Alvarez-Ayllon, Alejandro and Palomo-Duarte, Manuel and Dodero, Juan Manuel},
	year = {2019},
	keywords = {Data Engineering, Data Exploration, Data analysis, Data mining, Data structures, Database Systems, Distributed databases, Indexing, Interactive Systems, Systematic Mapping Study, Time factors, ★},
	pages = {10691--10717},
}

@unpublished{Alvarez2021inference,
	title = {Inference of common multidimensional equally-distributed attributes},
	copyright = {All rights reserved},
	url = {https://arxiv.org/abs/2104.09809},
	author = {Alvarez-Ayllon, Alejandro and Palomo-Duarte, Manuel and Dodero, Juan-Manuel},
	year = {2021},
	note = {arXiv: 2104.09809 [cs.DB]},
}

@techreport{alvarez-ayllon_informe_nodate,
	title = {Informe {Progreso} 2020},
	author = {Alvarez-Ayllon, Alejandro},
}

@techreport{alvarez-ayllon_informe_nodate-1,
	title = {Informe {Progreso} 2019},
	author = {Alvarez-Ayllon, Alejandro},
}

@phdthesis{10498/14132,
	title = {Generación dinámica de invariantes para composiciones de servicios web en {WS}-{BPEL}},
	url = {http://hdl.handle.net/10498/14132},
	abstract = {En los últimos años, las arquitecturas orientadas a servicios están cambiando la filosofía de desarrollo de software en muchos entornos. El uso de servicios web facilita significativamente la interoperabilidad entre sistemas, permitiendo programar sistemas de gran tamaño usando otros más simples de manera sencilla. El principal lenguaje para componer servicios es WS-BPEL 2.0, que ha sido estandarizado por OASIS con la participación de las grandes empresas del sector informático. Sin embargo, las principales técnicas de prueba no han sido adaptadas a WS-BPEL, quedando como uno de los principales retos para su adopción. El objetivo de esta tesis es estudiar la validez de la generación dinámica de invariantes (también conocida como generación de invariantes potenciales) para apoyar la prueba de caja blanca de composiciones de servicios web en WS-BPEL. En primer lugar, la tesis comprueba la viabilidad de la generación dinámica de invariantes para WS-BPEL. Para ello se presenta una arquitectura basada en el generador dinámico de invariantes Daikon, que ha demostrado ser útil para lenguajes imperativos como C/C++, Java y Perl. Daikon es software libre, y se ha integrado con código propio y otros dos sistemas libres: el motor de ejecución compatible WS-BPEL 2.0 ActiveBPEL y la biblioteca de prueba unitaria para WS-BPEL BPELUnit, que incorpora un mecanismo de simulación de servicios web (pues puede haber servicios no disponibles para pruebas). Los tres sistemas han sido adaptados para crear Takuan, el único generador dinámico de invariantes para WS-BPEL hasta la fecha. Tras implementar Takuan se realizaron pruebas para comprobar la utilidad de los invariantes que generaba. En ellas se observaron diversas mejoras específicas de WS-BPEL que permitirían optimizar su rendimiento. Tras implementarlas y evaluarlas, se obtuvo una mejora sustancial tanto en el tiempo de ejecución de Takuan como en la cantidad de invariantes que produce, descartando muchos invariantes no informativos y redundantes. Una vez estuvieron dichas mejoras implementadas se demostró la estabilidad de los invariantes generados por Takuan, permitiendo asegurar su correcto funcionamiento con un conjunto soporte adecuado. Los resultados obtenidos permiten afirmar la validez de la generación dinámica de invariantes para apoyar la prueba de caja blanca de composiciones WS-BPEL.},
	author = {Palomo Duarte, Manuel},
	month = aug,
	year = {2011},
	keywords = {Composición de servicios con WS-BPEL, arquitecturas orientadas a servicios, generación dinámica de invariantes, prueba de software, servicios web},
}

@inproceedings{Blsius2017,
	address = {Dagstuhl, Germany},
	series = {Leibniz international proceedings in informatics ({LIPIcs})},
	title = {The parameterized complexity of dependency detection in relational databases},
	volume = {63},
	isbn = {978-3-95977-023-1},
	url = {http://drops.dagstuhl.de/opus/volltexte/2017/6920},
	doi = {10.4230/LIPIcs.IPEC.2016.6},
	booktitle = {11th international symposium on parameterized and exact computation ({IPEC} 2016)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Bläsius, Thomas and Friedrich, Tobias and Schirneck, Martin},
	editor = {Guo, Jiong and Hermelin, Danny},
	year = {2017},
	note = {ISSN: 1868-8969
tex.urn: urn:nbn:de:0030-drops-69202},
	pages = {6:1--6:13},
}

@misc{Dua:2019,
	title = {{UCI} machine learning repository},
	url = {http://archive.ics.uci.edu/ml},
	publisher = {University of California, Irvine, School of Information and Computer Sciences},
	author = {Dua, Dheeru and Graff, Casey},
	year = {2017},
}

@article{gaulton_chembl_2016,
	title = {The {ChEMBL} database in 2017},
	volume = {45},
	issn = {0305-1048},
	url = {https://doi.org/10.1093/nar/gkw1074},
	doi = {10.1093/nar/gkw1074},
	abstract = {ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.},
	number = {D1},
	journal = {Nucleic Acids Research},
	author = {Gaulton, Anna and Hersey, Anne and Nowotka, Michał and Bento, A. Patrícia and Chambers, Jon and Mendez, David and Mutowo, Prudence and Atkinson, Francis and Bellis, Louisa J. and Cibrián-Uhalte, Elena and Davies, Mark and Dedman, Nathan and Karlsson, Anneli and Magariños, María Paula and Overington, John P. and Papadatos, George and Smit, Ines and Leach, Andrew R.},
	month = nov,
	year = {2016},
	pages = {D945--D954},
}

@article{jiang_holistic_2020,
	title = {Holistic primary key and foreign key detection},
	volume = {54},
	issn = {1573-7675},
	url = {https://doi.org/10.1007/s10844-019-00562-z},
	doi = {10.1007/s10844-019-00562-z},
	abstract = {Primary keys (PKs) and foreign keys (FKs) are important elements of relational schemata in various applications, such as query optimization and data integration. However, in many cases, these constraints are unknown or not documented. Detecting them manually is time-consuming and even infeasible in large-scale datasets. We study the problem of discovering primary keys and foreign keys automatically and propose an algorithm to detect both, namely Holistic Primary Key and Foreign Key Detection (HoPF). PKs and FKs are subsets of the sets of unique column combinations (UCCs) and inclusion dependencies (INDs), respectively, for which efficient discovery algorithms are known. Using score functions, our approach is able to effectively extract the true PKs and FKs from the vast sets of valid UCCs and INDs. Several pruning rules are employed to speed up the procedure. We evaluate precision and recall on three benchmarks and two real-world datasets. The results show that our method is able to retrieve on average 88\% of all primary keys, and 91\% of all foreign keys. We compare the performance of HoPF with two baseline approaches that both assume the existence of primary keys.},
	number = {3},
	journal = {Journal of Intelligent Information Systems},
	author = {Jiang, Lan and Naumann, Felix},
	month = jun,
	year = {2020},
	pages = {439--461},
}

@article{kossmann_data_2022,
	title = {Data dependencies for query optimization: a survey},
	volume = {31},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-021-00676-3},
	doi = {10.1007/s00778-021-00676-3},
	abstract = {Effective query optimization is a core feature of any database management system. While most query optimization techniques make use of simple metadata, such as cardinalities and other basic statistics, other optimization techniques are based on more advanced metadata including data dependencies, such as functional, uniqueness, order, or inclusion dependencies. This survey provides an overview, intuitive descriptions, and classifications of query optimization and execution strategies that are enabled by data dependencies. We consider the most popular types of data dependencies and focus on optimization strategies that target the optimization of relational database queries. The survey supports database vendors to identify optimization opportunities as well as DBMS researchers to find related work and open research questions.},
	number = {1},
	journal = {The VLDB Journal},
	author = {Kossmann, Jan and Papenbrock, Thorsten and Naumann, Felix},
	month = jan,
	year = {2022},
	pages = {1--22},
}

@article{EuclidDesprez2020,
	title = {Euclid preparation. {X}. {The} euclid photometric-redshift challenge},
	volume = {644},
	doi = {10.1051/0004-6361/202039403},
	journal = {Astronomy \& Astrophysics},
	author = {Desprez, G. and Paltani, S. and Coupon, J. and Euclid Collaboration and Alvarez-Ayllon, A. and Amaro, V. and Brescia, M. and Brodwin, M. and Cavuoti, S. and De Vicente-Albendea, J. and Fotopoulou, S. and Hatfield, P. W. and Hartley, W. G. and Ilbert, O. and Jarvis, M. J. and Longo, G. and Rau, M. M. and Saha, R. and Speagle, J. S. and Tramacere, A. and Castellano, M. and Dubath, F. and Galametz, A. and Kuemmel, M. and Laigle, C. and Merlin, E. and Mohr, J. J. and Pilo, S. and Salvato, M. and Andreon, S. and Auricchio, N. and Baccigalupi, C. and Balaguera-Antolínez, A. and Baldi, M. and Bardelli, S. and Bender, R. and Biviano, A. and Bodendorf, C. and Bonino, D. and Bozzo, E. and Branchini, E. and Brinchmann, J. and Burigana, C. and Cabanac, R. and Camera, S. and Capobianco, V. and Cappi, A. and Carbone, C. and Carretero, J. and Carvalho, C. S. and Casas, R. and Casas, S. and Castander, F. J. and Castignani, G. and Cimatti, A. and Cledassou, R. and Colodro-Conde, C. and Congedo, G. and Conselice, C. J. and Conversi, L. and Copin, Y. and Corcione, L. and Courtois, H. M. and Cuby, J. -G. and Da Silva, A. and de la Torre, S. and Degaudenzi, H. and Di Ferdinando, D. and Douspis, M. and Duncan, C. A. J. and Dupac, X. and Ealet, A. and Fabbian, G. and Fabricius, M. and Farrens, S. and Ferreira, P. G. and Finelli, F. and Fosalba, P. and Fourmanoit, N. and Frailis, M. and Franceschi, E. and Fumana, M. and Galeotta, S. and Garilli, B. and Gillard, W. and Gillis, B. and Giocoli, C. and Gozaliasl, G. and Graciá-Carpio, J. and Grupp, F. and Guzzo, L. and Hailey, M. and Haugan, S. V. H. and Holmes, W. and Hormuth, F. and Humphrey, A. and Jahnke, K. and Keihanen, E. and Kermiche, S. and Kilbinger, M. and Kirkpatrick, C. C. and Kitching, T. D. and Kohley, R. and Kubik, B. and Kunz, M. and Kurki-Suonio, H. and Ligori, S. and Lilje, P. B. and Lloro, I. and Maino, D. and Maiorano, E. and Marggraf, O. and Markovic, K. and Martinet, N. and Marulli, F. and Massey, R. and Maturi, M. and Mauri, N. and Maurogordato, S. and Medinaceli, E. and Mei, S. and Meneghetti, M. and Metcalf, R. Benton and Meylan, G. and Moresco, M. and Moscardini, L. and Munari, E. and Niemi, S. and Padilla, C. and Pasian, F. and Patrizii, L. and Pettorino, V. and Pires, S. and Polenta, G. and Poncet, M. and Popa, L. and Potter, D. and Pozzetti, L. and Raison, F. and Renzi, A. and Rhodes, J. and Riccio, G. and Rossetti, E. and Saglia, R. and Sapone, D. and Schneider, P. and Scottez, V. and Secroun, A. and Serrano, S. and Sirignano, C. and Sirri, G. and Stanco, L. and Stern, D. and Sureau, F. and Tallada Crespí, P. and Tavagnacco, D. and Taylor, A. N. and Tenti, M. and Tereno, I. and Toledo-Moreo, R. and Torradeflot, F. and Valenziano, L. and Valiviita, J. and Vassallo, T. and Viel, M. and Wang, Y. and Welikala, N. and Whittaker, L. and Zacchei, A. and Zamorani, G. and Zoubian, J. and Zucca, E.},
	month = dec,
	year = {2020},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, catalogs, galaxies: distances and redshifts, surveys, techniques: miscellaneous},
	pages = {A31},
}

@inproceedings{lasri2016clustering,
	title = {Clustering and classification using a self-organizing {MAP}: {The} main flaw and the improvement perspectives},
	booktitle = {2016 {SAI} computing conference ({SAI})},
	author = {Lasri, Rafik},
	year = {2016},
	pages = {1315--1318},
}

@article{cottrell1995two,
	title = {Two or three things that we know about the {Kohonen} algorithm},
	author = {Cottrell, Marie and Fort, Jean-Claude and Pagés, Gilles},
	year = {1995},
	note = {Publisher: Citeseer},
}

@article{chwialkowski2015fast,
	title = {Fast two-sample testing with analytic representations of probability measures},
	volume = {28},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chwialkowski, Kacper P and Ramdas, Aaditya and Sejdinovic, Dino and Gretton, Arthur},
	year = {2015},
	keywords = {Interesting},
	pages = {1981--1989},
}

@article{jitkrittum2016interpretable,
	title = {Interpretable distribution features with maximum testing power},
	volume = {29},
	journal = {Advances in Neural Information Processing Systems},
	author = {Jitkrittum, Wittawat and Szabó, Zoltán and Chwialkowski, Kacper P and Gretton, Arthur},
	year = {2016},
	keywords = {Interesting},
	pages = {181--189},
}

@article{kim2021classification,
	title = {Classification accuracy as a proxy for two-sample testing},
	volume = {49},
	number = {1},
	journal = {The Annals of Statistics},
	author = {Kim, Ilmun and Ramdas, Aaditya and Singh, Aarti and Wasserman, Larry},
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Basics, Interesting},
	pages = {411--434},
}

@article{blanchard2010semi,
	title = {Semi-supervised novelty detection},
	volume = {11},
	journal = {The Journal of Machine Learning Research},
	author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
	year = {2010},
	note = {Publisher: JMLR. org},
	keywords = {Interesting, to-read},
	pages = {2973--3009},
}

@inproceedings{duch2009universal,
	title = {Universal learning machines},
	booktitle = {International conference on neural information processing},
	author = {Duch, Włodzisław and Maszczyk, Tomasz},
	year = {2009},
	note = {tex.organization: Springer},
	pages = {206--215},
}

@inproceedings{Apirak2012,
	title = {Improve the {SOM} classifier with the {Fuzzy} {Integral} technique},
	doi = {10.1109/ICTKE.2012.6152395},
	booktitle = {2011 ninth international conference on {ICT} and knowledge engineering},
	author = {Jirayusakul, Apirak},
	year = {2012},
	pages = {1--4},
}

@article{kuzilek_open_2017,
	title = {Open {University} {Learning} {Analytics} dataset},
	volume = {4},
	issn = {2052-4463},
	url = {https://doi.org/10.1038/sdata.2017.171},
	doi = {10.1038/sdata.2017.171},
	abstract = {Learning Analytics focuses on the collection and analysis of learners’ data to improve their learning experience by providing informed guidance and to optimise learning materials. To support the research in this area we have developed a dataset, containing data from courses presented at the Open University (OU). What makes the dataset unique is the fact that it contains demographic data together with aggregated clickstream data of students’ interactions in the Virtual Learning Environment (VLE). This enables the analysis of student behaviour, represented by their actions. The dataset contains the information about 22 courses, 32,593 students, their assessment results, and logs of their interactions with the VLE represented by daily summaries of student clicks (10,655,280 entries). The dataset is freely available at https://analyse.kmi.open.ac.uk/open\_datasetunder a CC-BY 4.0 license.},
	number = {1},
	journal = {Scientific Data},
	author = {Kuzilek, Jakub and Hlosta, Martin and Zdrahal, Zdenek},
	month = nov,
	year = {2017},
	pages = {170171},
}

@article{saglia_euclid_nodate,
	title = {Euclid preparation: yyy. {The} {Complete} {Calibration} of the {Colour}–{Redshift} {Relation} survey: {LBT} observations and data release},
	author = {Saglia, R.P. and de Nicola, S.},
}

@article{smith1972algorithm,
	title = {Algorithm as 53: {Wishart} variate generator},
	volume = {21},
	number = {3},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Smith, WB and Hocking, RR},
	year = {1972},
	note = {Publisher: JSTOR},
	pages = {341--345},
}

@misc{press1993numerical,
	title = {Numerical recipes in fortran: the art of scientific computing},
	publisher = {Cambridge University Press},
	author = {H, Press William and A, Teukolsky Saul},
	year = {1993},
}

@article{sriperumbudur2010hilbert,
	title = {Hilbert space embeddings and metrics on probability measures},
	volume = {11},
	journal = {The Journal of Machine Learning Research},
	author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Schölkopf, Bernhard and Lanckriet, Gert RG},
	year = {2010},
	note = {Publisher: JMLR. org},
	pages = {1517--1561},
}

@article{lecun1998gradient,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	note = {Publisher: Ieee},
	pages = {2278--2324},
}

@article{rosenbaum2005exact,
	title = {An exact distribution-free test comparing two multivariate distributions based on adjacency},
	volume = {67},
	number = {4},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Rosenbaum, Paul R},
	year = {2005},
	note = {publisher: Wiley Online Library},
	pages = {515--530},
}

@article{Bostrom2007,
	title = {On the definition of information fusion as a field of research},
	author = {Boström, Henrik and Andler, Sten and Brohede, Marcus and Johansson, Ronnie and Karlsson, Alexander and Laere, Joeri and Niklasson, Lars and Klingegård, Maria and Persson, Anne and Ziemke, Tom},
	month = jan,
	year = {2007},
	note = {tex.ids= article},
}

@inproceedings{Wu2009,
	title = {{FastBit}: {Interactively} searching massive data},
	doi = {10.1088/1742-6596/180/1/012053},
	abstract = {As scientific instruments and computer simulations produce more and more data, the task of locating the essential information to gain insight becomes increasingly difficult. FastBit is an efficient software tool to address this challenge. In this article, we present a summary of the key underlying technologies, namely bitmap compression, encoding, and binning. Together these techniques enable FastBit to answer structured (SQL) queries orders of magnitude faster than popular database systems. To illustrate how FastBit is used in applications, we present three examples involving a high-energy physics experiment, a combustion simulation, and an accelerator simulation. In each case, FastBit significantly reduces the response time and enables interactive exploration on terabytes of data.},
	booktitle = {Journal of {Physics}: {Conference} {Series}},
	author = {Wu, K. and Ahern, S. and Bethel, E. W. and Chen, J. and Childs, H. and Cormier-Michel, E. and Geddes, C. and Gu, J. and Hagen, H. and Hamann, B. and Koegler, W. and Lauret, J. and Meredith, J. and Messmer, P. and Otoo, E. and Perevoztchikov, V. and Poskanzer, A. and {Prabhat} and Rübel, O. and Shoshani, A. and Sim, A. and Stockinger, K. and Weber, G. and Zhang, W. M.},
	year = {2009},
	note = {ISSN: 17426596},
}

@article{cai2020two,
	title = {Two-sample test based on classification probability},
	volume = {13},
	number = {1},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Cai, Haiyan and Goggin, Bryan and Jiang, Qingtang},
	year = {2020},
	note = {Publisher: Wiley Online Library},
	pages = {5--13},
}

@article{jaccard1912distribution,
	title = {The distribution of the flora in the alpine zone. 1},
	volume = {11},
	number = {2},
	journal = {New phytologist},
	author = {Jaccard, Paul},
	year = {1912},
	note = {Publisher: Wiley Online Library},
	pages = {37--50},
}

@misc{noauthor_is_nodate,
	title = {Is {Authy} better than {Google} {Authenticator}? {\textbar} {ShieldPlanet}.com},
	shorttitle = {Is {Authy} better than {Google} {Authenticator}?},
	url = {https://shieldplanet.com/is-authy-better-than-google-authenticator/},
	language = {en-us},
	urldate = {2021-08-29},
}

@article{campelo_sample_2019,
	title = {Sample size estimation for power and accuracy in the experimental comparison of algorithms},
	volume = {25},
	issn = {1572-9397},
	url = {https://doi.org/10.1007/s10732-018-9396-7},
	doi = {10.1007/s10732-018-9396-7},
	abstract = {Experimental comparisons of performance represent an important aspect of research on optimization algorithms. In this work we present a methodology for defining the required sample sizes for designing experiments with desired statistical properties for the comparison of two methods on a given problem class. The proposed approach allows the experimenter to define desired levels of accuracy for estimates of mean performance differences on individual problem instances, as well as the desired statistical power for comparing mean performances over a problem class of interest. The method calculates the required number of problem instances, and runs the algorithms on each test instance so that the accuracy of the estimated differences in performance is controlled at the predefined level. Two examples illustrate the application of the proposed method, and its ability to achieve the desired statistical properties with a methodologically sound definition of the relevant sample sizes.},
	number = {2},
	journal = {Journal of Heuristics},
	author = {Campelo, Felipe and Takahashi, Fernanda},
	month = apr,
	year = {2019},
	keywords = {to-read},
	pages = {305--338},
}

@article{campelo_sample_2020,
	title = {Sample size calculations for the experimental comparison of multiple algorithms on multiple problem instances},
	volume = {26},
	issn = {1572-9397},
	url = {https://doi.org/10.1007/s10732-020-09454-w},
	doi = {10.1007/s10732-020-09454-w},
	abstract = {This work presents a statistically principled method for estimating the required number of instances in the experimental comparison of multiple algorithms on a given problem class of interest. This approach generalises earlier results by allowing researchers to design experiments based on the desired best, worst, mean or median-case statistical power to detect differences between algorithms larger than a certain threshold. Holm’s step-down procedure is used to maintain the overall significance level controlled at desired levels, without resulting in overly conservative experiments. This paper also presents an approach for sampling each algorithm on each instance, based on optimal sample size ratios that minimise the total required number of runs subject to a desired accuracy in the estimation of paired differences. A case study investigating the effect of 21 variants of a custom-tailored Simulated Annealing for a class of scheduling problems is used to illustrate the application of the proposed methods for sample size calculations in the experimental comparison of algorithms.},
	number = {6},
	journal = {Journal of Heuristics},
	author = {Campelo, Felipe and Wanner, Elizabeth F.},
	month = dec,
	year = {2020},
	pages = {851--883},
}

@article{zhang2015astronomy,
	title = {Astronomy in the big data era},
	volume = {14},
	doi = {10.5334/dsj-2015-011},
	journal = {Data Science Journal},
	author = {Zhang, Yanxia and Zhao, Yongheng},
	year = {2015},
	note = {Publisher: Ubiquity Press},
}

@incollection{hey2009the,
	title = {Jim {Gray} on {eScience}: {A} {Transformed} {Scientific} {Method}},
	isbn = {978-0-9825442-0-4},
	url = {https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/},
	abstract = {Increasingly, scientific breakthroughs will be powered by advanced computing capabilities that help researchers manipulate and explore massive datasets. The speed at which any given scientific discipline advances will depend on how well its researchers collaborate with one another, and with technologists, in areas of eScience such as databases, workflow management, visualization, and cloud computing technologies. In The Fourth Paradigm: Data-Intensive Scientific Discovery, the collection of essays expands on the vision of pioneering computer scientist Jim Gray for a new, fourth paradigm of discovery based on data-intensive science and offers insights into how it can be fully realized. Critical praise for The Fourth Paradigm “The individual essays—and The Fourth Paradigm as a whole—give readers a glimpse of the horizon for 21st-century research and, at their best, a peek at what lies beyond. It’s a journey well worth taking.” — James P. Collins School of Life Sciences, Arizona State University Purchase from Amazon Paperback Kindle version From the back cover “The impact of Jim Gray’s thinking is continuing to get people to think in a new way about how data and software are redefining what it means to do science." — Bill Gates, Chairman, Microsoft Corporation “I often tell people working in eScience that they aren’t in this field because they are visionaries or super-intelligent—it’s because they care about science and they are alive now. It is about technology changing the world, and science taking advantage of it, to do more and do better.” — Rhys Francis, Australian eResearch Infrastructure Council “One of the greatest challenges for 21st-century science is how we respond to this new era of data-intensive science. This is recognized as a new paradigm beyond experimental and theoretical research and computer simulations of natural phenomena—one that requires new tools, techniques, and ways of working.” — Douglas Kell, University of Manchester “The contributing authors in this volume have done an extraordinary job of helping to refine an understanding of this new paradigm from a variety of disciplinary perspectives.” — Gordon Bell, Microsoft Research Microsoft Research is honored to provide initial website hosting for this book launch.},
	booktitle = {The fourth paradigm: {Data}-intensive scientific discovery},
	publisher = {Microsoft Research},
	author = {Hey, Tony and Tansley, Stewart and Tolle, Kristin},
	month = oct,
	year = {2009},
	pages = {xvii--xxxi},
}

@misc{noauthor_orcid_nodate,
	title = {{ORCID}},
	url = {https://orcid.org/signin?response_type=code&redirect_uri=https:%2F%2Fwww.overleaf.com%2Fusers%2Fauth%2Forcid%2Fcallback&scope=%2Fauthenticate&state=wMDWVkeiIcFutJNntweACF4c&client_id=APP-7LF990G7W35DXRZT},
	urldate = {2021-08-20},
}

@inproceedings{liu_effective_2008,
	title = {Effective pruning techniques for mining quasi-cliques},
	booktitle = {Joint {European} conference on machine learning and knowledge discovery in databases},
	publisher = {Springer},
	author = {Liu, Guimei and Wong, Limsoon},
	year = {2008},
	note = {tex.ids= liu2008effective
tex.organization: Springer},
	keywords = {Interesting},
	pages = {33--49},
}

@inproceedings{flajolet2007hyperloglog,
	title = {Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm},
	url = {https://hal.inria.fr/hal-00406166},
	booktitle = {Discrete {Mathematics} and {Theoretical} {Computer} {Science}},
	author = {Flajolet, Philippe and Fusy, Eric and Gandouet, Olivier and Meunier, Frédéric},
	year = {2007},
	pages = {137--156},
}

@article{HOWSON2009177,
	title = {Can logic be combined with probability? {Probably}},
	volume = {7},
	issn = {1570-8683},
	url = {https://www.sciencedirect.com/science/article/pii/S1570868307000857},
	doi = {https://doi.org/10.1016/j.jal.2007.11.003},
	abstract = {In the nineteen sixties seminal work was done by Gaifman and then Scott and Krauss in adapting the concepts, tools and procedures of the model theory of modern logic to provide a corresponding model theory in which notions of probabilistic consistency and consequence are defined analogously to the way they are defined (semantically) in the deductive case. The present paper proposes a way of extending these ideas beyond the bounds of a formalised language, even the infinitary language of Scott and Krauss, to achieve a logic having the power and expressiveness of the modern mathematical theory of probability.},
	number = {2},
	journal = {Journal of Applied Logic},
	author = {Howson, Colin},
	year = {2009},
	keywords = {Consistency, Formal languages, Gah, Logic, Probability},
	pages = {177--187},
}

@article{cover1991elements,
	title = {Elements of information theory},
	author = {Cover, Thomas M and Thomas, Joy A},
	year = {1991},
	keywords = {Gah},
}

@inproceedings{10.1145/2791347.2791358,
	address = {New York, NY, USA},
	series = {{SSDBM} '15},
	title = {Towards automated prediction of relationships among scientific datasets},
	isbn = {978-1-4503-3709-0},
	url = {https://doi.org/10.1145/2791347.2791358},
	doi = {10.1145/2791347.2791358},
	abstract = {Before scientists can analyze, publish, or share their data, they often need to determine how their datasets are related. Determining relationships helps scientists identify the most complete version of a dataset, detect versions of datasets that complement each other, and determine multiple datasets that overlap. In previous work, we showed how observable relationships between two datasets help scientists recall their original derivation connection. While that work helped with identifying relationships between two datasets, it is infeasible for scientists to use it for finding relationships between all possible pairs in a large collection of datasets. In order to deal with larger numbers of datasets, we are extending our methodology with a relationship-prediction system, ReDiscover, a tool to identify pairs from a collection of datasets that are most likely related and the relationship between them. We report on the initial design of ReDiscover, which uses machine-learning methods such as Conditional Random Fields and Support Vector Machines to the relationship-discovery problem. Our preliminarily evaluation shows that ReDiscover predicted relationships with an average accuracy of 87\%.},
	booktitle = {Proceedings of the 27th international conference on scientific and statistical database management},
	publisher = {Association for Computing Machinery},
	author = {Alawini, Abdussalam and Maier, David and Tufte, Kristin and Howe, Bill and Nandikur, Rashmi},
	year = {2015},
	note = {Number of pages: 5
Place: La Jolla, California
tex.articleno: 35},
	keywords = {conditional random fields (CRFs), data extraction, data profiling, schema matching, support vector machines (SVMs)},
}

@inproceedings{Milo2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {Automating exploratory data analysis via machine learning: {An} overview},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3383126},
	doi = {10.1145/3318464.3383126},
	abstract = {Exploratory Data Analysis (EDA) is an important initial step for any knowledge discovery process, in which data scientists interactively explore unfamiliar datasets by issuing a sequence of analysis operations (e.g. filter, aggregation, and visualization). Since EDA is long known as a difficult task, requiring profound analytical skills, experience, and domain knowledge, a plethora of systems have been devised over the last decade in order to facilitate EDA.In particular, advancements in machine learning research have created exciting opportunities, not only for better facilitating EDA, but to fully automate the process. In this tutorial, we review recent lines of work for automating EDA. Starting from recommender systems for suggesting a single exploratory action, going through kNN-based classifiers and active-learning methods for predicting users' interestingness preferences, and finally to fully automating EDA using state-of-the-art methods such as deep reinforcement learning and sequence-to-sequence models.We conclude the tutorial with a discussion on the main challenges and open questions to be dealt with in order to ultimately reduce the manual effort required for EDA.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Milo, Tova and Somech, Amit},
	year = {2020},
	note = {Number of pages: 6
Place: Portland, OR, USA},
	keywords = {EDA, data exploration, exploratory data analysis},
	pages = {2617--2622},
}

@inproceedings{Alawini2014,
	title = {Helping scientists reconnect their datasets},
	doi = {10.1145/2618243.2618263},
	booktitle = {Proceedings of the 26th international conference on scientific and statistical database management},
	author = {Alawini, Abdussalam and Maier, David and Tufte, Kristin and Howe, Bill},
	year = {2014},
	pages = {1--12},
}

@inproceedings{Chirigati2016,
	address = {New York, NY, USA},
	series = {{SIGMOD} '16},
	title = {Data polygamy: {The} many-many relationships among urban spatio-temporal data sets},
	isbn = {978-1-4503-3531-7},
	url = {https://doi.org/10.1145/2882903.2915245},
	doi = {10.1145/2882903.2915245},
	abstract = {The increasing ability to collect data from urban environments, coupled with a push towards openness by governments, has resulted in the availability of numerous spatio-temporal data sets covering diverse aspects of a city. Discovering relationships between these data sets can produce new insights by enabling domain experts to not only test but also generate hypotheses. However, discovering these relationships is difficult. First, a relationship between two data sets may occur only at certain locations and/or time periods. Second, the sheer number and size of the data sets, coupled with the diverse spatial and temporal scales at which the data is available, presents computational challenges on all fronts, from indexing and querying to analyzing them. Finally, it is non-trivial to differentiate between meaningful and spurious relationships. To address these challenges, we propose Data Polygamy, a scalable topology-based framework that allows users to query for statistically significant relationships between spatio-temporal data sets. We have performed an experimental evaluation using over 300 spatial-temporal urban data sets which shows that our approach is scalable and effective at identifying interesting relationships.},
	booktitle = {Proceedings of the 2016 international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Chirigati, Fernando and Doraiswamy, Harish and Damoulas, Theodoros and Freire, Juliana},
	year = {2016},
	note = {Number of pages: 15
Place: San Francisco, California, USA},
	keywords = {relationship querying, spatio-temporal data analysis, topology-based techniques, urban data},
	pages = {1011--1025},
}

@article{dolgov2020approximation,
	title = {Approximation and sampling of multivariate probability distributions in the tensor train decomposition},
	volume = {30},
	number = {3},
	journal = {Statistics and Computing},
	author = {Dolgov, Sergey and Anaya-Izquierdo, Karim and Fox, Colin and Scheichl, Robert},
	year = {2020},
	note = {Publisher: Springer},
	pages = {603--625},
}

@inproceedings{Slurm2003,
	address = {Berlin, Heidelberg},
	title = {{SLURM}: {Simple} linux utility for resource management},
	isbn = {978-3-540-39727-4},
	abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
	booktitle = {Job scheduling strategies for parallel processing},
	publisher = {Springer Berlin Heidelberg},
	author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
	editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
	year = {2003},
	pages = {44--60},
}

@article{Hellmich2014,
	title = {{DPM} — efficient storage in diverse environments},
	volume = {513},
	issn = {1742-6588},
	url = {http://iopscience.iop.org/1742-6596/513/4/042025},
	doi = {10.1088/1742-6596/513/4/042025},
	abstract = {Recent developments, including low power devices, cluster file systems and cloud storage, represent an explosion in the possibilities for deploying and managing grid storage. In this paper we present how different technologies can be leveraged to build a storage service with differing cost, power, performance, scalability and reliability profiles, using the popular storage solution Disk Pool Manager (DPM/dmlite) as the enabling technology. The storage manager DPM is designed for these new environments, allowing users to scale up and down as they need it, and optimizing their computing centers energy efficiency and costs. DPM runs on high-performance machines, profiting from multi-core and multi-CPU setups. It supports separating the database from the metadata server, the head node, largely reducing its hard disk requirements. Since version 1.8.6, DPM is released in EPEL and Fedora, simplifying distribution and maintenance, but also supporting the ARM architecture beside i386 and x86 64, allowing it to run the smallest low-power machines such as the Raspberry Pi or the CuBox. This usage is facilitated by the possibility to scale horizontally using a main database and a distributed memcached-powered namespace cache. Additionally, DPM supports a variety of storage pools in the backend, most importantly HDFS, S3-enabled storage, and cluster file systems, allowing users to fit their DPM installation exactly to their needs. In this paper, we investigate the power-efficiency and total cost of ownership of various DPM configurations. We develop metrics to evaluate the expected performance of a setup both in terms of namespace and disk access considering the overall cost including equipment, power consumptions, or data/storage fees. The setups tested range from the lowest scale using Raspberry Pis with only 700MHz single cores and a 100Mbps network connections, over conventional multi-core servers to typical virtual machine instances in cloud settings. We evaluate the combinations of different name server setups, for example load-balanced clusters, with different storage setups, from using a classic local configuration to private and public clouds. 1. Introduction In the age of big data and petascale computing, energy efficiency is becoming more and more important. The density of computing power has increased enough for computing centers to be limited not by space, but by the available power. Locations for large computing infrastructure projects are chosen on the basis of the surrounding climate to mimimise effort and cost for cooling. So while the computing power, the available memory, and storage density are continuously increasing, the energy consumed turns out to be a major constraint for higher-performance computing or clusters, making it important to investigate raising energy efficiency. One way to increase the energy efficiency of a computing center is by consolidation of services on a virtualised infrastructure. Another is to invest in specialised hardware to increase the},
	journal = {Journal of Physics: Conference Series},
	author = {Hellmich, Martin and Furano, Fabrizio and Smith, David and Brito Da Rocha, Ricardo and Alvarez-Ayllon, Alejandro and Manzi, Andrea and Keeble, Oliver and Calvet, Ivan and Regala Miguel, Antonio},
	year = {2014},
}

@article{Furano,
	title = {Towards an {HTTP} {Ecosystem} for {HEP} {Data} {Access}},
	volume = {513},
	issn = {17426596},
	url = {http://iopscience.iop.org/1742-6596/513/3/032034},
	doi = {10.1088/1742-6596/513/3/032034},
	abstract = {In this contribution we present a vision for the use of the HTTP protocol for data access and data management in the context of HEP. The evolution of the DPM/LFC software stacks towards a modern framework that can be plugged into Apache servers triggered various initiatives that successfully demonstrated the use of HTTP-based protocols for data access, federation and transfer. This includes the evolution of the FTS3 system towards being able to manage third-party transfers using HTTP. Given the flexibility of the methods, the feature set may also include a subset of the SRM functionality that is relevant to disk systems. The application domain for such an ecosystem of services goes from large scale, Grid-like computing to the data access from laptops, profiting from tools that are shared with the Web community, like browsers, clients libraries and others. Particular focus was put into emphasizing the flexibility of the frameworks, which can interface with a very broad range of components, data stores, catalogues and metadata stores, including the possibility of building high performance dynamic federations of endpoints that build on the fly the feeling of a unique, seamless very efficient system. The overall goal is to leverage standards and standard practices, and use them to provide the higher level functionalities that are needed to fulfil the complex problem of Data Access in HEP. Other points of interest are about harmonizing the possibilities given by the HTTP/WebDAV protocols with existing frameworks like ROOT and already existing Storage Federations based on the XROOTD framework. We also provide quantitative evaluations of the performance that is achievable using HTTP for remote transfer and remote I/O in the context of HEP data. The idea is to contribute the parts that can make possible an ecosystem of services and applications, where the HEP-related features are covered, and the door is open to standard solutions and tools provided by third parties, in the context of the Web and Cloud technologies.},
	journal = {J. Phys.: Conf. Ser},
	author = {Furano, Fabrizio and Devresse, Adrien and Keeble, Oliver and Hellmich, Martin and Alvarez-Ayllon, Alejandro},
}

@article{AlejandroAlvarez2014,
	title = {{FTS3}: {New} {Data} {Movement} {Service} {For} {WLCG}},
	issn = {17426596},
	doi = {10.1088/1742-6596/513/3/032081},
	journal = {Journal of Physics: Conference Series},
	author = {Alvarez-Ayllon, Alejandro and Michail, Salichos and Michail K., Simon and Oliver, Keeble},
	year = {2014},
	pages = {7},
}

@article{kuijken2019fourth,
	title = {The fourth data release of the {Kilo}-{Degree} {Survey}: ugri imaging and nine-band optical-{IR} photometry over 1000 square degrees},
	volume = {625},
	journal = {Astronomy \& Astrophysics},
	author = {Kuijken, K and Heymans, C and Dvornik, A and Hildebrandt, H and de Jong, JTA and Wright, AH and Erben, T and Bilicki, M and Giblin, B and Shan, H-Y and {others}},
	year = {2019},
	note = {Publisher: EDP Sciences},
	pages = {A2},
}

@phdthesis{alawini2016,
	title = {Identifying relationships between scientific datasets},
	url = {https://pdxscholar.library.pdx.edu/open_access_etds/2922/},
	school = {Portland State University},
	author = {Alawini, Abdussalam},
	year = {2016},
	doi = {10.15760/etd.2918},
	keywords = {Basics, Gah, Tesis},
}

@article{wishart1928generalised,
	title = {The generalised product moment distribution in samples from a normal multivariate population},
	doi = {10.2307/2331939},
	journal = {Biometrika},
	author = {Wishart, John},
	year = {1928},
	note = {Publisher: JSTOR},
	pages = {32--52},
}

@inproceedings{shaabani2016detecting,
	title = {Detecting maximum inclusion dependencies without candidate generation},
	doi = {10.1007/978-3-319-44406-2_1},
	booktitle = {International conference on database and expert systems applications},
	author = {Shaabani, Nuhad and Meinel, Christoph},
	year = {2016},
	note = {tex.organization: Springer},
	pages = {118--133},
}

@article{papenbrock2015divide,
	title = {Divide \& conquer-based inclusion dependency discovery},
	volume = {8},
	doi = {10.14778/2752939.2752946},
	number = {7},
	journal = {Proceedings of the VLDB Endowment},
	author = {Papenbrock, Thorsten and Kruse, Sebastian and Quiané-Ruiz, Jorge-Arnulfo and Naumann, Felix},
	year = {2015},
	note = {Publisher: VLDB Endowment},
	pages = {774--785},
}

@misc{jones_scipy_2001,
	title = {{SciPy}: {Open} source scientific tools for {Python}},
	url = {http://www.scipy.org/},
	author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and {others}},
	year = {2001},
}

@techreport{dawson2008comparing,
	title = {Comparing floating point numbers},
	institution = {Cygnus Software},
	author = {Dawson, Bruce},
	year = {2008},
}

@incollection{Borne2001,
	title = {Data {Mining} in {Astronomical} {Databases}},
	url = {http://dx.doi.org/10.1007/10849171_88},
	abstract = {A Virtual Observatory (VO) will enable transparent and efficient access, search, retrieval, and visualization of data across multiple data repositories, which are generally heterogeneous and distributed. Aspects of data mining that apply to a variety of science user scenarios with a VO are reviewed.},
	booktitle = {Mining the {Sky}},
	publisher = {Springer},
	author = {Borne, Kirk D.},
	year = {2000},
	doi = {10.1007/10849171_88},
	note = {arXiv: astro-ph/0010583},
	keywords = {★},
	pages = {671--673},
}

@misc{the_international_journal_of_escience_future_nodate,
	title = {Future {Generation} {Computer} {Systems} - {Guidelines}},
	publisher = {Elsevier},
	author = {The International Journal of eScience},
}

@article{anonymous_discovering_nodate,
	title = {Discovering {Events} in {Wikipedia} {Pages} through {Co}-reference},
	abstract = {A great deal of data is nowadays available in textual form. To allow this data to reach its full potential, there has been an increasing interest in extracting structural information from it. A well-studied topic towards that direction is entity extraction, that aims at identifying references and information about real world entities. An equally  important, but relatively understudied, topic is event extraction, i.e., the identification of real world events that have occurred at some point in time. We focus on Wikipedia, that constitutes the world’s largest knowledge base and aim at identifying real world events that have occurred in the past. Most existing event identification techniques are based on observable temporal anomalies in the data, which means that they fall short in identifying events from descriptions in the texts. In this work we provide a novel approach to event extraction that is based on temporal co-reference among entities. We consider an event to be a set of entities that collectively experience some relationships between them in a specific period of time. In the Wikipedia context, this translates to mutual reference across such entities for some specific time period.
We formally define the problem, describe three different solution implementations, and perform an extensive experimental evaluation.
The results illustrate that the events that our approach can identify are much more than those that other techniques can identify, and those explicitly declared events by the  Wikipedia users.},
	author = {Anonymous},
	keywords = {Use Cases, to-read},
}

@article{pullan_dynamic_2006,
	title = {Dynamic local search for the maximum clique problem},
	volume = {25},
	journal = {Journal of Artificial Intelligence Research},
	author = {Pullan, Wayne and Hoos, Holger H},
	year = {2006},
	pages = {159--185},
}

@article{doi:10.1080/03610918.2012.665546,
	title = {Nonparametric simultaneous tests for location and scale testing: {A} comparison of several methods},
	volume = {42},
	url = {https://doi.org/10.1080/03610918.2012.665546},
	doi = {10.1080/03610918.2012.665546},
	abstract = {The two-sample location-scale problem arises in many situations like climate dynamics, bioinformatics, medicine, and finance. To address this problem, the nonparametric approach is considered because in practice, the normal assumption is often not fulfilled or the observations are too few to rely on the central limit theorem, and moreover outliers, heavy tails and skewness may be possible. In these situations, a nonparametric test is generally more robust and powerful than a parametric test. Various nonparametric tests have been proposed for the two-sample location-scale problem. In particular, we consider tests due to Lepage, Cucconi, Podgor-Gastwirth, Neuhäuser, Zhang, and Murakami. So far all these tests have not been compared. Moreover, for the Neuhäuser test and the Murakami test, the power has not been studied in detail. It is the aim of the article to review and compare these tests for the jointly detection of location and scale changes by means of a very detailed simulation study. It is shown that both the Podgor–Gastwirth test and the computationally simpler Cucconi test are preferable. Two actual examples within the medical context are discussed.},
	number = {6},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {{Marco Marozzi}},
	year = {2013},
	note = {Publisher: Taylor \& Francis
tex.eprint: https://doi.org/10.1080/03610918.2012.665546},
	keywords = {Basics, Interesting},
	pages = {1298--1317},
}

@book{casella2002statistical,
	title = {Statistical inference},
	volume = {2},
	publisher = {Duxbury Pacific Grove, CA},
	author = {Casella, George and Berger, Roger L},
	year = {2002},
}

@article{deemer1951jacobians,
	title = {The {Jacobians} of certain matrix transformations useful in multivariate analysis: {Based} on lectures of {PL} {Hsu} at the {University} of {North} {Carolina}, 1947},
	volume = {38},
	number = {3/4},
	journal = {Biometrika},
	author = {Deemer, Walter L and Olkin, Ingram},
	year = {1951},
	note = {Publisher: JSTOR},
	pages = {345--367},
}

@book{giri2014multivariate,
	title = {Multivariate statistical inference},
	publisher = {Academic Press},
	author = {Giri, Narayan C},
	year = {2014},
	keywords = {Basics},
}

@techreport{springer1979algebra,
	title = {The algebra of random variables},
	author = {Springer, Melvin Dali},
	year = {1979},
}

@article{Cortes1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	number = {3},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	pages = {273--297},
}

@article{Agarwal2017,
	title = {Nearest-{Neighbor} {Searching} {Under} {Uncertainty} {I}},
	volume = {58},
	issn = {1432-0444},
	url = {https://doi.org/10.1007/s00454-017-9903-x},
	doi = {10.1007/s00454-017-9903-x},
	abstract = {Nearest-neighbor queries, which ask for returning the nearest neighbor of a query point in a set of points, are important and widely studied in many fields because of a wide range of applications. In many of these applications, such as sensor databases, location based services, face recognition, and mobile data, the location of data is imprecise. We therefore study nearest-neighbor queries in a probabilistic framework in which the location of each input point and/or query point is specified as a probability density function and the goal is to return the point that minimizes the expected distance, which we refer to as the expected nearest neighbor (\$\${\textbackslash}mathop \{{\textbackslash}mathrm \{ENN\}\}\$\$ENN). We present methods for computing an exact \$\${\textbackslash}mathop \{{\textbackslash}mathrm \{ENN\}\}\$\$ENNor an \$\${\textbackslash}varepsilon \$\$ε-approximate \$\${\textbackslash}mathop \{{\textbackslash}mathrm \{ENN\}\}\$\$ENN, for a given error parameter \$\$0{\textless}{\textbackslash}varepsilon {\textless} 1\$\$0{\textless}ε{\textless}1, under different distance functions. These methods build a data structure of near-linear size and answer \$\${\textbackslash}mathop \{{\textbackslash}mathrm \{ENN\}\}\$\$ENNqueries in polylogarithmic or sublinear time, depending on the underlying function. As far as we know, these are the first nontrivial methods for answering exact or \$\${\textbackslash}varepsilon \$\$ε-approximate \$\${\textbackslash}mathop \{{\textbackslash}mathrm \{ENN\}\}\$\$ENNqueries with provable performance guarantees. Moreover, we extend our results to answer exact or \$\${\textbackslash}varepsilon \$\$ε-approximate k-\$\${\textbackslash}mathop \{{\textbackslash}mathrm \{ENN\}\}\$\$ENNqueries.},
	number = {3},
	journal = {Discrete \& Computational Geometry},
	author = {Agarwal, Pankaj K. and Efrat, Alon and Sankararaman, Swaminathan and Zhang, Wuzhou},
	month = oct,
	year = {2017},
	pages = {705--745},
}

@techreport{noauthor_summary_nodate,
	title = {A {Summary} of {Error} {Propagation}},
	url = {http://ipl.physics.harvard.edu/wp-uploads/2013/03/PS3_Error_Propagation_sp13.pdf},
}

@article{Casanova1984,
	title = {Inclusion dependencies and their interaction with functional dependencies},
	volume = {28},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/0022000084900758},
	doi = {https://doi.org/10.1016/0022-0000(84)90075-8},
	abstract = {Inclusion dependencies, or INDs (which can say, for example, that every manager is an employee) are studied, including their interaction with functional dependencies, or FDs. A simple complete axiomatization for INDs is presented, and the decision problem for INDs is shown to be PSPACE-complete. (The decision problem for INDs is the problem of determining whether or not Σ logically implies a, given a set Σ of INDs and a single IND a). It is shown that finite implication (implication over databases with a finite number of tuples) is the same as unrestricted implications for INDs, although finite implication and unrestricted implication are distinct for FDs and INDs taken together. It is shown that, although there are simple complete axiomatizations for FDs alone and for INDs alone, there is no complete axiomatization for FDs and INDs taken together, in which every rule is k-mary for some fixed k (and in particular, there is no finite complete axiomatization.) Thus, no k-mary axiomatization can fully describe the interaction between FDs and INDs. This is true whether we consider finite implication or unrestricted implication. In the case of finite implication, this result holds, even if no relation scheme has more than two attributes, and if all of the dependencies are unary (a dependency is unary if the left-hand side and right-hand side each contain only one attribute). In the case of unrestricted implication, the result holds, even if no relation scheme has more than three attributes, each FD is unary, and each IND is binary.},
	number = {1},
	journal = {Journal of Computer and System Sciences},
	author = {Casanova, Marco A. and Fagin, Ronald and Papadimitriou, Christos H.},
	year = {1984},
	pages = {29 -- 59},
}

@article{mannila1997,
	title = {Levelwise {Search} and {Borders} of {Theories} in {Knowledge} {Discovery}},
	volume = {1},
	issn = {1573-756X},
	url = {https://doi.org/10.1023/A:1009796218281},
	doi = {10.1023/A:1009796218281},
	abstract = {One of the basic problems in knowledge discovery in databases (KDD) is the following: given a data set r, a class L of sentences for defining subgroups of r, and a selection predicate, find all sentences of L deemed interesting by the selection predicate. We analyze the simple levelwise algorithm for finding all such descriptions. We give bounds for the number of database accesses that the algorithm makes. For this, we introduce the concept of the border of a theory, a notion that turns out to be surprisingly powerful in analyzing the algorithm. We also consider the verification problem of a KDD process: given r and a set of sentences S \$\$ {\textbackslash}subseteq \$\$L determine whether S is exactly the set of interesting statements about r. We show strong connections between the verification problem and the hypergraph transversal problem. The verification problem arises in a natural way when using sampling to speed up the pattern discovery step in KDD.},
	number = {3},
	journal = {Data Mining and Knowledge Discovery},
	author = {Mannila, Heikki and Toivonen, Hannu},
	month = sep,
	year = {1997},
	pages = {241--258},
}

@article{funabiki1992neural,
	title = {A neural network model for finding a near-maximum clique},
	volume = {14},
	number = {3},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Funabiki, Nubuo and Takefuji, Yoshiyasu and Lee, Kuo-Chun},
	year = {1992},
	note = {Publisher: Elsevier},
	pages = {340--344},
}

@phdthesis{shaabani2019discovering,
	title = {On discovering and incrementally updating inclusion dependencies},
	author = {Shaabani, Nuhad},
	year = {2019},
	keywords = {Interesting},
}

@inproceedings{li2020lisa,
	title = {{LISA}: {A} learned index structure for spatial data},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} international conference on management of data},
	author = {Li, Pengfei and Lu, Hua and Zheng, Qian and Yang, Long and Pan, Gang},
	year = {2020},
	keywords = {to-read},
	pages = {2119--2133},
}

@article{Dewar_2018,
	title = {Connectivity in hypergraphs},
	volume = {61},
	issn = {1496-4287},
	url = {http://dx.doi.org/10.4153/CMB-2018-005-9},
	doi = {10.4153/cmb-2018-005-9},
	number = {2},
	journal = {Canadian Mathematical Bulletin},
	author = {Dewar, Megan and Pike, David and Proos, John},
	month = jun,
	year = {2018},
	note = {Publisher: Canadian Mathematical Society},
	keywords = {Basics, to-read},
	pages = {252--271},
}

@article{YANG20091312,
	title = {A {TCNN} filter algorithm to maximum clique problem},
	volume = {72},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231208005183},
	doi = {https://doi.org/10.1016/j.neucom.2008.10.012},
	abstract = {In this paper, coefficient sensitiveness of transiently chaotic neural network (TCNN) for maximum clique problem (MCP) is analyzed. By introducing a new definition of adaptivity into TCNN and utilizing a neuron filter, the domain of coefficient selection is enlarged to search globally optimal solution and near-optimal solution on MCP. Based on our analysis on the characteristic of TCNN, we propose a TCNN filter algorithm to shrink the network scale and improve solution quality. The algorithm effectively relaxes the coefficient sensitiveness and improves network ability to solve MCP. Simulations have been performed to verify the validity of our algorithm.},
	number = {4},
	journal = {Neurocomputing},
	author = {Yang, Gang and Yi, Junyan and Zhang, Zhiqiang and Tang, Zheng},
	year = {2009},
	keywords = {Maximum clique problem, Neuron filter, Shrinking mechanism, Transiently chaotic neural network},
	pages = {1312 -- 1318},
}

@article{tomita2006worst,
	title = {The worst-case time complexity for generating all maximal cliques and computational experiments},
	volume = {363},
	number = {1},
	journal = {Theoretical computer science},
	author = {Tomita, Etsuji and Tanaka, Akira and Takahashi, Haruhisa},
	year = {2006},
	note = {Publisher: Elsevier},
	pages = {28--42},
}

@article{CAZALS2008564,
	title = {A note on the problem of reporting maximal cliques},
	volume = {407},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397508003903},
	doi = {https://doi.org/10.1016/j.tcs.2008.05.010},
	abstract = {Reporting the maximal cliques of a graph is a fundamental problem arising in many areas. This note bridges the gap between three papers addressing this problem: the original paper of Bron–Kerbosh [C. Bron, J. Kerbosch, Algorithm 457: Finding all cliques of an undirected graph, Communication of ACM 16 (9) (1973) 575–577], and two papers recently published in TCS, namely that of Tomita et al. [Tomita, A. Tanaka, H. Takahashi, The worst-case time complexity for generating all maximal cliques and computational experiments, Theoretical Computer Science 363 (1) (2006) 28–42], and that of Koch [I. Koch, Fundamental study: Enumerating all connected maximal common subgraphs in two graphs, Theoretical Computer Science 250 (1–2) (2001) 1–30]. In particular, we show that the strategy of Tomita et al. is a simple modification of the Bron–Kerbosch algorithm, based on an (un-exploited) observation raised in Koch’s paper.},
	number = {1},
	journal = {Theoretical Computer Science},
	author = {Cazals, F. and Karande, C.},
	year = {2008},
	keywords = {Enumeration problems, Maximal cliques, Maximal common subgraphs, Output sensitive algorithms},
	pages = {564 -- 568},
}

@article{RAO20082768,
	title = {Solving some {NP}-complete problems using split decomposition},
	volume = {156},
	issn = {0166-218X},
	url = {http://www.sciencedirect.com/science/article/pii/S0166218X07005100},
	doi = {https://doi.org/10.1016/j.dam.2007.11.013},
	abstract = {We show how to use the split decomposition to solve some NP-hard optimization problems on graphs. We give algorithms for clique problem and domination-type problems. Our main result is an algorithm to compute a coloration of a graph using its split decomposition. Finally we show that the clique-width of a graph is bounded if and only if the clique-width of each representative graph in its split decomposition is bounded.},
	number = {14},
	journal = {Discrete Applied Mathematics},
	author = {Rao, Michaël},
	year = {2008},
	keywords = {Clique-width, Graph algorithms, Graph coloring, NP-complete problems, Split decomposition},
	pages = {2768 -- 2780},
}

@article{demetrovics1995some,
	title = {Some remarks on generating {Armstron} and inferring functional dependencies relation},
	volume = {12},
	number = {2},
	journal = {Acta Cybernetica},
	author = {Demetrovics, János and Thi, Vu Duc},
	year = {1995},
	pages = {167--180},
}

@article{hall2005geometric,
	title = {Geometric representation of high dimension, low sample size data},
	volume = {67},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Hall, Peter and Marron, James Stephen and Neeman, Amnon},
	year = {2005},
	note = {Publisher: Wiley Online Library},
	pages = {427--444},
}

@article{burges_tutorial_1998,
	title = {A {Tutorial} on {Support} {Vector} {Machines} for {Pattern} {Recognition}},
	volume = {2},
	issn = {1573-756X},
	url = {https://doi.org/10.1023/A:1009715923555},
	doi = {10.1023/A:1009715923555},
	abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
	number = {2},
	journal = {Data Mining and Knowledge Discovery},
	author = {Burges, Christopher J.C.},
	month = jun,
	year = {1998},
	pages = {121--167},
}

@article{10.14778/2794367.2794377,
	title = {Functional dependency discovery: {An} experimental evaluation of seven algorithms},
	volume = {8},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2794367.2794377},
	doi = {10.14778/2794367.2794377},
	number = {10},
	journal = {Proc. VLDB Endow.},
	author = {Papenbrock, Thorsten and Ehrlich, Jens and Marten, Jannik and Neubert, Tommy and Rudolph, Jan-Peer and Schönberg, Martin and Zwiener, Jakob and Naumann, Felix},
	month = jun,
	year = {2015},
	note = {Number of pages: 12
Publisher: VLDB Endowment
tex.issue\_date: June 2015},
	pages = {1082--1093},
}

@inproceedings{_mining_nodate,
	title = {Mining {Data} {Quality} {Rules} for {Data} {Migrations}: {A} {Study} on {Material} {Master} {Data}},
	abstract = {Master data sets are an important asset for organizations
and their quality must be good to ensure organizational success. At the same time, data migrations are complex and they often result in impaired data sets of lower quality. In particular, data quality issues that involve multiple attributes are difficult to identify and can only be re solved by manual data quality checks. 

In this paper, we are investigating a real-world migration of material master data. Our goal is to ensure data quality by mining the target data set for data quality rules. In a data migration incoming data sets must comply with these rules to be
migrated. For generating data quality rules, we used a SVM for rules at a schema level and Association Rule Learning for rules at the instance level.

We found that both methods produce rules of high accuracy and are suitable for ensuring data quality. As an ensemble, the two methods are adequate to manage common real-world data characteristics such as sparsity or mixed values.},
	author = {??, ??},
}

@misc{noauthor_springer_nodate,
	title = {Springer {Guidelines} for {Authors} of {Proceedings}},
	abstract = {The abstract is a mandatory element that should summarize the contents of the paper and should contain up to 250 words. Abstract and keywords
are made freely available in SpringerLink. Please use 9pt font size for the abstract and 10pt for the text.},
}

@incollection{Ferri1994,
	title = {Comparative study of techniques for large-scale feature selection},
	volume = {16},
	booktitle = {Machine intelligence and pattern recognition},
	publisher = {Elsevier},
	author = {Ferri, FJ and Pudil, Pavel and Hatef, Mohamad and Kittler, Josef},
	year = {1994},
	pages = {403--413},
}

@inproceedings{Herdin2005,
	series = {Vehicular {Technology} {Conference}, 2005. {VTC} 2005-{Spring}. 2005 {IEEE} 61st},
	title = {Correlation matrix distance, a meaningful measure for evaluation of non-stationary {MIMO} channels},
	volume = {1},
	isbn = {0-7803-8887-9},
	doi = {10.1109/VETECS.2005.1543265},
	author = {Herdin, Markus and Czink, N. and Ozcelik, H. and Bonek, Ernst},
	month = jan,
	year = {2005},
	pages = {136 -- 140 Vol. 1},
}

@article{Cunningham1985,
	title = {Optimal attack and reinforcement of a network},
	volume = {32},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/3828.3829},
	doi = {10.1145/3828.3829},
	number = {3},
	journal = {Journal of the ACM},
	author = {Cunningham, William H.},
	month = jul,
	year = {1985},
	note = {Number of pages: 13
Publisher: Association for Computing Machinery
tex.address: New York, NY, USA
tex.issue\_date: July 1985},
	pages = {549--561},
}

@inproceedings{ZhaoWeijie2015,
	address = {New York, NY, USA},
	title = {Vertical partitioning for query processing over raw data},
	isbn = {978-1-4503-3709-0},
	url = {http://dl.acm.org/citation.cfm?id=2791347.2791369},
	doi = {10.1145/2791347.2791369},
	abstract = {Traditional databases are not equipped with the adequate functionality to handle the volume and variety of "Big Data". Strict schema definition and data loading are prerequisites even for the most primitive query session. Raw data processing has been proposed as a schema-on-demand alternative that provides instant access to the data. When loading is an option, it is driven exclusively by the current-running query, resulting in sub-optimal performance across a query workload. In this paper, we investigate the problem of workload-driven raw data processing with partial loading. We model loading as fully-replicated binary vertical partitioning. We provide a linear mixed integer programming optimization formulation that we prove to be NP-hard. We design a two-stage heuristic that comes within close range of the optimal solution in a fraction of the time. We extend the optimization formulation and the heuristic to pipelined raw data processing, scenario in which data access and extraction are executed concurrently. We provide three case-studies over real data formats that confirm the accuracy of the model when implemented in a state-of-the-art pipelined operator for raw data processing.},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {ACM},
	author = {Zhao, Weijie and Cheng, Yu and Rusu, Florin},
	year = {2015},
	note = {Series Title: SSDBM '15},
	keywords = {Astronomy, RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {1--12},
}

@inproceedings{ZhaoHan2015,
	title = {On the relationship between sum-product networks and {Bayesian} networks},
	booktitle = {International conference on machine learning},
	author = {Zhao, Han and Melibari, Mazen and Poupart, Pascal},
	year = {2015},
	pages = {116--124},
}

@inproceedings{Zhang2011Meihui,
	address = {New York, NY, USA},
	series = {{SIGMOD} ’11},
	title = {Automatic discovery of attributes in relational databases},
	isbn = {978-1-4503-0661-4},
	url = {https://doi.org/10.1145/1989323.1989336},
	doi = {10.1145/1989323.1989336},
	booktitle = {Proceedings of the 2011 {ACM} {SIGMOD} international conference on management of data},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Meihui and Hadjieleftheriou, Marios and Ooi, Beng Chin and Procopiuc, Cecilia M. and Srivastava, Divesh},
	year = {2011},
	note = {Number of pages: 12
Place: Athens, Greece},
	keywords = {attribute discovery, schema matching},
	pages = {109--120},
}

@article{ZhangHe2011,
	title = {Identifying relevant studies in software engineering},
	volume = {53},
	issn = {09505849},
	doi = {10.1016/j.infsof.2010.12.010},
	abstract = {Context: Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective: The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method: We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. Results: We conducted two participant-observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion: We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE. © 2011 Elsevier B.V. All rights reserved.},
	number = {6},
	journal = {Information and Software Technology},
	author = {Zhang, He and Babar, Muhammad Ali and Tell, Paolo},
	year = {2011},
	note = {ISBN: 0950-5849},
	keywords = {Evidence-based software engineering, Quasi-gold standard, Systematic Reviews},
	pages = {625--637},
}

@inproceedings{Mozafari2017AQP,
	address = {New York, NY, USA},
	title = {Approximate {Query} {Engines} : {Commercial} {Challenges} and {Research} {Opportunities}},
	isbn = {978-1-4503-4197-4},
	url = {https://dl.acm.org/citation.cfm?id=3056098},
	doi = {10.1145/3035918.3056098},
	abstract = {Recent years have witnessed a surge of interest in Approximate Query Processing (AQP) solutions, both in academia and the commercial world. In addition to well-known open problems in this area, there are many new research chal-lenges that have surfaced as a result of the first interaction of AQP technology with commercial and real-world customers. We categorize these into deployment, planning, and inter-face challenges. At the same time, AQP settings introduce many interesting opportunities that would not be possible in a database with precise answers. These opportunities create hopes for overcoming some of the major limitations of traditional database systems. For example, we discuss how a database can reuse its past work in a generic way, and become smarter as it answers new queries. Our goal in this talk is to suggest some of the exciting research directions in this field that are worth pursuing.},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Mozafari, Barzan},
	year = {2017},
	note = {Series Title: SIGMOD '17
ISSN: 07308078},
	keywords = {cluster:Query Approximation, interactive response times, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Opinion Papers, ★},
	pages = {5--8},
}

@article{Krishnan2016Marriage,
	title = {The {Marriage} of {Incremental} and {Approximate} {Computing}},
	volume = {abs/1611.0},
	url = {http://arxiv.org/abs/1611.08573},
	abstract = {Most data analytics systems that require low-latency execution and efficient utilization of computing resources, increasingly adopt two computational paradigms, namely, incremental and approximate computing. Incremental computation updates the output incrementally instead of re-computing everything from scratch for successive runs of a job with input changes. Approximate computation returns an approximate output for a job instead of the exact output. Both paradigms rely on computing over a subset of data items instead of computing over the entire dataset, but they differ in their means for skipping parts of the computation. Incremental computing relies on the memoization of intermediate results of sub-computations, and reusing these memoized results across jobs for sub-computations that are unaffected by the changed input. Approximate computing relies on representative sampling of the entire dataset to compute over a subset of data items. In this thesis, we make the observation that these two computing paradigms are complementary, and can be married together! The high level idea is to: design a sampling algorithm that biases the sample selection to the memoized data items from previous runs. To concretize this idea, we designed an online stratified sampling algorithm that uses self-adjusting computation to produce an incrementally updated approximate output with bounded error. We implemented our algorithm in a data analytics system called IncAppox based on Apache Spark Streaming. Our evaluation of the system shows that IncApprox achieves the benefits of both incremental and approximate computing.},
	author = {Krishnan, Dhanya R},
	year = {2016},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@inproceedings{Krishnan2016IncApprox,
	address = {Republic and Canton of Geneva, Switzerland},
	title = {{IncApprox}: {A} {Data} {Analytics} {System} for {Incremental} {Approximate} {Computing}},
	isbn = {978-1-4503-4143-1},
	url = {https://doi.org/10.1145/2872427.2883026},
	doi = {10.1145/2872427.2883026},
	abstract = {Incremental and approximate computations are increasingly being adopted for data analytics to achieve low-latency execution and efficient utilization of computing resources. Incremental computation updates the output incrementally instead of re-computing everything from scratch for successive runs of a job with input changes. Approximate computation returns an approximate output for a job instead of the exact output. Both paradigms rely on computing over a subset of data items instead of computing over the entire dataset, but they differ in their means for skipping parts of the computation. Incremental computing relies on the memoization of intermediate results of sub-computations, and reusing these memoized results across jobs. Approximate computing relies on representative sampling of the entire dataset to compute over a subset of data items. In this paper, we observe that these two paradigms are complementary, and can be married together! Our idea is quite simple: design a sampling algorithm that biases the sample selection to the memoized data items from previous runs. To realize this idea, we designed an online stratified sampling algorithm that uses self-adjusting computation to produce an incrementally updated approximate output with bounded error. We implemented our algorithm in a data analytics system called IncApprox based on Apache Spark Streaming. Our evaluation using micro-benchmarks and real-world case-studies shows that IncApprox achieves the benefits of both incremental and approximate computing.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Krishnan, Dhanya R and Quoc, Do Le and Bhatotia, Pramod and Fetzer, Christof and Rodrigues, Rodrigo},
	year = {2016},
	note = {Series Title: WWW '16},
	keywords = {RInteractive: Yes, cluster:Query Approximation, dependance graph, error estimation, incremental computation, layer:Middleware, memoization, real-time processing, self-adjusting computation, stratified sampling, stream processing, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1133--1144},
}

@article{kipf2019sosd,
	title = {{SOSD}: {A} benchmark for learned indexes},
	journal = {arXiv preprint arXiv:1911.13014},
	author = {Kipf, Andreas and Marcus, Ryan and van Renen, Alexander and Stoian, Mihail and Kemper, Alfons and Kraska, Tim and Neumann, Thomas},
	year = {2019},
	note = {arXiv: 1911.13014 [cs.DB]},
	keywords = {Interesting},
}

@inproceedings{Alagiannis2012Efficient,
	address = {New York, NY, USA},
	title = {{NoDB}: {Efficient} {Query} {Execution} on {Raw} {Data} {Files}},
	volume = {58},
	isbn = {978-1-4503-1247-9},
	url = {http://dl.acm.org/citation.cfm?doid=2847579.2830508},
	doi = {10.1145/2830508},
	abstract = {As data collections become larger and larger, users are faced with increasing bottlenecks in their data analysis. More data means more time to prepare and to load the data into the database before executing the desired queries. Many applications already avoid using database systems, for example, scientific data analysis and social networks, due to the complexity and the increased data-to-query time, that is, the time between getting the data and retrieving its first useful results. For many applications data collections keep growing fast, even on a daily basis, and this data deluge will only increase in the future, where it is expected to have much more data than what we can move or store, let alone analyze.{\textbackslash}r{\textbackslash}n{\textbackslash}r{\textbackslash}nWe here present the design and roadmap of a new paradigm in database systems, called NoDB, which do not require data loading while still maintaining the whole feature set of a modern database system. In particular, we show how to make raw data files a first-class citizen, fully integrated with the query engine. Through our design and lessons learned by implementing the NoDB philosophy over a modern Database Management Systems (DBMS), we discuss the fundamental limitations as well as the strong opportunities that such a research path brings. We identify performance bottlenecks specific for in situ processing, namely the repeated parsing and tokenizing overhead and the expensive data type conversion. To address these problems, we introduce an adaptive indexing mechanism that maintains positional information to provide efficient access to raw data files, together with a flexible caching structure. We conclude that NoDB systems are feasible to design and implement over modern DBMS, bringing an unprecedented positive effect in usability and performance},
	booktitle = {Proceedings of the 2012 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Alagiannis, Ioannis and Borovica, Renata and Branco, Miguel and Idreos, Stratos and Ailamaki, Anastasia and Borovica-Gajic, Renata and Branco, Miguel and Idreos, Stratos and Ailamaki, Anastasia},
	month = nov,
	year = {2012},
	note = {arXiv: 1401.1406v2
Series Title: SIGMOD '12
Issue: 12
ISSN: 00010782},
	keywords = {RRaw: Yes, cluster:Adaptive Indexing, in situ querying, layer:Database Layer, positional map, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {112--121},
}

@article{Langford2001,
	title = {Is the property of being positively correlated transitive?},
	volume = {55},
	number = {4},
	journal = {The American Statistician},
	author = {Langford, Eric and Schwertman, Neil and Owens, Margaret},
	year = {2001},
	note = {Publisher: Taylor \& Francis},
	keywords = {Basics},
	pages = {322--325},
}

@article{cheng2002learning,
	title = {Learning {Bayesian} networks from data: an information-theory based approach},
	volume = {137},
	number = {1-2},
	journal = {Artificial intelligence},
	author = {Cheng, Jie and Greiner, Russell and Kelly, Jonathan and Bell, David and Liu, Weiru},
	year = {2002},
	note = {tex.ids: Cheng2002
tex.publisher: Elsevier},
	keywords = {Basics, Bayesian belief nets, Conditional independence test, Data mining, Information theory, Knowledge discovery, Learning, Monotone DAG-faithful, Probabilistic model},
	pages = {43--90},
}

@article{gregorutti2017correlation,
	title = {Correlation and variable importance in random forests},
	volume = {27},
	number = {3},
	journal = {Statistics and Computing},
	author = {Gregorutti, Baptiste and Michel, Bertrand and Saint-Pierre, Philippe},
	year = {2017},
	note = {tex.ids: gregorutti2017
tex.publisher: Springer},
	pages = {659--678},
}

@inproceedings{Doan2001,
	title = {Reconciling schemas of disparate data sources: {A} machine-learning approach},
	booktitle = {Proceedings of the 2001 {ACM} {SIGMOD} international conference on {Management} of data},
	author = {Doan, AnHai and Domingos, Pedro and Halevy, Alon Y},
	year = {2001},
	note = {tex.ids: doan2001reconciling},
	keywords = {Schema Matching},
	pages = {509--520},
}

@article{DeMarchi2009,
	title = {Unary and n-ary inclusion dependency discovery in relational databases},
	volume = {32},
	number = {1},
	journal = {Journal of Intelligent Information Systems},
	author = {De Marchi, Fabien and Lopes, Stéphane and Petit, Jean-Marc},
	year = {2009},
	note = {Publisher: Springer},
	pages = {53--73},
}

@article{Dy2004,
	title = {Feature selection for unsupervised learning},
	volume = {5},
	number = {Aug},
	journal = {Journal of machine learning research},
	author = {Dy, Jennifer G and Brodley, Carla E},
	year = {2004},
	keywords = {Basics, Interesting},
	pages = {845--889},
}

@phdthesis{Gao2019,
	title = {Extracting and utilizing hidden structures in large datasets},
	url = {https://www.ideals.illinois.edu/handle/2142/104764},
	abstract = {The hidden structure within datasets --- capturing the inherent structure within the data not explicitly captured or encoded in the data format --- can often be automatically extracted and used to improve various data processing applications. Utilizing such hidden structure enables us to potentially surpass traditional algorithms that do not take this structure into account. In this thesis, we propose a general framework for algorithms that automatically extract and employ hidden structures to improve data processing performance, and discuss a set of design principles for developing such algorithms. We provide three examples to demonstrate the power of this framework in practice, showcasing how we can use hidden structures to either outperform state-of-the-art methods, or enable new applications that are previously impossible. We believe that this framework can offer new opportunities for the design of algorithms that surpass the current limit, and empower new applications in database research and many other data-centric disciplines.},
	school = {University of Illinois at Urbana-Champaign},
	author = {Gao, Yihan},
	year = {2019},
	keywords = {Interesting, Tesis, type:Proposal of Solution},
}

@article{kimura2009,
	title = {Correlation maps: a compressed access method for exploiting soft functional dependencies},
	volume = {2},
	number = {1},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kimura, Hideaki and Huo, George and Rasin, Alexander and Madden, Samuel and Zdonik, Stanley B},
	year = {2009},
	note = {tex.ids: kimura2009correlation
tex.publisher: VLDB Endowment
publisher: VLDB Endowment},
	keywords = {Gah},
	pages = {1222--1233},
}

@inproceedings{bauckmann2007efficiently,
	title = {Efficiently detecting inclusion dependencies},
	booktitle = {2007 {IEEE} 23rd international conference on data engineering},
	author = {Bauckmann, Jana and Leser, Ulf and Naumann, Felix and Tietz, Véronique},
	year = {2007},
	note = {tex.organization: IEEE},
	pages = {1448--1450},
}

@article{Huhtala1999,
	title = {{TANE}: {An} efficient algorithm for discovering functional and approximate dependencies},
	volume = {42},
	number = {2},
	journal = {The computer journal},
	author = {Huhtala, Yka and Kärkkäinen, Juha and Porkka, Pasi and Toivonen, Hannu},
	year = {1999},
	note = {Publisher: OUP},
	pages = {100--111},
}

@inproceedings{Vogel2011,
	title = {Instance-based ‘one-to-some’{Assignment} of {Similarity} {Measures} to {Attributes}},
	booktitle = {{OTM} confederated international conferences" {On} the {Move} to {Meaningful} {Internet} {Systems}"},
	author = {Vogel, Tobias and Naumann, Felix},
	year = {2011},
	note = {tex.organization: Springer},
	keywords = {Schema Matching},
	pages = {412--420},
}

@inproceedings{Madhavan2005,
	title = {Corpus-based schema matching},
	booktitle = {21st international conference on data engineering ({ICDE}'05)},
	author = {Madhavan, Jayant and Bernstein, Philip A and Doan, AnHai and Halevy, Alon},
	year = {2005},
	note = {tex.organization: IEEE},
	pages = {57--68},
}

@article{Tschirschnitz2017,
	title = {Detecting inclusion dependencies on very many tables},
	volume = {42},
	number = {3},
	journal = {ACM Transactions on Database Systems (TODS)},
	author = {Tschirschnitz, Fabian and Papenbrock, Thorsten and Naumann, Felix},
	year = {2017},
	note = {Publisher: ACM New York, NY, USA},
	keywords = {Schema Matching},
	pages = {1--29},
}

@article{Kohn2019,
	title = {Making {Compiling} {Query} {Engines} {Practical}},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2019.2905235},
	abstract = {Compiling queries to machine code is a very efficient way for executing queries. One often overlooked problem with compilation is the time it takes to generate machine code. Even with fast compilation frameworks like LLVM, generating machine code for complex queries often takes hundreds of milliseconds. Such durations can be a major disadvantage for workloads that execute many complex, but quick queries. To solve this problem, we propose an adaptive execution framework, which dynamically switches from interpretation to compilation. We also propose a fast bytecode interpreter for LLVM, which can execute queries without costly translation to machine code and dramatically reduces the query latency. Adaptive execution is fine-grained, and can execute code paths of the same query using different execution modes. Our evaluation shows that this approach achieves optimal performance in a wide variety of settings---low latency for small data sets and maximum throughput for large data sizes. Besides compilation time, we also focus on debugging, which is another important challenge of compilation-based query engines. To address this problem, we present a novel, database-specific debugger for compiling query engines.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Kohn, A and Leis, V and Neumann, T},
	year = {2019},
	keywords = {Computer bugs, Debugging, Engines, Optimization},
	pages = {1},
}

@inproceedings{Dasu2002,
	title = {Mining database structure; or, how to build a data quality browser},
	booktitle = {Proceedings of the 2002 {ACM} {SIGMOD} international conference on {Management} of data},
	author = {Dasu, Tamraparni and Johnson, Theodore and Muthukrishnan, Shanmugauelayut and Shkapenyuk, Vladislav},
	year = {2002},
	keywords = {Schema Matching},
	pages = {240--251},
}

@article{kruse2016,
	title = {Data anamnesis: {Admitting} raw data into an organization.},
	volume = {39},
	number = {2},
	journal = {IEEE Data Eng. Bull.},
	author = {Kruse, Sebastian and Papenbrock, Thorsten and Harmouch, Hazar and Naumann, Felix},
	year = {2016},
	keywords = {Interesting, Schema Discovery},
	pages = {8--20},
}

@article{sorzano2014survey,
	title = {A survey of dimensionality reduction techniques},
	journal = {arXiv preprint arXiv:1403.2877},
	author = {Sorzano, Carlos Oscar Sánchez and Vargas, Javier and Montano, A Pascual},
	year = {2014},
}

@article{yang2020qd,
	title = {Qd-tree: {Learning} data layouts for big data analytics},
	journal = {arXiv preprint arXiv:2004.10898},
	author = {Yang, Zongheng and Chandramouli, Badrish and Wang, Chi and Gehrke, Johannes and Li, Yinan and Minhas, Umar Farooq and Larson, Per-Åke and Kossmann, Donald and Acharya, Rajeev},
	year = {2020},
	keywords = {Adaptive Indexing, Interesting},
}

@article{Sayers2015,
	title = {Probabilistic record linkage},
	volume = {45},
	issn = {0300-5771},
	url = {https://doi.org/10.1093/ije/dyv322},
	doi = {10.1093/ije/dyv322},
	abstract = {Studies involving the use of probabilistic record linkage are becoming increasingly common. However, the methods underpinning probabilistic record linkage are not widely taught or understood, and therefore these studies can appear to be a ‘black box’ research tool. In this article, we aim to describe the process of probabilistic record linkage through a simple exemplar. We first introduce the concept of deterministic linkage and contrast this with probabilistic linkage. We illustrate each step of the process using a simple exemplar and describe the data structure required to perform a probabilistic linkage. We describe the process of calculating and interpreting matched weights and how to convert matched weights into posterior probabilities of a match using Bayes theorem. We conclude this article with a brief discussion of some of the computational demands of record linkage, how you might assess the quality of your linkage algorithm, and how epidemiologists can maximize the value of their record-linked research using robust record linkage methods.},
	number = {3},
	journal = {International Journal of Epidemiology},
	author = {Sayers, Adrian and Ben-Shlomo, Yoav and Blom, Ashley W and Steele, Fiona},
	month = dec,
	year = {2015},
	note = {tex.eprint: https://academic.oup.com/ije/article-pdf/45/3/954/24170194/dyv322.pdf},
	keywords = {to-read},
	pages = {954--964},
}

@article{Steorts2016,
	title = {A bayesian approach to graphical record linkage and deduplication},
	volume = {111},
	url = {https://doi.org/10.1080/01621459.2015.1105807},
	doi = {10.1080/01621459.2015.1105807},
	abstract = {ABSTRACTWe propose an unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation involves the representation of the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate transitive linkage probabilities across records (and represent this visually), and propagate the uncertainty of record linkage into later analyses. Our method makes it particularly easy to integrate record linkage with post-processing procedures such as logistic regression, capture–recapture, etc. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously record linkage approaches, despite the high-dimensional parameter space. We illustrate our method using longitudinal data from the National Long Term Care Survey and with data from the Italian Survey on Household and Wealth, where we assess the accuracy of our method and show it to be better in terms of error rates and empirical scalability than other approaches in the literature. Supplementary materials for this article are available online.},
	number = {516},
	journal = {Journal of the American Statistical Association},
	author = {Steorts, Rebecca C. and Hall, Rob and Fienberg, Stephen E.},
	year = {2016},
	note = {Publisher: Taylor \& Francis
tex.eprint: https://doi.org/10.1080/01621459.2015.1105807},
	keywords = {Interesting},
	pages = {1660--1672},
}

@inproceedings{de2008introduction,
	title = {An introduction to diffusion maps},
	booktitle = {Proceedings of the 19th symposium of the pattern recognition association of south africa ({PRASA} 2008), cape town, south africa},
	author = {De la Porte, J and Herbst, BM and Hereman, W and Van Der Walt, SJ},
	year = {2008},
	keywords = {Diffusion Maps, Interesting},
	pages = {15--25},
}

@article{lenz2006measuring,
	title = {Measuring the disclosure protection of micro aggregated business microdata. {An} analysis taking as an example the german structure of costs survey},
	volume = {22},
	number = {4},
	journal = {Journal of Official Statistics},
	author = {Lenz, Rainer},
	year = {2006},
	note = {tex.publisher: Statistics Sweden (SCB)},
	pages = {681},
}

@inproceedings{5071714,
	title = {Different models for model matching: {An} analysis of approaches to support model differencing},
	doi = {10.1109/CVSM.2009.5071714},
	abstract = {Calculating differences between models is an important and challenging task in Model Driven Engineering. Model differencing involves a number of steps starting with identifying matching model elements, calculating and representing their differences, and finally visualizing them in an appropriate way. In this paper, we provide an overview of the fundamental steps involved in the model differencing process and summarize the advantages and shortcomings of existing approaches for identifying matching model elements. To assist potential users in selecting one of the existing methods for the problem at stake, we investigate the trade-offs these methods impose in terms of accuracy and effort required to implement each one of them.},
	booktitle = {2009 {ICSE} workshop on comparison and versioning of software models},
	author = {Kolovos, D. S. and Di Ruscio, D. and Pierantonio, A. and Paige, R. F.},
	month = may,
	year = {2009},
	keywords = {Basics, Context modeling, Environmental management, Model Matching, Model driven engineering, Visualization, graph theory, model differencing process, model driven engineering, simulation languages},
	pages = {1--6},
}

@article{Silva2016,
	title = {Analyzing related raw data files through dataflows},
	volume = {28},
	issn = {1532-0634},
	url = {http://dx.doi.org/10.1002/cpe.3616},
	doi = {10.1002/cpe.3616},
	abstract = {Computer simulations may ingest and generate high numbers of raw data files. Most of these files follow a de facto standard format established by the application domain, for example, Flexible Image Transport System for astronomy. Although these formats are supported by a variety of programming languages, libraries, and programs, analyzing thousands or millions of files requires developing specific programs. Database management systems (DBMS) are not suited for this, because they require loading the raw data and structuring it, which becomes heavy at large scale. Systems like NoDB, RAW, and FastBit have been proposed to index and query raw data files without the overhead of using a database management system. However, these solutions are focused on analyzing one single large file instead of several related files. In this case, when related files are produced and required for analysis, the relationship among elements within file contents must be managed manually, with specific programs to access raw data. Thus, this data management may be time-consuming and error-prone. When computer simulations are managed by a scientific workflow management system (SWfMS), they can take advantage of provenance data to relate and analyze raw data files produced during workflow execution. However, SWfMS registers provenance at a coarse grain, with limited analysis on elements from raw data files. When the SWfMS is dataflow-aware, it can register provenance data and the relationships among elements of raw data files altogether in a database, which is useful to access the contents of a large number of files. In this paper, we propose a dataflow approach for analyzing element data from several related raw data files. Our approach is complementary to the existing single raw data file analysis approaches. We use the Montage workflow from astronomy and a workflow from Oil and Gas domain as data-intensive case studies. Our experimental results for the Montage workflow explore different types of raw data flows like showing all linear transformations involved in projection simulation programs, considering specific mosaic elements from input repositories. The cost for raw data extraction is approximately 3.7\% of the total application execution time. Copyright © 2015 John Wiley \& Sons, Ltd.},
	number = {8},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Silva, Vítor and de Oliveira, Daniel and Valduriez, Patrick and Mattoso, Marta},
	year = {2016},
	keywords = {Astronomy, Interesting, RDistributed: Yes, RRaw: Yes, Scientific Computing, cluster:Flexible Engines, dataflow, layer:Database Layer, raw data file analysis, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {2528--2545},
}

@inproceedings{cumin:hal-01455715,
	address = {Venice, Italy},
	title = {Data {Exploration} with {SQL} using {Machine} {Learning} {Techniques}},
	isbn = {978-3-89318-073-8},
	url = {https://hal.archives-ouvertes.fr/hal-01455715},
	abstract = {Nowadays data scientists have access to gigantic data, many of them being accessible through SQL. Despite the inherent simplicity of SQL, writing relevant and efficient SQL queries is known to be difficult, especially for databases having a large number of attributes or meaningless attribute names. In this paper, we propose a " rewriting " technique to help data scientists formulate SQL queries, to rapidly and intuitively explore their big data, while keeping user input at a minimum, with no manual tuple specification or labeling. For a user specified query, we define a negation query, which produces tuples that are not wanted in the initial query's answer. Since there is an exponential number of such negation queries, we describe a pseudo-polynomial heuristic to pick the negation closest in size to the initial query, and construct a balanced learning set whose positive examples correspond to the results desired by analysts, and negative examples to those they do not want. The initial query is reformulated using machine learning techniques and a new query, more efficient and diverse, is obtained. We have implemented a prototype and conducted experiments on real-life datasets and synthetic query workloads to assess the scalability and precision of our proposition. A preliminary qualitative experiment conducted with astrophysicists is also described.},
	booktitle = {International {Conference} on {Extending} {Database} {Technology} ({EDBT})},
	author = {Cumin, Julien and Petit, Jean-marc and Scuturici, Vasile-marian and Surdu, Sabina and Cumin, Julien and Petit, Jean-marc and Scuturici, Vasile-marian and Surdu, Sabina and Exploration, Data and Surdu, Sabina},
	month = mar,
	year = {2017},
	keywords = {Big Data, UserStudy:yes, cluster:Assisted Query Formulation, knowledge discovery in databases, layer:User Interaction, mendation, pattern mining, query recom-, query rewriting, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {96--107},
}

@article{Bikakis2017,
	title = {A hierarchical aggregation framework for efficient multilevel visual exploration and analysis},
	volume = {8},
	issn = {22104968},
	url = {https://arxiv.org/abs/1511.04750},
	doi = {10.3233/SW-160226},
	abstract = {Data exploration and visualization systems are of great importance in the Big Data era, in which the volume and heterogeneity of available information make it difficult for humans to manually explore and analyse data. Most traditional systems operate in an offline way, limited to accessing preprocessed (static) sets of data. They also restrict themselves to dealing with small dataset sizes, which can be easily handled with conventional techniques. However, the Big Data era has realized the availability of a great amount and variety of big datasets that are dynamic in nature; most of them offer API or query endpoints for online access, or the data is received in a stream fashion. Therefore, modern systems must address the challenge of on-the-fly scalable visualizations over large dynamic sets of data, offering efficient exploration techniques, as well as mechanisms for information abstraction and summarization. In this work, we present a generic model for personalized multilevel exploration and analysis over large dynamic sets of numeric and temporal data. Our model is built on top of a lightweight tree-based structure which can be efficiently constructed on-the-fly for a given set of data. This tree structure aggregates input objects into a hierarchical multiscale model. Considering different exploration scenarios over large datasets, the proposed model enables efficient multilevel exploration, offering incremental construction and prefetching via user interaction, and dynamic adaptation of the hierarchies based on user preferences. A thorough theoretical analysis is presented, illustrating the efficiency of the proposed model. The proposed model is realized in a web-based prototype tool, called SynopsViz that offers multilevel visual exploration and analysis over Linked Data datasets.},
	number = {1},
	journal = {Semantic Web},
	author = {Bikakis, Nikos and Papastefanatos, George and Skourla, Melina and Sellis, Timos},
	year = {2017},
	note = {arXiv: 1511.04750},
	keywords = {Big Data, Summarization, UserStudy:yes, Visualization, cluster:Adaptive Indexing, data reduction, hierarchical navigation, incremental indexing, layer:Database Layer, linked data, multiresolution, multiscale, personalized exploration, progressive, supercluster:Indexes, type:Proposal of Solution},
	pages = {139--179},
}

@article{Chaudhuri2017,
	title = {Approximate {Query} {Processing}: {No} {Silver} {Bullet}},
	issn = {07308078},
	url = {https://dl.acm.org/citation.cfm?id=3056097},
	doi = {10.1145/3035918.3056097},
	abstract = {In this paper, we reflect on the state of the art of Approximate Query Processing. Although much technical progress has been made in this area of research, we are yet to see its impact on products and services. We discuss two promising avenues to pursue towards in-tegrating Approximate Query Processing into data platforms.},
	journal = {Proceedings of the 2017 ACM International Conference on Management of Data},
	author = {Chaudhuri, Surajit and Ding, Bolin and Kandula, Srikanth},
	year = {2017},
	note = {ISBN: 978-1-4503-4197-4},
	keywords = {Big Data, cluster:Query Approximation, error guarantee, layer:Middleware, olap, pre-computation, query optimization, query processing, sampling, supercluster:Interactive Performance Optimizations, type:Evaluation Research},
	pages = {511--519},
}

@article{Bonifati2014a,
	title = {A {Paradigm} for {Learning} {Queries} on {Big} {Data}},
	url = {https://dl.acm.org/citation.cfm?id=2658842},
	doi = {10.1145/2658840.2658842},
	abstract = {Specifying a database query using a formal query language is typically a challenging task for non-expert users. In the context of big data, this problem becomes even harder as it requires the users to deal with database instances of big sizes and hence difficult to visualize. Such instances usu- ally lack a schema to help the users specify their queries, or have an incomplete schema as they come from disparate data sources. In this paper, we propose a novel paradigm for interactive learning of queries on big data, without assuming any knowledge of the database schema. The paradigm can be applied to different database models and a class of queries adequate to the database model. In particular, in this paper we present two instantiations that validated the proposed paradigm for learning relational join queries and for learn- ing path queries on graph databases. Finally, we discuss the challenges of employing the paradigm for further data models and for learning cross-model schema mappings.},
	journal = {Association for Computing Machinery},
	author = {Bonifati, Angela and Ciucanu, Radu and Lemay, Aurélien and Staworko, Slawek},
	year = {2014},
	note = {ISBN: 9781450331869},
	keywords = {Big Data, learning, query inference},
	pages = {7--12},
}

@article{Wienhofen2015,
	title = {Empirical {Big} {Data} {Research}: {A} {Systematic} {Literature} {Mapping}},
	url = {http://arxiv.org/abs/1509.03045},
	abstract = {Background: Big Data is a relatively new field of research and technology, and literature reports a wide variety of concepts labeled with Big Data. The maturity of a research field can be measured in the number of publications containing empirical results. In this paper we present the current status of empirical research in Big Data. Method: We employed a systematic mapping method with which we mapped the collected research according to the labels Variety, Volume and Velocity. In addition, we addressed the application areas of Big Data. Results: We found that 151 of the assessed 1778 contributions contain a form of empirical result and can be mapped to one or more of the 3 V's and 59 address an application area. Conclusions: The share of publications containing empirical results is well below the average compared to computer science research as a whole. In order to mature the research on Big Data, we recommend applying empirical methods to strengthen the confidence in the reported results. Based on our trend analysis we consider Variety to be the most promising uncharted area in Big Data.},
	number = {7465},
	journal = {Information Systems},
	author = {Wienhofen, Leendert and Mathisen, Bjørn Magnus and Roman, Dumitru},
	year = {2015},
	note = {arXiv: 1509.03045},
	keywords = {Big Data, Systematic Reviews, empirical, trend analysis},
	pages = {18},
}

@article{Kelley:2017:OMA:3051083.3055280,
	title = {Obtaining and {Managing} {Answer} {Quality} for {Online} {Data}-{Intensive} {Services}},
	volume = {2},
	issn = {23763639},
	url = {http://dl.acm.org/citation.cfm?doid=3051083.3055280},
	doi = {10.1145/3055280},
	abstract = {Online data-intensive (OLDI) services use anytime algorithms to compute over large amounts of data and respond quickly. Interactive response times are a priority, so OLDI services parallelize query execution across distributed software components and return best effort answers based on the data so far processed. Omitted data from slow components could lead to better answers, but tracing online how much better the answers could be is difficult. We propose Ubora, a design approach to measure the effect of slow-running components on the quality of answers. Ubora randomly samples online queries and executes them a second time. The first online execution omits data from slow components and provides interactive answers. The second execution uses mature results from intermediate components completed after the online execution finishes. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of services, including Hadoop/Yarn, Apache Lucene, the EasyRec Recommendation Engine, and the OpenEphyra question-answering system. Ubora computes answer quality with more mature executions per second than competing approaches that do not use memoization. With Ubora, we show that answer quality is effective at guiding online admission control. While achieving the same answer quality on high-priority queries, our adaptive controller had 55\% higher peak throughput on low-priority queries than a competing controller guided by the rate of timeouts.},
	number = {2},
	journal = {ACM Transactions on Modeling and Performance Evaluation of Computing Systems},
	author = {Kelley, Jaimie and Stewart, Christopher and Morris, Nathaniel and Tiwari, Devesh and He, Yuxiong and Elnikety, Sameh},
	month = apr,
	year = {2017},
	note = {Publisher: ACM
Place: New York, NY, USA},
	keywords = {Big Data},
	pages = {1--31},
}

@inproceedings{Fan:2015:QBD:2745754.2745771,
	address = {New York, NY, USA},
	title = {Querying {Big} {Data} by {Accessing} {Small} {Data}},
	isbn = {978-1-4503-2757-2},
	url = {http://dl.acm.org/citation.cfm?id=2745754.2745771},
	doi = {10.1145/2745754.2745771},
	abstract = {This paper investigates the feasibility of querying big data by accessing a bounded amount of the data. We study boundedly evaluable queries under a form of access constraints, when their evaluation cost is determined by the queries and constraints only. While it is undecidable to determine whether FO queries are boundedly evaluable, we show that for several classes of FO queries, the bounded evaluability problem is decidable. We also provide characterization and effective syntax for their boundedly evaluable queries. When a query Q is not boundedly evaluable, we study two approaches to approximately answering Q under access constraints. (1) We search for upper and lower envelopes of Q that are boundedly evaluable and warrant a constant accuracy bound. (2) We instantiate a minimum set of variables (parameters) in Q such that the specialized query is boundedly evaluable. We study problems for deciding the existence of envelopes and bounded specialized queries, and establish their complexity for various classes of FO queries.},
	booktitle = {Proceedings of the 34th {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {ACM},
	author = {Fan, Wenfei and Geerts, Floris and Cao, Yang and Deng, Ting and Lu, Ping},
	year = {2015},
	note = {Series Title: PODS '15},
	keywords = {Big Data, cluster:Query Approximation, layer:Middleware, query answering, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {173--184},
}

@inproceedings{Sridhar2014,
	title = {Optimizing database load and extract for big data era},
	volume = {8422 LNCS},
	isbn = {978-3-319-05812-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-05813-9_34},
	doi = {10.1007/978-3-319-05813-9_34},
	abstract = {With growing and pervasive interest in Big Data, SQL relational databases need to compete with data management by Hadoop, NoSQL and NoDB. Database research has mainly focused on result generation by query processing. But SQL databases require data in-place before queries may be processed. The process of DB loading has been a bottleneck leading to external ETL/ELT techniques for loading large data sets. This paper focuses on DB engine level techniques for optimizing both data loads and extracts in an MPP, shared-nothing SQL database, dbX, available on in-house commodity hardware and cloud systems. The agile, data loading of dbX exploits parallelism at multiple levels to achieve TBs of data load per hour making it suitable for cloud and continuous actionable knowledge applications. Implementation techniques at DB engine level, extensions to load/extract syntax and performance results are presented. Load optimization techniques help to speed up data extract to flat files and CTAS type SQL queries too. We show linear scale up with cluster scale out for load/extract in public cloud and commodity hardware systems without recourse to database tuning or use of expensive database appliances. © 2014 Springer International Publishing Switzerland.},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	author = {Sridhar, K. T. and Sakkeer, M. A.},
	year = {2014},
	note = {Issue: PART 2
ISSN: 16113349},
	keywords = {Big Data, Parallel DBMS, cluster:Adaptive Loading, dbX, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {503--512},
}

@article{Shu-Sheng2015,
	title = {A {New} {ETL} {Approach} {Based} on {Data} {Virtualization}},
	volume = {30},
	issn = {10009000},
	url = {http://link.springer.com/article/10.1007/s11390-015-1524-3},
	doi = {10.1007/s11390-015-1524-3},
	abstract = {ETL (Extract-Transform-Load) usually includes three phases: extraction, transformation, and loading. In building data warehouse, it plays the role of data injection and is the most time-consuming activity. Thus it is necessary to improve the performance of ETL. In this paper, a new ETL approach, TEL (Transform-Extract-Load) is proposed. The TEL approach applies virtual tables to realize the transformation stage before extraction stage and loading stage, without data staging area or staging database which stores raw data extracted from each of the disparate source data systems. The TEL approach reduces the data transmission load, and improves the performance of query from access layers. Experimental results based on our proposed benchmarks show that the TEL approach is feasible and practical.},
	number = {2},
	journal = {Journal of Computer Science and Technology},
	author = {Guo, Shu Sheng and Yuan, Zi Mu and Sun, Ao Bing and Yue, Qiang},
	year = {2015},
	note = {ISBN: 1139001515},
	keywords = {Big Data, ETL, data virtualization, heterogeneous database},
	pages = {311--323},
}

@article{Godfrey2015,
	title = {Interactive {Visualization} of {Large} {Data} {Sets}},
	volume = {11},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/7457691/},
	doi = {10.1109/TKDE.2016.2557324},
	abstract = {Visualization provides a powerful means for data analysis. But to be practical, visual analytics tools must support smooth and flexible use of visualizations at a fast rate. This becomes increasingly onerous with the ever-increasing size of real-world datasets. First, large databases make interaction more difficult once query response time exceeds several seconds. Second, any attempt to show all data points will overload the visualization, resulting in chaos that will only confuse the user. Over the last few years substantial effort has been put into addressing both of these issues and many innovative solutions have been proposed. Indeed, data visualization is a topic that is too large to be addressed in a single survey paper. Thus, we restrict our attention here to interactive visualization of large data sets. Our focus then is skewed in a natural way towards query processing problem -provided by an underlying database system -rather than to the actual data visualization problem.},
	number = {12},
	journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
	author = {Godfrey, Parke and Gryz, Jarek and Lasek, Piotr and University, York},
	year = {2015},
	note = {ISBN: 1041-4347},
	keywords = {Big Data, Data aggregation, Index Terms—database visualization, data indexing, interactive visualization},
	pages = {1041--4347},
}

@article{Chen2009,
	title = {Privacy-preserving data publishing},
	volume = {2},
	doi = {10.1561/1900000008},
	journal = {Foundations and Trends in Databases},
	author = {Chen, Bee-Chung and Kifer, Daniel and LeFevre, Kristen and Machanavajjhala, Ashwin},
	month = jan,
	year = {2009},
	pages = {1--167},
}

@article{bishop1994mixture,
	title = {Mixture density networks},
	abstract = {Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classifications problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution
of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the e ectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.},
	author = {Bishop, Christopher M},
	year = {1994},
	note = {tex.publisher: Aston University},
}

@article{Kruse2018,
	title = {Efficient discovery of approximate dependencies},
	volume = {11},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3192965.3192968},
	doi = {10.14778/3192965.3192968},
	number = {7},
	journal = {Proc. VLDB Endow.},
	author = {Kruse, Sebastian and Naumann, Felix},
	month = mar,
	year = {2018},
	note = {tex.issue\_date: March 2018
tex.numpages: 14
tex.publisher: VLDB Endowment},
	keywords = {Interesting},
	pages = {759--772},
}

@inproceedings{ilyas2004cords,
	title = {{CORDS}: automatic discovery of correlations and soft functional dependencies},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMOD} international conference on {Management} of data},
	author = {Ilyas, Ihab F and Markl, Volker and Haas, Peter and Brown, Paul and Aboulnaga, Ashraf},
	year = {2004},
	pages = {647--658},
}

@article{kipf2018learned,
	title = {Learned cardinalities: {Estimating} correlated joins with deep learning},
	journal = {arXiv preprint arXiv:1809.00677},
	author = {Kipf, Andreas and Kipf, Thomas and Radke, Bernhard and Leis, Viktor and Boncz, Peter and Kemper, Alfons},
	year = {2018},
}

@article{nathan2019,
	title = {Learning multi-dimensional indexes},
	journal = {arXiv preprint arXiv:1912.01668},
	author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
	year = {2019},
	keywords = {Interesting},
}

@inproceedings{wu2019,
	title = {Designing succinct secondary indexing mechanism by exploiting column correlations},
	booktitle = {Proceedings of the 2019 international conference on management of data},
	author = {Wu, Yingjun and Yu, Jia and Tian, Yuanyuan and Sidle, Richard and Barber, Ronald},
	year = {2019},
	keywords = {Gah},
	pages = {1223--1240},
}

@article{rohde_matching_2006,
	title = {Matching of catalogues by probabilistic pattern classification},
	volume = {369},
	issn = {0035-8711},
	url = {https://doi.org/10.1111/j.1365-2966.2006.10304.x},
	doi = {10.1111/j.1365-2966.2006.10304.x},
	abstract = {We consider the statistical problem of catalogue matching from a machine learning perspective with the goal of producing probabilistic outputs, and using all available information. A framework is provided that unifies two existing approaches to producing probabilistic outputs in the literature, one based on combining distribution estimates and the other based on combining probabilistic classifiers. We apply both of these to the problem of matching the H i Parkes All Sky Survey radio catalogue with large positional uncertainties to the much denser SuperCOSMOS catalogue with much smaller positional uncertainties. We demonstrate the utility of probabilistic outputs by a controllable completeness and efficiency trade-off and by identifying objects that have high probability of being rare. Finally, possible biasing effects in the output of these classifiers are also highlighted and discussed.},
	number = {1},
	urldate = {2020-02-29},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Rohde, D. J. and Gallagher, M. R. and Drinkwater, M. J. and Pimbblet, K. A.},
	month = may,
	year = {2006},
	pages = {2--14},
}

@inproceedings{Kamat2017,
	address = {New York, NY, USA},
	title = {A {Unified} {Correlation}-based {Approach} to {Sampling} {Over} {Joins}},
	isbn = {978-1-4503-5282-6},
	url = {http://dl.acm.org/citation.cfm?doid=3085504.3085524},
	doi = {10.1145/3085504.3085524},
	abstract = {Supporting sampling in the presence of joins is an important problem in data analysis, but is inherently challenging due to the need to avoid correlation between output tuples. Current solutions provide either correlated or non-correlated samples. Sampling might not always be feasible in the non-correlated sampling-based approaches -- the sample size or intermediate data size might be exceedingly large. On the other hand, a correlated sample may not be representative of the join. This paper presents a unified strategy towards join sampling, while considering sample correlation every step of the way. We provide two key contributions. First, in the case where a correlated sample is acceptable, we provide techniques, for all join types, to sample base relations so that their join is as random as possible. Second, in the case where a correlated sample is not acceptable, we provide enhancements to the state-of-the-art algorithms to reduce their execution time and intermediate data size.},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {ACM},
	author = {Kamat, Niranjan and Nandi, Arnab},
	year = {2017},
	note = {Series Title: SSDBM '17},
	keywords = {Correlation, Join, Random, Randomness, Sampling, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {1--12},
}

@article{Bonnici2013,
	title = {A subgraph isomorphism algorithm and its application to biochemical data},
	volume = {14},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-14-S7-S13},
	doi = {10.1186/1471-2105-14-S7-S13},
	abstract = {Graphs can represent biological networks at the molecular, protein, or species level. An important query is to find all matches of a pattern graph to a target graph. Accomplishing this is inherently difficult (NP-complete) and the efficiency of heuristic algorithms for the problem may depend upon the input graphs. The common aim of existing algorithms is to eliminate unsuccessful mappings as early as and as inexpensively as possible.},
	number = {7},
	journal = {BMC Bioinformatics},
	author = {Bonnici, Vincenzo and Giugno, Rosalba and Pulvirenti, Alfredo and Shasha, Dennis and Ferro, Alfredo},
	month = apr,
	year = {2013},
	pages = {S13},
}

@inproceedings{Yasin2013,
	title = {Incremental {Bayesian} network structure learning in high dimensional domains},
	booktitle = {2013 5th international conference on modeling, simulation and applied optimization ({ICMSAO})},
	author = {Yasin, Amanullah and Leray, Philippe},
	year = {2013},
	note = {tex.organization: IEEE},
	keywords = {Interesting},
	pages = {1--6},
}

@article{Trapp2019,
	title = {Bayesian learning of sum-product networks},
	author = {Trapp, Martin and Peharz, Robert and Ge, Hong and Pernkopf, Franz and Ghahramani, Zoubin},
	year = {2019},
	note = {arXiv: 1905.10884 [cs.LG]},
	keywords = {Interesting},
}

@article{Poon2012,
	title = {Sum-product networks: {A} new deep architecture},
	volume = {abs/1202.3732},
	url = {http://arxiv.org/abs/1202.3732},
	journal = {CoRR},
	author = {Poon, Hoifung and Domingos, Pedro M.},
	year = {2012},
}

@article{Hilprecht2019,
	title = {{DeepDB}: learn from data, not from queries!},
	author = {Hilprecht, Benjamin and Schmidt, Andreas and Kulessa, Moritz and Molina, Alejandro and Kersting, Kristian and Binnig, Carsten},
	year = {2019},
	note = {arXiv: 1909.00607 [cs.DB]},
	keywords = {Interesting, cluster:Query Approximation, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@article{Thirumuruganathan2019,
	title = {Approximate {Query} {Processing} using {Deep} {Generative} {Models}},
	volume = {abs/1903.1},
	url = {http://arxiv.org/abs/1903.10000},
	abstract = {Data is generated at an unprecedented rate surpassing our ability to analyze them. One viable solution that was pioneered by the database community is Approximate Query Processing (AQP). AQP seeks to provide approximate answers to queries in a fraction of time needed for computing exact answers. This is often achieved by running the query on a pre-computed or on-demand derived sample and generating estimates for the entire dataset based on the result. In this work, we explore a novel approach for AQP utilizing deep learning (DL). We use deep generative models, an unsupervised learning based approach, to learn the data distribution faithfully in a compact manner (typically few hundred KBs). Queries could be answered approximately by generating samples from the learned model. This approach eliminates the dependency of AQP to a sample of fixed size and allows us to satisfy arbitrary accuracy requirements by generating as many samples as needed very fast. While we specifically focus on variational autoencoders (VAE), we demonstrate how our approach could also be used for other popular DL models such as generative adversarial networks (GAN) and deep Bayesian networks (DBN). Our other contributions include (a) identifying model bias and minimizing it through a rejection sampling based approach (b) An algorithm to build model ensembles for AQP for improved accuracy and (c) an analysis of VAE latent space to understand its suitability to AQP. Our extensive experiments show that deep learning is a very promising approach for AQP.},
	journal = {CoRR},
	author = {Thirumuruganathan, Saravanan and Hasan, Shohedul and Koudas, Nick and Das, Gautam},
	year = {2019},
	note = {arXiv: 1903.10000},
	keywords = {Interesting, cluster:Query Approximation, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@article{Ding2019,
	title = {{ALEX}: an updatable adaptive learned index},
	author = {Ding, Jialin and Minhas, Umar Farooq and Zhang, Hantian and Li, Yinan and Wang, Chi and Chandramouli, Badrish and Gehrke, Johannes and Kossmann, Donald and Lomet, David},
	year = {2019},
	note = {arXiv: 1905.08898 [cs.DB]},
	keywords = {Interesting, cluster:Indexes, supercluster:Indexes, type:Proposal of Solution},
}

@inproceedings{Woltmann2019,
	address = {New York, NY, USA},
	series = {{aiDM} '19},
	title = {Cardinality estimation with local deep learning models},
	isbn = {978-1-4503-6802-5},
	url = {http://doi.acm.org/10.1145/3329859.3329875},
	doi = {10.1145/3329859.3329875},
	booktitle = {Proceedings of the second international workshop on exploiting artificial intelligence techniques for data management},
	publisher = {ACM},
	author = {Woltmann, Lucas and Hartmann, Claudio and Thiele, Maik and Habich, Dirk and Lehner, Wolfgang},
	year = {2019},
	note = {event-place: Amsterdam, Netherlands
tex.acmid: 3329875
tex.articleno: 5
tex.numpages: 8},
	keywords = {Interesting, cluster:Query Approximation, type:Proposal of Solution},
	pages = {5:1--5:8},
}

@article{Tsamardinos2006,
	title = {The max-min hill-climbing {Bayesian} network structure learning algorithm},
	volume = {65},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-006-6889-7},
	doi = {10.1007/s10994-006-6889-7},
	abstract = {We present a new algorithm for Bayesian network structure learning, called Max-Min Hill-Climbing (MMHC). The algorithm combines ideas from local learning, constraint-based, and search-and-score techniques in a principled and effective way. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. In our extensive empirical evaluation MMHC outperforms on average and in terms of various metrics several prototypical and state-of-the-art algorithms, namely the PC, Sparse Candidate, Three Phase Dependency Analysis, Optimal Reinsertion, Greedy Equivalence Search, and Greedy Search. These are the first empirical results simultaneously comparing most of the major Bayesian network algorithms against each other. MMHC offers certain theoretical advantages, specifically over the Sparse Candidate algorithm, corroborated by our experiments. MMHC and detailed results of our study are publicly available at http://www.dsl-lab.org/supplements/mmhc{\textless}span class="nocase"{\textgreater}\_{\textless}/span{\textgreater}paper/mmhc{\textless}span class="nocase"{\textgreater}\_{\textless}/span{\textgreater}index.html.},
	number = {1},
	journal = {Machine Learning},
	author = {Tsamardinos, Ioannis and Brown, Laura E. and Aliferis, Constantin F.},
	month = oct,
	year = {2006},
	pages = {31--78},
}

@article{Budavri2008,
	title = {Probabilistic cross-identification of astronomical sources},
	volume = {679},
	url = {https://doi.org/10.1086%2F587156},
	doi = {10.1086/587156},
	abstract = {We present a general probabilistic formalism for cross-identifying astronomical point sources in multiple observations. Our Bayesian approach, symmetric in all observations, is the foundation of a unified framework for object matching, where not only spatial information, but also physical properties, such as colors, redshift, and luminosity, can be considered in a natural way. We provide a practical recipe to implement an efficient recursive algorithm to evaluate the Bayes factor over a set of catalogs with known circular errors in positions. This new methodology is crucial for studies leveraging the synergy of today’s multiwavelength observations and to enter the time domain science of the upcoming survey telescopes.},
	number = {1},
	journal = {The Astrophysical Journal},
	author = {Budavári, Tamás and Szalay, Alexander S.},
	month = may,
	year = {2008},
	note = {tex.publisher: IOP Publishing},
	pages = {301--309},
}

@article{josse2013,
	title = {Measures of dependence between random vectors and tests of independence. {Literature} review},
	author = {Josse, Julie and Holmes, Susan},
	year = {2013},
	note = {arXiv: 1307.7383 [stat.ME]},
}

@article{doran1981,
	title = {There’sa {SMART} way to write management’s goals and objectives},
	volume = {70},
	number = {11},
	journal = {Management review},
	author = {Doran, George T and {others}},
	year = {1981},
	pages = {35--36},
}

@inproceedings{kluyver2016jupyter,
	title = {Jupyter {Notebooks}-a publishing format for reproducible computational workflows.},
	booktitle = {{ELPUB}},
	author = {Kluyver, Thomas and Ragan-Kelley, Benjamin and Pérez, Fernando and Granger, Brian E and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica B and Grout, Jason and Corlay, Sylvain and {others}},
	year = {2016},
	pages = {87--90},
}

@article{perez2007ipython,
	title = {{IPython}: a system for interactive scientific computing},
	volume = {9},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Pérez, Fernando and Granger, Brian E},
	year = {2007},
	note = {tex.publisher: IEEE},
	pages = {21--29},
}

@inproceedings{lopez2013,
	title = {The randomized dependence coefficient},
	booktitle = {Advances in neural information processing systems},
	author = {Lopez-Paz, David and Hennig, Philipp and Schölkopf, Bernhard},
	year = {2013},
	pages = {1--9},
}

@misc{noauthor_fits_nodate,
	title = {{FITS} {Standard} {Document}},
	url = {https://fits.gsfc.nasa.gov/standard40/fits_standard40aa-le.pdf},
}

@inproceedings{taylor2005topcat,
	title = {{TOPCAT} \& {STIL}: starlink table/{VOTable} processing software},
	volume = {347},
	booktitle = {Astronomical data analysis software and systems {XIV}},
	author = {Taylor, Mark B},
	year = {2005},
	pages = {29},
}

@article{Babu2001,
	title = {{SPARTAN}: {A} model-based semantic compression system for massive data tables},
	volume = {30},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/376284.375693},
	doi = {10.1145/376284.375693},
	number = {2},
	journal = {SIGMOD Rec.},
	author = {Babu, Shivnath and Garofalakis, Minos and Rastogi, Rajeev},
	month = may,
	year = {2001},
	note = {tex.acmid: 375693
tex.address: New York, NY, USA
tex.issue\_date: June 2001
tex.numpages: 12
tex.publisher: ACM},
	keywords = {Interesting},
	pages = {283--294},
}

@book{koller2009probabilistic,
	title = {Probabilistic graphical models: principles and techniques},
	abstract = {A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions.},
	publisher = {MIT press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
	keywords = {Basics},
}

@article{schwarz1978estimating,
	title = {Estimating the dimension of a model},
	volume = {6},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	number = {2},
	journal = {The annals of statistics},
	author = {Schwarz, Gideon and {others}},
	year = {1978},
	note = {tex.publisher: Institute of Mathematical Statistics},
	keywords = {Basics},
	pages = {461--464},
}

@article{Mozafari2015,
	title = {A {Handbook} for {Building} an {Approximate} {Query} {Engine}},
	volume = {38},
	abstract = {There has been much research on various aspects of Approximate Query Processing (AQP), such as different sampling strategies, error estimation mechanisms, and various types of data synopses. How-ever, many subtle challenges arise when building an actual AQP engine that can be deployed and used by real world applications. These subtleties are often ignored (or at least not elaborated) by the theo-retical literature and academic prototypes alike. For the first time to the best of our knowledge, in this article, we focus on these subtle challenges that one must address when designing an AQP system. Our intention for this article is to serve as a handbook listing critical design choices that database practi-tioners must be aware of when building or using an AQP system, not to prescribe a specific solution to each challenge.},
	number = {3},
	journal = {IEEE Data Engineering Bulletin},
	author = {Mozafari, Barzan and Niu, Ning},
	year = {2015},
	keywords = {Basics},
	pages = {3--29},
}

@inproceedings{Izquierdo2013,
	title = {Discovering implicit schemas in {JSON} data},
	booktitle = {International conference on web engineering},
	author = {Izquierdo, Javier Luis Cánovas and Cabot, Jordi},
	year = {2013},
	note = {tex.organization: Springer},
	keywords = {Interesting},
	pages = {68--83},
}

@article{Mei2019,
	title = {{RSATree}: distribution-aware data representation of large-scale tabular datasets for flexible visual query},
	doi = {10.1109/TVCG.2019.2934800},
	abstract = {Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts' flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Mei, H. and Chen, W. and Wei, Y. and Hu, Y. and Zhou, S. and Lin, B. and Zhao, Y. and Xia, J.},
	year = {2019},
	keywords = {Data visualization, Histograms, Interesting, R-tree, Time factors, Visualization, hashing, large-scale data visualization, summed area table},
	pages = {1--1},
}

@inproceedings{kandel2011wrangler,
	title = {Wrangler: {Interactive} visual specification of data transformation scripts},
	booktitle = {Proceedings of the {SIGCHI} conference on human factors in computing systems},
	author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
	year = {2011},
	note = {tex.organization: ACM},
	pages = {3363--3372},
}

@article{Bentley:1975:MBS:361002.361007,
	title = {Multidimensional {Binary} {Search} {Trees} {Used} for {Associative} {Searching}},
	volume = {18},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/361002.361007},
	doi = {10.1145/361002.361007},
	number = {9},
	journal = {Commun. ACM},
	author = {Bentley, Jon Louis},
	month = sep,
	year = {1975},
	note = {tex.acmid: 361007
tex.issue\_date: Sept. 1975
tex.location: New York, NY, USA
tex.numpages: 9
tex.publisher: ACM},
	keywords = {information retrieval system, intersection queries, key, nearest neighbor queries, partial match queries},
	pages = {509--517},
}

@article{abuzaid2018diff,
	title = {{DIFF}: a relational interface for large-scale data explanation},
	volume = {12},
	number = {4},
	journal = {Proceedings of the VLDB Endowment},
	author = {Abuzaid, Firas and Kraft, Peter and Suri, Sahaana and Gan, Edward and Xu, Eric and Shenoy, Atul and Ananthanarayan, Asvin and Sheu, John and Meijer, Erik and Wu, Xi and {others}},
	year = {2018},
	note = {tex.publisher: VLDB Endowment},
	pages = {419--432},
}

@article{qin2018pigeonring,
	title = {Pigeonring: {A} principle for faster thresholded similarity search},
	volume = {12},
	number = {1},
	journal = {Proceedings of the VLDB Endowment},
	author = {Qin, Jianbin and Xiao, Chuan},
	year = {2018},
	note = {tex.publisher: VLDB Endowment},
	pages = {28--42},
}

@article{huang2018optimization,
	title = {Optimization for active learning-based interactive database exploration},
	volume = {12},
	number = {1},
	journal = {Proceedings of the VLDB Endowment},
	author = {Huang, Enhui and Peng, Liping and Palma, Luciano Di and Abdelkafi, Ahmed and Liu, Anna and Diao, Yanlei},
	year = {2018},
	note = {tex.publisher: VLDB Endowment},
	keywords = {interesting},
	pages = {71--84},
}

@book{hanneke_theory_nodate,
	title = {Theory of {Active} {Learning}},
	url = {http://www.stevehanneke.com/docs/active-survey/active-survey.pdf},
	abstract = {Active learning is a protocol for supervised machine learning, in whicha learning algorithm sequentially requests the labels of selected datapoints from a large pool of unlabeled data. This contrasts with passivelearning, where the labeled data are taken at random. The objective inactive learning is to produce a highly-accurate classifier, ideally usingfewer labels than the number of random labeled data sufficient for pas-sive learning to achieve the same. This article describes recent advancesin our understanding of the theoretical benefits of active learning, andimplications for the design of effective active learning algorithms. Muchof the article focuses on a particular technique, namely disagreement-based active learning, which by now has amassed a mature and coherentliterature. It also briefly surveys several alternative approaches fromthe literature. The emphasis is on theorems regarding the performanceof a few general algorithms, including rigorous proofs where appropri-ate. However, the presentation is intended to be pedagogical, focusingon results that illustrate fundamental ideas, rather than obtaining thestrongest or most general known theorems. The intended audience in-cludes researchers and advanced graduate students in machine learningand statistics, interested in gaining a deeper understanding of the re-cent and ongoing developments in the theory of active learning.},
	author = {Hanneke, Steve},
}

@article{marcus2019neo,
	title = {Neo: {A} learned query optimizer},
	journal = {arXiv preprint arXiv:1904.03711},
	author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
	year = {2019},
}

@inproceedings{sukhbaatar2015end,
	title = {End-to-end memory networks},
	booktitle = {Advances in neural information processing systems},
	author = {Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and {others}},
	year = {2015},
	pages = {2440--2448},
}

@article{idreos2019learning,
	title = {Learning {Key}-{Value} {Store} {Design}},
	journal = {arXiv preprint arXiv:1907.05443},
	author = {Idreos, Stratos and Dayan, Niv and Qin, Wilson and Akmanalp, Mali and Hilgard, Sophie and Ross, Andrew and Lennon, James and Jain, Varun and Gupta, Harshita and Li, David and {others}},
	year = {2019},
}

@article{rae_meta-learning_2019,
	title = {Meta-{Learning} {Neural} {Bloom} {Filters}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/rae19a.html},
	abstract = {There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression. In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In applications where inputs arrive at high throughput, or are ephemeral, training a network from scratch is not practical. This motivates the need for few-shot neural data structures. In this paper we explore the learning of approximate set membership over a set of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which is able to achieve significant compression gains over classical Bloom Filters and existing memory-augmented neural networks.},
	journal = {Proceedings of Machine Learning Research},
	author = {Rae, Jack and Bartunov, Sergey and Lillicrap, Timothy},
	year = {2019},
}

@article{youngren_multi-resolution_2017,
	title = {A multi-resolution {HEALPix} data structure for spherically mapped point data.},
	volume = {3},
	issn = {2405-8440 2405-8440 2405-8440},
	doi = {10.1016/j.heliyon.2017.e00332},
	abstract = {Data describing entities with locations that are points on a sphere are described as  spherically mapped. Several data structures designed for spherically mapped data  have been developed. One of them, known as Hierarchical Equal Area iso-Latitude  Pixelization (HEALPix), partitions the sphere into twelve diamond-shaped equal-area  base cells and then recursively subdivides each cell into four diamond-shaped  subcells, continuing to the desired level of resolution. Twelve quadtrees, one  associated with each base cell, store the data records associated with that cell and  its subcells. HEALPix has been used successfully for numerous applications, notably  including cosmic microwave background data analysis. However, for applications  involving sparse point data HEALPix has possible drawbacks, including inefficient  memory utilization, overwriting of proximate points, and return of spurious points  for certain queries. A multi-resolution variant of HEALPix specifically optimized  for sparse point data was developed. The new data structure allows different areas  of the sphere to be subdivided at different levels of resolution. It combines  HEALPix positive features with the advantages of multi-resolution, including reduced  memory requirements and improved query performance. An implementation of the new  Multi-Resolution HEALPix (MRH) data structure was tested using spherically mapped  data from four different scientific applications (warhead fragmentation  trajectories, weather station locations, galaxy locations, and synthetic locations).  Four types of range queries were applied to each data structure for each dataset.  Compared to HEALPix, MRH used two to four orders of magnitude less memory for the  same data, and on average its queries executed 72\% faster.},
	language = {eng},
	number = {6},
	journal = {Heliyon},
	author = {Youngren, Robert W. and Petty, Mikel D.},
	month = jun,
	year = {2017},
	pmid = {28721391},
	pmcid = {PMC5484980},
	pages = {e00332},
}

@misc{SDSSSqlLogs,
	title = {{SQL} {Logs}},
	url = {http://skyserver.sdss.org/log/en/traffic/sql.asp},
	author = {{SDSS}},
}

@misc{SDSSSamples,
	title = {Sample {SQL} {Queries}},
	url = {http://cas.sdss.org/dr15/en/help/docs/realquery.aspx},
	author = {{SDSS}},
}

@phdthesis{Dash2011,
	title = {Automated {Physical} {Design}: {A} {Combinatorial} {Optimization} {Approach}},
	abstract = {One of the most challenging tasks for the database administrator is physically de- signing the database (by selecting design features such as indexes, materialized views, and partitions) to attain optimal performance for a given workload. These features, however, impose storage and maintenance overhead on the database, thus requiring precise selection to balance the performance and the overhead. As the space of the de- sign features is vast, and their interactions hard to quantify, the DBAs spend enormous amount of resources to identify the optimal set of features.},
	school = {Intel},
	author = {Dash, Debabrata},
	year = {2011},
	keywords = {★},
}

@techreport{Noddle2015,
	title = {Euclid {SGS} {Architecture} and {Design} {Document} {Volume} 2: {Data} {Processing} ({Architecture} {Overview})},
	abstract = {This document created for the PRR (Preliminary Requirement Review) is updated for the SGS System Requirement Review (SRR), this is the 2.1 release. The objectives of this document are to recap the main structural requirements of the Euclid Consortium Science Ground Segment, to describe the constraints, to introduce the guiding principles of the hardware and software architecture and shortly to introduce the breakdown of the functional architecture.},
	author = {Noddle, K and Delouis, J M and Dabin, Christophe and Apostolakos, Nikolaos and Mellier, Yannick},
	year = {2015},
}

@techreport{Hoar2015,
	title = {{SGS} {Architecture} {Design} {Document} {Volume} 1: {Top}-{Level} {Structure}, {Data} {Management}},
	abstract = {This document provides a high level overview of the SGS architecture as a whole with the intent to provide the reader with references to documents which describe specific aspects of the SGS in more detail. It describes the logical and physical architecture of the SGS; the data management needs of the SGS; the role of the Euclid Archive in meeting these needs and a brief description of how that will be implemented through the Euclid Archive System (EAS).},
	author = {Hoar, John and Dabin, Christophe and Belikov, Andrey and Williams, Rees and Nieto, Sara and Arviset, Christophe and Buenadicha, Guillermo and Melchoir, Martin and Mellier, Yannick},
	year = {2015},
}

@article{Petersen2007,
	title = {Systematic {Mapping} {Studies} in {Software} {Engineering}},
	volume = {17},
	issn = {02181940},
	url = {http://dl.acm.org/citation.cfm?id=2227115.2227123},
	doi = {10.1142/S0218194007003112},
	abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
	journal = {12th International Conference on Evaluation and Assessment in Software Engineering},
	author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
	year = {2007},
	note = {ISBN: 0-7695-2555-5},
	keywords = {Evidence Based Software Engineering, Systematic Reviews, evidence based software engineering},
	pages = {10},
}

@article{Coifman2006,
	title = {Diffusion maps},
	issn = {10635203},
	doi = {10.1016/j.acha.2006.04.006},
	abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods. © 2006.},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, Stéphane},
	year = {2006},
	pmid = {5561994},
	note = {arXiv: math/0506090
ISBN: 1063-5203},
	keywords = {Diffusion metric, Diffusion processes, Dimensionality reduction, Eigenmaps, Graph Laplacian, Manifold learning},
}

@article{Moon2017,
	title = {{PHATE}: {A} {Dimensionality} {Reduction} {Method} for {Visualizing} {Trajectory} {Structures} in {High}-{Dimensional} {Biological} {Data}},
	url = {https://www.biorxiv.org/content/early/2017/03/24/120378},
	doi = {10.1101/120378},
	abstract = {In recent years, dimensionality reduction methods have become critical for visualization, exploration, and interpretation of high-throughput, high-dimensional biological data, as they enable the extraction of major trends in the data while discarding noise. However, biological data contains a type of predominant structure that is not preserved in commonly used methods such as PCA and tSNE, namely, branching progression structure. This structure, which is often non-linear, arises from underlying biological processes such as differentiation, graded responses to stimuli, and population drift, which generate cellular (or population) diversity. We propose a novel, affinity-preserving embedding called PHATE (Potential of Heat-diffusion for Affinity-based Trajectory Embedding), designed explicitly to preserve progression structure in data. PHATE provides a denoised, two or three-dimensional visualization of the complete branching trajectory structure in high-dimensional data. It uses heat-diffusion processes, which naturally denoise the data, to compute cell-cell affinities. Then, PHATE creates a diffusion-potential geometry by free-energy potentials of these processes. This geometry captures high-dimensional trajectory structures, while enabling a natural embedding of the intrinsic data geometry. This embedding accurately visualizes trajectories and data distances, without requiring strict assumptions typically used by path-finding and tree-fitting algorithms, which have recently been used for pseudotime orderings or tree-renderings of cellular data. Furthermore, PHATE supports a wide range of data exploration tasks by providing interpretable overlays on top of the visualization. We show that such overlays can emphasize and reveal trajectory end-points, branch points and associated split-decisions, progression-forming variables (e.g., specific genes), and paths between developmental events in cellular state-space. We demonstrate PHATE on single-cell RNA sequencing and mass cytometry data pertaining to embryoid body differentiation, IPSC reprogramming, and hematopoiesis in the bone marrow. We also demonstrate PHATE on non-single cell data including single-nucleotide polymorphism (SNP) measurements of European populations, and 16s sequencing of gut microbiota.},
	journal = {Doi.Org},
	author = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Chen, William and Hirn, Matthew J and Coifman, Ronald R and Ivanova, Natalia B and Wolf, Guy and Krishnaswamy, Smita},
	year = {2017},
	note = {Publisher: Cold Spring Harbor Laboratory},
	keywords = {★},
	pages = {120378},
}

@inproceedings{Idreos2019a,
	title = {From {Auto}-tuning {One} {Size} {Fits} {All} to {Self}-designed and {Learned} {Data}-intensive {Systems}},
	isbn = {978-1-4503-5643-5},
	doi = {10.1145/3299869.3314034},
	abstract = {We survey new opportunities to design data systems, data structures and algorithms that can adapt to both data and query workloads. Data keeps growing, hardware keeps changing and new applications appear ever more frequently. One size does not fit all, but data-intensive applications would like to balance and control memory requirements, read costs, write costs, as well as monetary costs on the cloud. This calls for tailored data systems, storage, and computation solutions that match the exact requirements of the scenario at hand. Such systems should be “synthesized” quickly and nearly automatically, removing the human system designers and administrators from the loop as much as possible to keep up with the quick evolution of applications and workloads. In addition, such systems should “learn” from both past and current system performance and workload patterns to keep adapting their design. We survey new trends in 1) self-designed, and 2) learned data systems and how these technologies can apply to relational, NoSQL, and big data systems as well as to broad data science applications. We focus on both recent research advances and practical applications of this technology, as well as numerous open research opportunities that come from their fusion. We specifically highlight recent work on data structures, algorithms, and query optimization, and how machine learning inspired designs as well as a detailed mapping of the possible design space of solutions can drive innovation to create tailored systems. We also position and connect with past seminal system designs and research in auto-tuning, modular/extensible, and adaptive data systems to highlight the new challenges as well as the opportunities to combine past and new technologies.},
	author = {Kraska, Tim and Idreos, Stratos},
	year = {2019},
}

@article{Neumann2014,
	title = {Efficiently compiling efficient query plans for modern hardware},
	issn = {21508097},
	doi = {10.14778/2002938.2002940},
	abstract = {As main memory grows, query performance is more and more determined by the raw CPU costs of query processing itself. The classical iterator style query processing technique is very simple and {\textbackslash}r{\textbackslash}nexible, but shows poor performance on modern CPUs due to lack of locality and frequent instruction mis-predictions. Several techniques like batch oriented processing or vectorized tuple processing have been proposed in the past to improve this situation, but even these techniques are frequently out-performed by hand-written execution plans. In this work we present a novel compilation strategy that translates a query into compact and e cient machine code using the LLVM compiler framework. By aiming at good code and data locality and predictable branch layout the resulting code frequently rivals the performance of hand-written C++ code. We integrated these techniques into the HyPer main memory database system and show that this results in excellent query performance while requiring only modest compilation time.},
	journal = {Proceedings of the VLDB Endowment},
	author = {Neumann, Thomas},
	year = {2014},
}

@article{Kersten:2018:EYA:3275366.3284966,
	title = {Everything you always wanted to know about compiled and vectorized queries but were afraid to ask},
	volume = {11},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3275366.3284966},
	doi = {10.14778/3275366.3275370},
	abstract = {The query engines of most modern database systems are either based on vectorization or data-centric code generation. These two state-of-the-art query processing paradigms are fundamentally different in terms of system structure and query execution code. Both paradigms were used to build fast systems. However, until today it is not clear which paradigm yields faster query execution, as many implementation-specific choices obstruct a direct comparison of ar-chitectures. In this paper, we experimentally compare the two models by implementing both within the same test system. This allows us to use for both models the same query processing algorithms, the same data structures, and the same parallelization framework to ultimately create an apples-to-apples comparison. We find that both are efficient, but have different strengths and weaknesses. Vector-ization is better at hiding cache miss latency, whereas data-centric compilation requires fewer CPU instructions, which benefits cache-resident workloads. Besides raw, single-threaded performance, we also investigate SIMD as well as multi-core parallelization and different hardware architectures. Finally, we analyze qualitative differences as a guide for system architects.},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kersten, Timo and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Pavlo, Andrew and Boncz, Peter},
	month = sep,
	year = {2018},
	note = {Publisher: VLDB Endowment},
	pages = {2209--2222},
}

@inproceedings{sablayrolles2018spreading,
	title = {Spreading vectors for similarity search},
	url = {https://openreview.net/forum?id=SkGuG2R5tm},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Jégou, Hervé},
	year = {2019},
}

@article{Idreos2018Periodic,
	title = {The {Periodic} {Table} of {Data} {Structures}},
	volume = {41},
	issn = {1939-1854},
	url = {https://www.semanticscholar.org/paper/The-Periodic-Table-of-Data-Structures-Idreos-Zoumpatianos/67296e6cd0084c301339889c4ef1f71a04406b3d},
	abstract = {We describe the vision of being able to reason about the design space of data structures. We break this down into two questions: 1) Can we know all data structures that is possible to design? 2) Can we compute the performance of arbitrary designs on a given hardware and workload without having to implement the design or even access the target hardware? If those challenges are possible, then an array of exciting opportunities would become feasible such as interactive what-if design to improve the productivity of data systems researchers and engineers, and informed decision making in industrial settings with regards to critical hardware/workload/data structure design issues. Then, even fully automated discovery of new data structure designs becomes possible. Furthermore, the structure of the design space itself provides numerous insights and opportunities such as the existence of design continuums that can lead to data systems with deep adaptivity, and a new understanding of the possible performance tradeoffs. Given the universal presence of data structures at the very core of any data-driven field across all sciences and industries, reasoning about their design can have significant benefits, making it more feasible (easier, faster and cheaper) to adopt tailored state-of-the-art storage solutions. And this effect is going to become increasingly more critical as data keeps growing, hardware keeps changing and more applications/fields realize the transformative power and potential of data analytics. This paper presents this vision and surveys first steps that demonstrate its feasibility.},
	number = {3},
	journal = {IEEE Data Eng. Bull},
	author = {Idreos, Stratos and Zoumpatianos, Konstantinos and Athanassoulis, Manos and Dayan, Niv and Hentschel, Brian and Kester, Michael S. and Guo, Demi and Maas, Lukas M. and Qin, Wilson and Wasay, Abdul and Sun, Yiyou},
	year = {2018},
	note = {ISBN: 0471416568},
	keywords = {★},
	pages = {64--75},
}

@inproceedings{Kraska2019,
	title = {{SageDB}: {A} {Learned} {Database} {System}},
	abstract = {Modern data processing systems are designed to be general purpose, in that they can handle a wide variety of different schemas, data types, and data distributions, and aim to provide efficient access to that data via the use of optimizers and cost models. This general purpose nature results in systems that do not take advantage of the characteristics of the particular application and data of the user. With SageDB we present a vision towards a new type of a data processing system, one which highly specializes to an application through code synthesis and machine learning. By model-ing the data distribution, workload, and hardware, SageDB learns the structure of the data and optimal access methods and query plans. These learned models are deeply embedded , through code synthesis, in essentially every component of the database. As such, SageDB presents radical departure from the way database systems are currently developed, raising a host of new problems in databases, machine learning and programming systems.},
	booktitle = {Conference on {Innovative} {Data} {Systems} {Research}},
	author = {Kraska, Tim and Alizadeh, Mohammad and Beutel, Alex and Chi, Ed H and Ding, Jialin and Kristo, Ani and Leclerc, Guillaume and Madden, Samuel and Mao, Hongzi and Nathan, Vikram},
	year = {2019},
}

@article{Idreos2019,
	title = {Design {Continuums} and the {Path} {Toward} {Self}-{Designing} {Key}-{Value} {Stores} that {Know} and {Learn}},
	abstract = {We introduce the concept of design continuums for the data layout of key-value stores. A design continuum unifies major distinct data structure designs under the same model. The critical insight and potential long-term impact is that such unifying models 1) render what we consider up to now as fundamentally different data structures to be seen as "views" of the very same overall design space, and 2) allow "seeing" new data structure designs with performance properties that are not feasible by existing designs. The core intuition behind the construction of design continuums is that all data structures arise from the very same set of fundamental design principles, i.e., a small set of data layout design concepts out of which we can synthesize any design that exists in the literature as well as new ones. We show how to construct , evaluate, and expand, design continuums and we also present the first continuum that unifies major data structure designs, i.e., B + tree, B tree, LSM-tree, and LSH-table. The practical benefit of a design continuum is that it creates a fast inference engine for the design of data structures. For example, we can near instantly predict how a specific design change in the underlying storage of a data system would affect performance, or reversely what would be the optimal data structure (from a given set of designs) given workload characteristics and a memory budget. In turn, these properties allow us to envision a new class of self-designing key-value stores with a substantially improved ability to adapt to workload and hardware changes by transitioning between drastically different data structure designs to assume a diverse set of performance properties at will.},
	author = {Idreos, Stratos and Dayan, Niv and Qin, Wilson and Akmanalp, Mali and Hilgard, Sophie and Ross, Andrew and Lennon, James and Jain, Varun and Gupta, Harshita and Li, David and Zhu, Zichen},
	keywords = {★},
}

@article{Tatsuma2011,
	title = {Diffusion {Hashing}},
	abstract = {With the worldwide spread of the broadband In- ternet, massive multimedia data including texts, images, and videos are increasing explosively and available for interactive applications over the Internet. At the same time, more and more attention has been paid to aiming at fast retrieval from massive multimedia databases. Hash-based Approximate Nearest Neigh- bor (ANN) search is a technology that achieves fast retrieval by regarding the hash key as a retrieval index, where the similarity of data is maintained and embedded in the neighborhood of the hash key. In other words, the closer the Hamming codes between hash keys, the more similar the data become. In general, short binary codes are preferred for storing hash keys and values. The difficulty is to define the similarity between data and reflect it in binary codes. In this paper, we propose Diffusion Hashing (DH) as a novel ANN search technique based on hashing with an anisotropic diffusion kernel. DH aims to transform the search index into as short binary codes as possible, preserving the similarity induced by random walk on the data manifold in higher dimensional space. From comparative experiments, we will demonstrate that DH outperforms previously known hash-based ANN search techniques including Locality Sensitive Hashing and Spectral Hashing. I.},
	author = {Tatsuma, Atsushi and Aono, Masaki},
	year = {2011},
}

@book{Lohr2009,
	title = {Sampling: design and analysis},
	publisher = {Nelson Education},
	author = {Lohr, Sharon},
	year = {2009},
}

@article{ZHANG20131341,
	title = {Systematic reviews in software engineering: {An} empirical investigation},
	volume = {55},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584912002029},
	doi = {https://doi.org/10.1016/j.infsof.2012.09.008},
	abstract = {Background Systematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE. Objective The main objective of this research is to empirically investigate the adoption, value, and use of SLRs in SE research from various perspectives. Method We used mixed-methods approach (systematically integrating tertiary literature review, semi-structured interviews and questionnaire-based survey) as it is based on a combination of complementary research methods which are expected to compensate each others’ limitations. Results A large majority of the participants are convinced of the value of using a rigourous and systematic methodology for literature reviews in SE research. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between methodological rigour and required effort. Conclusions SLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this relatively new methodology to SE research. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers about what can be expected from conducting SLRs and the potential impact of such reviews.},
	number = {7},
	journal = {Information and Software Technology},
	author = {Zhang, He and Babar, Muhammad Ali},
	year = {2013},
	keywords = {Evidence-based software engineering, Methodology adoption, Mixed-methods research, Research methodology, Systematic Reviews, Tertiary study},
	pages = {1341--1354},
}

@incollection{Deeks2001,
	title = {Undertaking {Systematic} {Reviews} of {Research} on {Effectiveness} {CRD} ' s {Guidance} for those {Carrying} {Out} or {Commissioning} {Reviews}},
	volume = {4},
	isbn = {1-900640-20-1},
	url = {http://opensigle.inist.fr/handle/10068/534964},
	abstract = {Policies and decisions about the organisation and delivery of health care and new research should be informed by comprehensive reviews evaluating the available research evidence. Robust systematic literature valuable source of information because reviews are a by locating, appraising and synthesising evidence from primary studies, they provide empirical answers to focussed questions about health care and related issues. In addition, by identifying both what we know and dont know, they help in planning new research. Systematic reviews differ from traditional reviews and commentaries produced by content experts in that they adhere to a scientific methodology which seeks to minimise bias and errors. Hence, rather than reflecting the views of experts, they generate balanced inferences based on the collation of the best available evidence. The NHS Centre for Reviews and Dissemination (CRD) first produced guidance for undertaking systematic reviews in 1996 in its Report Number 4. The original document provided a framework for carrying out systematic reviews of effectiveness and was used extensively to ensure a high standard in conducting reviews both by CRD and by other research groups. Although the principles behind reviewing research evidence systematically remain the same, various aspects of review methodology have developed substantially since that time. Therefore, the second edition of Report Number 4 updates the original guidance on effectiveness reviews. Recently, the growing use of systematic reviews in health policy and decision-making has placed new demands on reviewers who are increasingly being required to meet multiple objectives in reviews. The second edition responds to these developments by providing new guidance on the evaluation of tests, qualitative research and health economics in the context of effectiveness reviews. The purpose of this document is to provide practical guidance about various aspects of reviews in the light of current understanding of review methodology.},
	booktitle = {{CRD} {Report} {Number} 4 2nd {Edition}},
	author = {Bidwell, Susan and Chalmers, Sir Iain and Clarke, Mike and Crosbie, Gerry and Eastwood, Alison and Fry-smith, Ann and Harbour, Robin and Lewis, Ruth},
	year = {2001},
	doi = {1900640201},
	note = {Issue: 4
ISSN: 1900640201},
	pages = {152},
}

@article{Lamanna2016,
	title = {Large-scale data services for science: {Present} and future challenges},
	volume = {13},
	issn = {1531-8567},
	url = {https://doi.org/10.1134/S1547477116050344},
	doi = {10.1134/S1547477116050344},
	abstract = {I discuss some directions for evolving our data management services in the next years, using the experience in operating heavy-duty data storage services at CERN, notably for the Worldwide LHC Computing Grid (WLCG). These new developments are potentially useful beyond our community, wherever the success of a project depends on large computing resources and requires the active participation of a large and distributed collaborations.},
	number = {5},
	journal = {Physics of Particles and Nuclei Letters},
	author = {Lamanna, Massimo},
	month = sep,
	year = {2016},
	pages = {676--680},
}

@techreport{EOSOps2018,
	title = {{EOS} {Ops} at {CERN}},
	url = {https://indico.cern.ch/event/656157/contributions/2866314/},
	author = {Rousseau, Herve},
	year = {2018},
}

@article{Peters2015,
	title = {{EOS} as the present and future solution for data storage at {CERN}},
	volume = {664},
	url = {http://stacks.iop.org/1742-6596/664/i=4/a=042042},
	abstract = {EOS is an open source distributed disk storage system in production since 2011 at CERN. Development focus has been on low-latency analysis use cases for LHC 1 and non- LHC experiments and life-cycle management using JBOD 2 hardware for multi PB storage installations. The EOS design implies a split of hot and cold storage and introduced a change of the traditional HSM 3 functionality based workflows at CERN. The 2015 deployment brings storage at CERN to a new scale and foresees to breach 100 PB of disk storage in a distributed environment using tens of thousands of (heterogeneous) hard drives. EOS has brought to CERN major improvements compared to past storage solutions by allowing quick changes in the quality of service of the storage pools. This allows the data centre to quickly meet the changing performance and reliability requirements of the LHC experiments with minimal data movements and dynamic reconfiguration. For example, the software stack has met the specific needs of the dual computing centre set-up required by CERN and allowed the fast design of new workflows accommodating the separation of long-term tape archive and disk storage required for the LHC Run II. This paper will give a high-level state of the art overview of EOS with respect to Run II, introduce new tools and use cases and set the roadmap for the next storage solutions to come.},
	number = {4},
	journal = {Journal of Physics: Conference Series},
	author = {Peters, A J and Sindrilaru, E A and Adde, G},
	year = {2015},
	pages = {42042},
}

@misc{Hookway2008,
	title = {Pragmatism},
	url = {https://plato.stanford.edu/entries/pragmatism/},
	abstract = {Pragmatism was a philosophical tradition that originated in the United States around 1870. The most important of the ‘classical pragmatists’ were Charles Sanders Peirce (1839–1914), William James (1842–1910) and John Dewey (1859–1952). The influence of pragmatism declined during the first two thirds of the twentieth century, but it has undergone a revival since the 1970s with philosophers being increasingly willing to use the writings and ideas of the classical pragmatists, and also a number of thinkers, such as Richard Rorty, Hilary Putnam and Robert Brandom developing philosophical views that represent later stages of the pragmatist tradition. The core of pragmatism was the pragmatist maxim, a rule for clarifying the contents of hypotheses by tracing their ‘practical consequences’. In the work of Peirce and James, the most influential application of the pragmatist maxim was to the concept of truth. But the pragmatists have also tended to share a distinctive epistemological outlook, a fallibilist anti-Cartesian approach to the norms that govern inquiry.},
	journal = {Stanford Encyclopedia of Philosophy},
	author = {Hookway, Christopher},
	year = {2008},
}

@article{Pratt2016,
	title = {Pragmatism as ontology, not ({Just}) epistemology: {Exploring} the full horizon of pragmatism as an approach to {IR} theory},
	volume = {18},
	issn = {14682486},
	doi = {10.1093/isr/viv003},
	abstract = {This article is in many ways a pragmatist’s critique of pragmatism in international relations (IR), focusing on what practices scholars have engaged in by drawing upon pragmatism and how to resolve problems that become apparent in considering them. Scholars of IR have used pragmatism largely (though not exclusively) to examine issues of an epistemological or methodological nature, focusing mainly on pragmatism as a philosophy of science. Often overlooked, however, is that pragmatism is not just a philosophy of science, but a distinctive and in some respects quite radical school of metaphysics, and it implies a particularly flexible form of social ontology. I, thus, argue for broader horizons in pragmatist theory in IR. I criticize the overly epistemological or methodological focus of the existing ways many IR scholars have used pragmatism and discuss of how pragmatist social theory fits within existing scholarship in the field. Finally, I suggest how pragmatist social theory can contribute to ongoing IR research programs by dissolving the dualisms of agent and structure, realism and idealism, and normative and strategic action. In other words, as a coherent set of principles, pragmatism offers the foundations for a new movement in the study of international politics—indeed, such a movement has already begun, and I suggest that its horizons are particularly broad.},
	number = {3},
	journal = {International Studies Review},
	author = {Pratt, Simon Frankel},
	year = {2016},
	note = {ISBN: 1521-9488{\textbackslash}r1468-2486},
	keywords = {Practice theory, Pragmatism},
	pages = {508--527},
}

@article{Haack1977,
	title = {Pragmatism and {Ontology}: {Peirce} and {James}},
	volume = {31},
	issn = {00488143, 20330138},
	url = {http://www.jstor.org/stable/23944090},
	number = {121/122 (3/4)},
	journal = {Revue Internationale de Philosophie},
	author = {HAACK, SUSAN},
	year = {1977},
	note = {Publisher: Revue Internationale de Philosophie},
	pages = {377--400},
}

@article{Chua1986,
	title = {Radical {Developments} in {Accounting} {Thought}},
	issn = {0001-4826},
	doi = {10.2307/247360},
	abstract = {Mainstream accounting is grounded in a common set of philosophical assumptions about knowledge, the empirical world, and the relationship between theory and practice. This particular world-view, with its emphasis on hypothetico-deductivism and technical control, possesses certain strengths but has restricted the range of problems studied and the use of research methods. By changing this set of assumptions, fundamentally different and potentially rich research insights are obtained. Two alternative world-views and their underlying assumptions am elucidated-the interpretive and the critical. The consequences of conducting research within these philosophical traditions are discussed via a comparison between accounting research that is conducted on the "same" problem but from two different perspectives. In addition, some of the difficulties associated with these alternative perspectives are briefly dealt with},
	journal = {The Accounting Review},
	author = {Chua, Wai Fong},
	year = {1986},
	pmid = {4479026},
	note = {ISBN: 00014826},
}

@article{Orlikowski1991,
	title = {Studying information technology in organizations: {Research} approaches and assumptions},
	issn = {15265536},
	doi = {10.1287/isre.2.1.1},
	abstract = {We examined 155 information systems research articles published from 1983 to 1988 and found that although this research is not rooted in a single over- arching theoretical perspective, it does exhibit a single set of philosophical assumptions regarding the nature of the phenomena studied by information systems researchers, and what constitutes valid knowledge about those phe- nomena. We believe that a single research perspective for studying informa- tion systems phenomena is unnecessarily restrictive, and argue that there exist other philosophical assumptions that can inform studies of the relation- ships between information technology, people, and organizations. In this paper, we present two additional research philosophies for consideration-the interpretive and the critical-and for each we provide empirical examples to illustrate how they are used. We conclude by suggesting that much can be gained if a plurality of research perspectives is effectively employed to investi- gate information systems phenomena.},
	journal = {Information Systems Research},
	author = {Orlikowski, Wanda J. and Baroudi, Jack J.},
	year = {1991},
	pmid = {4431364},
	note = {arXiv: 1011.1669v3
ISBN: 1047-7047},
	keywords = {Critical research, Philosophical assumptions, Positmst research- Inlerprelivist research, Research approaches},
}

@book{Kuhn1970,
	title = {The structure of scientific revolutions},
	isbn = {0226458032; 0226458040; 226458032; 9780226458038; 9780226458045},
	abstract = {Contents: 01 Introduction: A role for history -- The route to normal science -- The nature of normal science -- Normal science as puzzle-solving -- The priority of paradigms -- Anomaly and the emergence of scientific discoveries -- Crisis and the emergence of scientific theories -- The response to crisis -- The nature and necessity of scientific revolutions -- Revolutions as changes of world view -- The invisibility of revolutions -- The resolution of revolutions -- Progress through revolutions.},
	author = {Kuhn, Thomas S and Cooper, Cary donor and Cooper, Raymond D donor and University of Chicago Press, Publisher},
	year = {1970},
	note = {Publication Title: International encyclopedia of unified science ; v. 2, no. 2.},
	keywords = {Imprint -- Illinois -- Chicago -- 1970, Philosophy, ProgrÃ¨s},
}

@book{Ritzer1991,
	title = {The {Paradigm} {Dialog} by {Egon} {Guba}},
	volume = {16},
	url = {http://www.jstor.org/stable/3340973},
	abstract = {"The contributors and editor of this work are to be commended for their successful efforts in delineating many of the concerns current within education and in calling for frank debate on these issues by all interested parties. Furthermore, they have stimulated good scholarship by readily admitting to the current state of affairs being one of more questions than answers and more confusion than clarity. They thus remind us that the search for knowledge is one fraught with conflict in a public arena. "The appropriate audience for this volume is assessed to be the reader who derives satisfaction from critical thinking. It would be appropriate for graduate students in education, human services, social sciences, or theology, or any person committed to the endeavor and process of education. "The Paradigm Dialog is one of those rare books that simultaneously stretches the mind while projecting one into self-reflection. For the applied practitioner, whether teacher, counselor, or consultant, the possibility of gaining further insight into the underlying assumptions which constrain one's pedagogy or practice is highly possible upon a critical reading." -The Journal of Applied Rehabilitation Counseling Is scientific positivism, long the reigning paradigm for research in the social sciences, the "best way" to conduct social research? This is the central question examined in The Paradigm Dialog. Recently three key challengers have appeared-postpositivism, critical theory, and constructivism. All three offer researchers new methodological approaches, and all three present fundamental questions that must be addressed. Can research be conducted between paradigms? Are they equally useful in answering questions of applied research? What constitutes good, or ethical, research in each? In this volume, these and other significant questions are examined by a multidisciplinary group of leading figures in qualitative research. Not surprisingly, there is no agreement on the "best" paradigm question, but the dialog offered in this compelling volume deftly explores important issues in selecting the proper paradigm for tackling a variety of research questions. With a group of contributors that reads like a veritable who's who in qualitative research, The Paradigm Dialog is a must for anyone conducting research in the social sciences.},
	author = {Ritzer, George},
	year = {1991},
	doi = {10.2307/3340973},
	note = {Publication Title: The Canadian Journal of Sociology
Issue: 4
ISSN: 03186431},
}

@article{Lee2004,
	title = {Thinking about social theory and philosophy for information systems},
	url = {http://scholar.google.com/scholar?q=intitle:Thinking+about+social+theory+and+philosophy+for+information+systems#0},
	doi = {citeulike-article-id:7887678},
	abstract = {The phrase 'social theory and philosophy for information systems' invites an examination of following terms: social, theory, philosophy, information, systems, information systems, philosophy for information systems, and social theory for information systems. I shall refrain (no doubt to the relief of the reader) from providing the definition and scholarly treatment that each and every one of these terms deserves. Instead, I will pursue just a few issues where my intention is to suggest an imagination that is helpful to thinking about philosophy, social theory and information systems. Much as C. Wright Mills (1959) sought to instill in his readers a sense of what he called 'the sociological imagination', I will attempt to suggest to the reader a means of thinking about philosophy, social theory and information systems that, in a way, is more important than whatever the content of such thinking might be. The content of such thinking can and should change from},
	journal = {Social theory and philosophy for Information Systems},
	author = {Lee, Allen S.},
	year = {2004},
	note = {ISBN: 0470851171},
	pages = {1--26},
}

@article{Baccarini1999,
	title = {Logical {Framework} {Method} for {Defining} {Project} {Success}},
	volume = {30},
	url = {https://www.pmi.org/learning/library/logical-framework-method-defining-project-success-5309},
	abstract = {Project success is a core concept of project management but its definition remains elusive. The project team must have a clearunderstanding of their project success objectives. This paper uses the logical framework method (LFM) as a foundation fordefining project success. Using LFM, four levels of project objectives are identified: goal, purpose, output, and input. It isproposed that project success consists of two components-product success and project management success. Productsuccess deals with goal and purpose; project management success deals with outputs and inputs.},
	number = {4},
	journal = {Project Management Journal},
	author = {Baccarini, David},
	year = {1999},
	keywords = {logical framework method, product/project success, project management success, project objectives},
	pages = {25--32},
}

@book{Pahl1984,
	title = {Engineering design},
	isbn = {978-0-85072-124-9},
	url = {https://books.google.ch/books?id=DYpRAAAAMAAJ},
	publisher = {Design Council},
	author = {Pahl, G and Beitz, W and Wallace, K},
	year = {1984},
}

@book{Pahl2007,
	title = {Engineering design: a systematic approach},
	isbn = {1-84628-318-3},
	url = {http://www.amazon.com/Engineering-Design-Systematic-Gerhard-Pahl/dp/1846283183},
	abstract = {The structure and content of the third edition forms the basis of the fourth edition. The topic of product planning has been extended through the integration of methods such as portfolio analysis and scenario planning. New sections have been introduced on effective organisation structures, on applying simultaneous engineering, on leadership and on team behaviour. The increasing importance of qual- ity assurance has reinforced the need to adopt systematic engineering design as a primary measure. This should be extended through the application of secondary measures, such as Quality Function Deployment (QFD) using the House of Quality. Developments in the area of sustainability have led to modifications in the section on design for recycling. Because of its general technical and economic importance, a new section on design to minimise wear has been introduced. The method of target costing has been included in the chapter on design for minimum cost. Finally, the chapter on CAD required updating.},
	publisher = {Springer London},
	author = {Pahl, G and Beitz, Wolfgang and Feldhusen, J and Grote, Karl-Heinrich},
	year = {2007},
	pmid = {23301817},
	doi = {10.1111/dsu.12130},
	note = {Series Title: Solid mechanics and its applications
Publication Title: Springer
Issue: 2
ISSN: 1524-4725},
}

@article{Zelkowitz1998,
	title = {Experimental models for validating technology},
	volume = {31},
	issn = {0018-9162},
	doi = {10.1109/2.675630},
	abstract = {Experimentation helps determine the effectiveness of proposed theories and methods. However, computer science has not developed a concise taxonomy of methods for demonstrating the validity of new techniques. Experimentation is a crucial part of attribute evaluation and can help determine whether methods used in accordance with some theory during product development will result in software being as effective as necessary. By looking at multiple examples of technology validation, the authors develop a taxonomy for software engineering experimentation that describes twelve different experimental approaches},
	number = {5},
	journal = {Computer},
	author = {Zelkowitz, M V and Wallace, D R},
	month = may,
	year = {1998},
	keywords = {Costs, Hardware, Paper technology, Particle measurements, Software measurement, Software safety, Testing, Time measurement, experimental models, product development, software engineering, software engineering experimentation, technology validation},
	pages = {23--31},
}

@article{Koen1988,
	title = {Toward a {Definition} of the {Engineering} {Method}},
	volume = {13},
	issn = {14695898},
	doi = {10.1080/03043798808939429},
	abstract = {Study of the engineering method is important in order to understand our world. The environment of man is a collage of engineering problem solutions. Political alliances and economic structures have changed dramatically as a result of the telephone, the computer, the atomic bomb, and space explorationÂ—all undeniably products of the engineering method. Look around the room in which you are now sitting. What do you find that was not developed, produced, or delivered by the engineer? What could be more important than to understand the strategy for change whose results surround us now and, some think, threaten to suffocate, to pollute, and to bomb us out of existence? This article redresses this situation by presenting a philosophically justifiable definition of Engineering Method that is gaining wide acceptance in the United States.},
	number = {3},
	journal = {European Journal of Engineering Education},
	author = {Koen, Billy V.},
	year = {1988},
	note = {ISBN: 0878231013},
	pages = {307--315},
}

@article{BRINKKEMPER1996,
	title = {Method engineering: engineering of information systems development methods and tools},
	volume = {38},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/0950584995010599},
	doi = {https://doi.org/10.1016/0950-5849(95)01059-9},
	abstract = {This paper proposes the term method engineering for the research field of the construction of information systems development methods and tools. Some research issues in method engineering are identified. One major research topic in method engineering is discussed in depth: situational methods, i.e. the configuration of a project approach that is tuned to the project at hand. A language and support tool for the engineering of situational methods are discussed.},
	number = {4},
	journal = {Information and Software Technology},
	author = {Brinkkemper, Sjaak},
	year = {1996},
	keywords = {Information systems development methods, Information systems research, Method engineering, Situational methods},
	pages = {275--280},
}

@book{Dym2012,
	edition = {2nd},
	title = {Engineering {Design}: {Representation} and {Reasoning}},
	isbn = {978-0-521-51429-3},
	url = {http://books.google.co.uk/books?id=Y61wmw2MPFIC},
	abstract = {Contrary to a lot of popular mythology, the designs of popular products and successful systems do not appear suddenly, or magically. The authors believe that symbolic representation, and related problem-solving methods, offer significant opportunities to clarify and articulate concepts of design to lay a better framework for design research and design education. Artificial Intelligence (AI) provides a substantial body of material concerned with understanding and modeling cognitive processes. This book adopts the vocabulary and paradigms of AI to enhance the presentation and explanation of design. It includes concepts from AI because of their explanatory power and because of their utility as possible ingredients of practical design activity. This second edition has been enriched by the inclusion of recent work on design reasoning, computational design, AI in design, and design cognition, with pointers to a wide cross section of the current literature.},
	publisher = {Cambridge University Press},
	author = {Dym, Clive L and Brown, David C},
	year = {2012},
	doi = {10.1017/CBO9781139031813},
	note = {Issue: 1990},
}

@misc{PostgreSQL,
	title = {{PostgreSQL}},
	url = {https://www.postgresql.org/},
}

@inproceedings{Park2017,
	address = {New York, NY, USA},
	title = {Database {Learning}: {Toward} a {Database} that {Becomes} {Smarter} {Every} {Time}},
	isbn = {978-1-4503-4197-4},
	url = {http://doi.acm.org/10.1145/3035918.3064013},
	doi = {10.1145/3035918.3064013},
	abstract = {In today's databases, previous query answers rarely benefit answering future queries. For the first time, to the best of our knowledge, we change this paradigm in an approximate query processing (AQP) context. We make the following observation: the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset. Exploiting and refining this knowledge should allow us to answer queries more analytically, rather than by reading enormous amounts of raw data. Also, processing more queries should continuously enhance our knowledge of the underlying distribution, and hence lead to increasingly faster response times for future queries. We call this novel idea---learning from past query answers---Database Learning. We exploit the principle of maximum entropy to produce answers, which are in expectation guaranteed to be more accurate than existing sample-based approximations. Empowered by this idea, we build a query engine on top of Spark SQL, called Verdict. We conduct extensive experiments on real-world query traces from a large customer of a major database vendor. Our results demonstrate that database learning supports 73.7\% of these queries, speeding them up by up to 23.0x for the same accuracy level compared to existing AQP systems.},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Park, Yongjoo and Tajik, Ahmad Shahab and Cafarella, Michael and Mozafari, Barzan},
	year = {2017},
	note = {Series Title: SIGMOD '17},
	keywords = {cluster:Query Approximation, database learning, layer:Middleware, machine learning, maximum entropy principle, online aggregation, supercluster:Interactive Performance Optimizations, type:Philosophical Paper},
	pages = {587--602},
}

@inproceedings{Fayyad1996a,
	title = {Knowledge {Discovery} and {Data} {Mining}: {Towards} a {Unifying} {Framework}},
	url = {http://dl.acm.org/citation.cfm?id=3001460.3001477},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {AAAI Press},
	author = {Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
	year = {1996},
	note = {Series Title: KDD'96},
	pages = {82--88},
}

@book{Reinartz1999,
	address = {Berlin, Heidelberg},
	title = {Focusing {Solutions} for {Data} {Mining} {Analytical} {Studies} and {Experimental} {Results} in {Real}-{World} {Domains}},
	isbn = {3-540-66429-7},
	publisher = {Springer-Verlag},
	author = {Reinartz, Thomas},
	year = {1999},
	note = {Publication Title: Case Analysis},
}

@article{Brachman1993,
	title = {Integrated support for data archaeology},
	volume = {2},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218215793000083},
	doi = {10.1142/S0218215793000083},
	number = {02},
	journal = {International Journal of Intelligent and Cooperative Information Systems},
	author = {Brachman, Ronald J and Selfridge, Peter G and Terveen, Loren G and Altman, Boris and Borgida, Alex and Halper, Fern and Kirk, Thomas and Lazar, Alan and McGuinness, Deborah L and Resnick, Lori Alperin},
	year = {1993},
	note = {ISBN: 0218-2157},
	pages = {159--185},
}

@article{Fayyad1996b,
	title = {The {KDD} {Process} for {Extracting} {Useful} {Knowledge} from {Volumes} of {Data}},
	volume = {39},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/240455.240464},
	doi = {10.1145/240455.240464},
	number = {11},
	journal = {Commun. ACM},
	author = {Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
	month = nov,
	year = {1996},
	note = {Publisher: ACM
Place: New York, NY, USA},
	pages = {27--34},
}

@misc{Dodero2017,
	title = {Exploratory {Data} {Analysis}},
	author = {Dodero, Juan Manuel},
	year = {2017},
	note = {Pages: 17},
}

@inproceedings{Begoli2012,
	title = {Design {Principles} for {Effective} {Knowledge} {Discovery} from {Big} {Data}},
	doi = {10.1109/WICSA-ECSA.212.32},
	booktitle = {2012 {Joint} {Working} {IEEE}/{IFIP} {Conference} on {Software} {Architecture} and {European} {Conference} on {Software} {Architecture}},
	author = {Begoli, E and Horey, J},
	month = aug,
	year = {2012},
	keywords = {Big Data, Computer architecture, Data handling, Data storage systems, Data visualization, Information management, Organizations, data collection processes, data dissemination practices, data mining, design principles, federal agency, knowledge discovery process, massive datasets, organizations, research and development, software architecture, system design principles},
	pages = {215--218},
}

@article{Geer2014,
	title = {Exploring with a {Purpose}},
	volume = {39},
	abstract = {Learning pays compound interest; as a person studies a subject, the more capable they become at learning even more about the subject. Just as a student cannot tackle the challenges of calculus without studying the prerequisites, we must have diligence in how we discover and build the prerequisite knowledge within cybersecurity.},
	number = {June},
	journal = {;login:},
	author = {Geer, Dan and Jacobs, Jay},
	year = {2014},
	pages = {47--49},
}

@book{Jacobs2014,
	title = {Data-{Driven} {Security}: {Analysis}, {Visualization} and {Dashboards}},
	isbn = {978-1-118-79372-5},
	abstract = {Uncover hidden patterns of data and respond with countermeasures Security professionals need all the tools at their disposal to increase their visibility in order to prevent security breaches and attacks. This careful guide explores two of the most powerful data analysis and visualization. You'll soon understand how to harness and wield data, from collection and storage to management and analysis as well as visualization and presentation. Using a hands-on approach with real-world examples, this book shows you how to gather feedback, measure the effectiveness of your security methods, and make better decisions. Everything in this book will have practical application for information security professionals. Helps IT and security professionals understand and use data, so they can thwart attacks and understand and visualize vulnerabilities in their networks Includes more than a dozen real-world examples and hands-on exercises that demonstrate how to analyze security data and intelligence and translate that information into visualizations that make plain how to prevent attacks Covers topics such as how to acquire and prepare security data, use simple statistical methods to detect malware, predict rogue behavior, correlate security events, and more Written by a team of well-known experts in the field of security and data analysis Lock down your networks, prevent hacks, and thwart malware by improving visibility into the environment, all through the power of data and Security Using Data Analysis, Visualization, and Dashboards.},
	author = {Jacobs, Jay and Rudis, Bob},
	year = {2014},
	doi = {10.1017/CBO9781107415324.004},
	note = {arXiv: 1011.1669v3
ISSN: 1098-6596},
	keywords = {★},
}

@book{Conway2012,
	title = {Machine {Learning} for {Hackers}: {Case} {Studies} and {Algorithms} to {Get} {You} {Started}},
	isbn = {978-1-4493-0371-6},
	abstract = {If you’re an experienced programmer interested in crunching data, this book will get you started with machine learning—a toolkit of algorithms that enables computers to train themselves to automate useful tasks. Authors Drew Conway and John Myles White help you understand machine learning and statistics tools through a series of hands-on case studies, instead of a traditional math-heavy presentation. Each chapter focuses on a specific problem in machine learning, such as classification, prediction, optimization, and recommendation. Using the R programming language, you’ll learn how to analyze sample datasets and write simple machine learning algorithms. Machine Learning for Hackers is ideal for programmers from any background, including business, government, and academic research. Develop a naïve Bayesian classifier to determine if an email is spam, based only on its text Use linear regression to predict the number of page views for the top 1,000 websites Learn optimization techniques by attempting to break a simple letter cipher Compare and contrast U.S. Senators statistically, based on their voting records Build a “whom to follow” recommendation system from Twitter data},
	author = {Conway, Drew and White, John Myles},
	year = {2012},
	pmid = {25246403},
	doi = {10.1017/CBO9781107415324.004},
	note = {arXiv: 1011.1669v3
Publication Title: O' Reilly
ISSN: 0717-6163},
	keywords = {Machine Learning, ★},
}

@inproceedings{Olken1986,
	address = {San Francisco, CA, USA},
	title = {Simple {Random} {Sampling} from {Relational} {Databases}},
	isbn = {0-934613-18-4},
	url = {http://dl.acm.org/citation.cfm?id=645913.671474},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Very} {Large} {Data} {Bases}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Olken, Frank and Rotem, Doron},
	year = {1986},
	note = {Series Title: VLDB '86},
	pages = {160--169},
}

@inproceedings{Yongjoo2017,
	title = {Active database learning},
	abstract = {Learning from Past Query Processing},
	booktitle = {{CIDR}'17},
	author = {Yongjoo, Park},
	year = {2017},
	pages = {2},
}

@techreport{Settles2010,
	title = {Active {Learning} {Literature} {Survey}},
	abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
	institution = {University of Wisconsin–Madison},
	author = {Settles, Burr},
	year = {2010},
	pages = {67},
}

@incollection{Efron1992,
	address = {New York, NY},
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	isbn = {978-1-4612-4380-9},
	url = {https://doi.org/10.1007/978-1-4612-4380-9_41},
	abstract = {We discuss the following problem given a random sample X = (X 1, X 2,…, X n) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = \$\${\textbackslash}theta {\textbackslash}left( \{{\textbackslash}hat F\} {\textbackslash}right) - {\textbackslash}theta {\textbackslash}left( F {\textbackslash}right)\$\$ , \$θ\$ some parameter of interest.) A general method, called the ``bootstrap'', is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	booktitle = {Breakthroughs in {Statistics}: {Methodology} and {Distribution}},
	publisher = {Springer New York},
	author = {Efron, Bradley},
	editor = {Kotz, Samuel and Johnson, Norman L},
	year = {1992},
	doi = {10.1007/978-1-4612-4380-9_41},
	pages = {569--593},
}

@inproceedings{Stonebraker2005,
	title = {One size fits all: an idea whose time has come and gone},
	doi = {10.1109/ICDE.2005.1},
	abstract = {The last 25 years of commercial DBMS development can be summed up in a single phrase: “One size fits all”. This phrase refers to the fact that the traditional DBMS architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the data- warehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into},
	booktitle = {21st {International} {Conference} on {Data} {Engineering} ({ICDE}'05)},
	author = {Stonebraker, M and Cetintemel, U},
	year = {2005},
	note = {ISSN: 1063-6382},
	keywords = {Computer architecture, Costs, Data warehouses, Databases, Delay, Engines, Laboratories, Prototypes, data warehouses, data-warehouse market, database management system, database market, relational database, relational databases, stream-processing market},
	pages = {2--11},
}

@article{Muhlbauer2013,
	title = {Instant {Loading} for {Main} {Memory} {Databases}},
	volume = {6},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/2556549.2556555},
	doi = {10.14778/2556549.2556555},
	number = {14},
	journal = {Proc. VLDB Endow.},
	author = {Mühlbauer, Tobias and Rödiger, Wolf and Seilbeck, Robert and Reiser, Angelika and Kemper, Alfons and Neumann, Thomas},
	year = {2013},
	note = {Publisher: VLDB Endowment},
	pages = {1702--1713},
}

@misc{FlorinRusu2013,
	title = {A {Survey} on {Array} {Storage}, {Query} {Languages}, and {Systems}},
	url = {https://arxiv.org/abs/1302.0103},
	abstract = {Since scientific investigation is one of the most important providers of massive amounts of ordered data, there is a renewed interest in array data processing in the context of Big Data. To the best of our knowledge, a unified resource that summarizes and analyzes array processing research over its long existence is currently missing. In this survey, we provide a guide for past, present, and future research in array processing. The survey is organized along three main topics. Array storage discusses all the aspects related to array partitioning into chunks. The identification of a reduced set of array operators to form the foundation for an array query language is analyzed across multiple such proposals. Lastly, we survey real systems for array processing. The result is a thorough survey on array data storage and processing that should be consulted by anyone interested in this research topic, independent of experience level. The survey is not complete though. We greatly appreciate pointers towards any work we might have forgotten to mention.},
	author = {Florin Rusu, Yu Cheng},
	year = {2013},
}

@misc{MonetDB,
	title = {{MonetDB}},
	url = {https://www.monetdb.org/},
}

@inproceedings{Vancea2012,
	address = {Berlin, Heidelberg},
	title = {Cooperative {Database} {Caching} within {Cloud} {Environments}},
	isbn = {978-3-642-30633-4},
	abstract = {Semantic caching is a technique used for optimizing the evaluation of database queries by caching results of old queries and using them when answering new queries. CoopSC is a cooperative database caching architecture, which extends the classic semantic caching approach by allowing clients to share their local caches in a cooperative matter. Thus, this approach decreases the response time of database queries and the amount of data sent by database server, because the server only answers those parts of queries that are not available in the cooperative cache. Since most cloud providers charge in a ``pay-per-use'' matter the amount of transferred data between the cloud environment and the outside world, using such a cooperative caching approach within cloud environmnents presents additional economical advantages. This paper studies possible use-cases of CoopSC within real-world cloud environment and outlines both the technical and economical gains.},
	booktitle = {Dependable {Networks} and {Services}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vancea, Andrei and Machado, Guilherme Sperb and D'Orazio, Laurent and Stiller, Burkhard},
	editor = {Sadre, Ramin and Novotný, Ji{\textbackslash}vrí and Čeleda, Pavel and Waldburger, Martin and Stiller, Burkhard},
	year = {2012},
	pages = {14--25},
}

@misc{GedeonProject,
	title = {Gedeon},
	url = {https://gforge.inria.fr/projects/gedeon/},
}

@article{Abouzeid2009,
	title = {{HadoopDB}: {An} {Architectural} {Hybrid} of {MapReduce} and {DBMS} {Technologies} for {Analytical} {Workloads}},
	volume = {2},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.14778/1687627.1687731},
	doi = {10.14778/1687627.1687731},
	number = {1},
	journal = {Proc. VLDB Endow.},
	author = {Abouzeid, Azza and Bajda-Pawlikowski, Kamil and Abadi, Daniel and Silberschatz, Avi and Rasin, Alexander},
	year = {2009},
	note = {Publisher: VLDB Endowment},
	pages = {922--933},
}

@article{Sjoberg2005,
	title = {A survey of controlled experiments in software engineering},
	volume = {31},
	issn = {00985589},
	doi = {10.1109/TSE.2005.97},
	abstract = {The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Sjøberg, Dag I.K. and Hannay, Jo E. and Hansen, Ove and Kampenes, Vigdis By and Karahasanović, Amela and Liborg, Nils Kristian and Rekdal, Anette C.},
	year = {2005},
	note = {ISBN: 0-7803-9507-7},
	keywords = {Empirical software engineering, Research methodology},
	pages = {733--753},
}

@misc{Gervasi2004,
	title = {Evaluating the structure of research papers: {A} case study},
	abstract = {This paper is triggered by a concern for the methodological soundness of research papers in RE. We propose a number of criteria for methodological soundness, and apply these to a random sample of 37 submissions to the RE'03 conference. From this application, we draw a number of conclusions that we claim are valid for a larger sample than just these 37 submissions. Our major observation is that most submissions in our sample are solution-oriented: they present a solution and illustrate it with a problem, rather than search for a solution to a given problem class; and most papers do not analyze why and when a solution works or does not work. We end with discussion of the need to improve the methodological soundness of research papers in RE.},
	author = {Gervasi, V and Wieringa, Roelf J and Heerkens, Johannes M G and Zowghi, D and Sim, S E},
	year = {2004},
	note = {Pages: 41-50},
	keywords = {EWI-10493, IR-64194, IS-Design science methodology, SCS-Services},
}

@article{TICHY1995,
	title = {Experimental evaluation in computer science: {A} quantitative study},
	volume = {28},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/016412129400111Y},
	doi = {https://doi.org/10.1016/0164-1212(94)00111-Y},
	number = {1},
	journal = {Journal of Systems and Software},
	author = {Tichy, Walter F and Lukowicz, Paul and Prechelt, Lutz and Heinz, Ernst A},
	year = {1995},
	pages = {9--18},
}

@inproceedings{Miller1968,
	address = {New York, NY, USA},
	title = {Response {Time} in {Man}-computer {Conversational} {Transactions}},
	url = {http://doi.acm.org/10.1145/1476589.1476628},
	doi = {10.1145/1476589.1476628},
	booktitle = {Proceedings of the {December} 9-11, 1968, {Fall} {Joint} {Computer} {Conference}, {Part} {I}},
	publisher = {ACM},
	author = {Miller, Robert B},
	year = {1968},
	note = {Series Title: AFIPS '68 (Fall, part I)},
	pages = {267--277},
}

@article{Lawrence2015,
	title = {Collecting the {Evidence}: {Improving} {Access} to {Grey} {Literature} and {Data} for {Public} {Policy} and {Practice}},
	volume = {46},
	url = {https://doi.org/10.1080/00048623.2015.1081712},
	doi = {10.1080/00048623.2015.1081712},
	number = {4},
	journal = {Australian Academic \& Research Libraries},
	author = {Lawrence, Amanda and Thomas, Julian and Houghton, John and Weldon, Paul},
	year = {2015},
	note = {Publisher: Routledge},
	pages = {229--249},
}

@article{Amendola2013,
	title = {Cosmology and fundamental physics with the {Euclid} satellite},
	volume = {16},
	issn = {14338351},
	doi = {10.12942/lrr-2013-6},
	abstract = {Euclid is a European Space Agency medium class mission selected for launch in 2020 within the Cosmic Vision 2015 2025 program. The main goal of Euclid is to understand the origin of the accelerated expansion of the universe. Euclid will explore the expansion history of the universe and the evolution of cosmic structures by measuring shapes and redshifts of galaxies as well as the distribution of clusters of galaxies over a large fraction of the sky. Although the main driver for Euclid is the nature of dark energy, Euclid science covers a vast range of topics, from cosmology to galaxy evolution to planetary research. In this review we focus on cosmology and fundamental physics, with a strong emphasis on science beyond the current standard models. We discuss five broad topics: dark energy and modified gravity, dark matter, initial conditions, basic assumptions and questions of methodology in the data analysis. This review has been planned and carried out within Euclid's Theory Working Group and is meant to provide a guide to the scientific themes that will underlie the activity of the group during the preparation of the Euclid mission.},
	journal = {Living Reviews in Relativity},
	author = {Amendola, Luca and Appleby, Stephen and Bacon, David and Baker, Tessa and Baldi, Marco and Bartolo, Nicola and Blanchard, Alain and Bonvin, Camille and Borgani, Stefano and Branchini, Enzo and Burrage, Clare and Camera, Stefano and Carbone, Carmelita and Casarini, Luciano and Cropper, Mark and de Rham, Claudia and Di Porto, Cinzia and Ealet, Anne and Ferreira, Pedro G. and Finelli, Fabio and García-Bellido, Juan and Giannantonio, Tommaso and Guzzo, Luigi and Heavens, Alan and Heisenberg, Lavinia and Heymans, Catherine and Hoekstra, Henk and Hollenstein, Lukas and Holmes, Rory and Horst, Ole and Jahnke, Knud and Kitching, Thomas D. and Koivisto, Tomi and Kunz, Martin and La Vacca, Giuseppe and March, Marisa and Majerotto, Elisabetta and Markovic, Katarina and Marsh, David and Marulli, Federico and Massey, Richard and Mellier, Yannick and Mota, David F. and Nunes, Nelson J. and Percival, Will and Pettorino, Valeria and Porciani, Cristiano and Quercellini, Claudia and Read, Justin and Rinaldi, Massimiliano and Sapone, Domenico and Scaramella, Roberto and Skordis, Constantinos and Simpson, Fergus and Taylor, Andy and Thomas, Shaun and Trotta, Roberto and Verde, Licia and Vernizzi, Filippo and Vollmer, Adrian and Wang, Yun and Weller, Jochen and Zlosnik, Tom},
	year = {2013},
	note = {arXiv: 1606.00180},
	keywords = {Cosmology, Galaxy evolution},
}

@article{Laureijs2011,
	title = {Euclid {Definition} {Study} {Report}},
	volume = {1110},
	issn = {15507998},
	url = {http://adsabs.harvard.edu/cgi-bin/nph-data_query?bibcode=2011arXiv1110.3193L&link_type=ABSTRACT%5Cnpapers2://publication/uuid/19A62241-2A3C-4227-AD45-A587BA6191B6},
	doi = {10.1088/0264-9381/18/14/306},
	abstract = {Euclid is a space-based survey mission from the European Space Agency designed to understand the origin of the Universe's accelerating expansion. It will use cosmological probes to investigate the nature of dark energy, dark matter and gravity by tracking their observational signatures on the geometry of the universe and on the cosmic history of structure formation. The mission is optimised for two independent primary cosmological probes: Weak gravitational Lensing (WL) and Baryonic Acoustic Oscillations (BAO). The Euclid payload consists of a 1.2 m Korsch telescope designed to provide a large field of view. It carries two instruments with a common field-of-view of {\textasciitilde}0.54 deg2: the visual imager (VIS) and the near infrared instrument (NISP) which contains a slitless spectrometer and a three bands photometer. The Euclid wide survey will cover 15,000 deg2 of the extragalactic sky and is complemented by two 20 deg2 deep fields. For WL, Euclid measures the shapes of 30-40 resolved galaxies per arcmin2 in one broad visible R+I+Z band (550-920 nm). The photometric redshifts for these galaxies reach a precision of dz/(1+z) {\textless} 0.05. They are derived from three additional Euclid NIR bands (Y, J, H in the range 0.92-2.0 micron), complemented by ground based photometry in visible bands derived from public data or through engaged collaborations. The BAO are determined from a spectroscopic survey with a redshift accuracy dz/(1+z) =0.001. The slitless spectrometer, with spectral resolution {\textasciitilde}250, predominantly detects Ha emission line galaxies. Euclid is a Medium Class mission of the ESA Cosmic Vision 2015-2025 programme, with a foreseen launch date in 2019. This report (also known as the Euclid Red Book) describes the outcome of the Phase A study.},
	journal = {arXiv.org},
	author = {Laureijs, R and Amiaux, J and Arduini, S and Auguères, J L and Brinchmann, J and Cole, R and Cropper, M and Dabin, C and Duvet, L and Ealet, A and Garilli, B and Gondoin, P and Guzzo, L and Hoar, J and Hoekstra, H and Holmes, R and Kitching, T and Maciaszek, T and Mellier, Y and Pasian, F and Percival, W and Rhodes, J and Saavedra Criado, G and Sauvage, M and Scaramella, R and Valenziano, L and Warren, S and Bender, R and Castander, F and Cimatti, A and Le Fèvre, O and Kurki-Suonio, H and Levi, M and Lilje, P and Meylan, G and Nichol, R and Pedersen, K and Popa, V and Rebolo Lopez, R and Rix, H-W and Rottgering, H and Zeilinger, W and Grupp, F and Hudelot, P and Massey, R and Meneghetti, M and Miller, L and Paltani, S and Paulin-Henriksson, S and Pires, S and Saxton, C and Schrabback, T and Seidel, G and Walsh, J and Aghanim, N and Amendola, L and Bartlett, J and Baccigalupi, C and Beaulieu, J P and Benabed, K and Cuby, J-G and Elbaz, D and Fosalba, P and Gavazzi, G and Helmi, A and Hook, I and Irwin, M and Kneib, J P and Kunz, M and Mannucci, F and Moscardini, L and Tao, C and Teyssier, R and Weller, J and Zamorani, G and Zapatero Osorio, M R and Boulade, O and Foumond, J J and Di Giorgio, A and Guttridge, P and James, A and Kemp, M and Martignac, J and Spencer, A and Walton, D and Blümchen, T and Bonoli, C and Bortoletto, F and Cerna, C and Corcione, L and Fabron, C and Jahnke, K and Ligori, S and Madrid, F and Martin, L and Morgante, G and Pamplona, T and Prieto, E and Riva, M and Toledo, R and Trifoglio, M and Zerbi, F and Abdalla, F and Douspis, M and Grenet, C and Borgani, S and Bouwens, R and Courbin, F and Delouis, J M and Dubath, P and Fontana, A and Frailis, M and Grazian, A and Koppenhöfer, J and Mansutti, O and Melchior, M and Mignoli, M and Mohr, J and Neissner, C and Noddle, K and Poncet, M and Scodeggio, M and Serrano, S and Shane, N and Starck, J-L and Surace, C and Taylor, A and Verdoes-Kleijn, G and Vuerli, C and Williams, O R and Zacchei, A and Altieri, B and Escudero Sanz, I and Kohley, R and Oosterbroek, T and Astier, P and Bacon, D and Bardelli, S and Baugh, C and Bellagamba, F and Benoist, C and Bianchi, D and Biviano, A and Branchini, E and Carbone, C and Cardone, V and Clements, D and Colombi, S and Conselice, C and Cresci, G and Deacon, N and Dunlop, J and Fedeli, C and Fontanot, F and Franzetti, P and Giocoli, C and Garcia-Bellido, J and Gow, J and Heavens, A and Hewett, P and Heymans, C and Holland, A and Huang, Z and Ilbert, O and Joachimi, B and Jennins, E and Kerins, E and Kiessling, A and Kirk, D and Kotak, R and Krause, O and Lahav, O and van Leeuwen, F and Lesgourgues, J and Lombardi, M and Magliocchetti, M and Maguire, K and Majerotto, E and Maoli, R and Marulli, F and Maurogordato, S and McCracken, H and McLure, R and Melchiorri, A and Merson, A and Moresco, M and Nonino, M and Norberg, P and Peacock, J and Pellò, R and Penny, M and Pettorino, V and Di Porto, C and Pozzetti, L and Quercellini, C and Radovich, M and Rassat, A and Roche, N and Ronayette, S and Rossetti, E and Sartoris, B and Schneider, P and Semboloni, E and Serjeant, S and Simpson, F and Skordis, C and Smadja, G and Smartt, S and Spano, P and Spiro, S and Sullivan, M and Tilquin, A and Trotta, R and Verde, L and Wang, Y and Williger, G and Zhao, G and Zoubian, J and Zucca, E},
	year = {2011},
	pmid = {21647702},
	note = {arXiv: 1110.3193
ISBN: 1545-4134},
	pages = {3193},
}

@inproceedings{Dubath2016,
	title = {The {Euclid} {Data} {Processing} {Challenges}},
	volume = {12},
	doi = {10.1017/S1743921317001521},
	abstract = {Euclid is a Europe-led cosmology space mission dedicated to a visible and near infrared survey of the entire extra-galactic sky. Its purpose is to deepen our knowledge of the dark content of our Universe. After an overview of the Euclid mission and science, this contribution describes how the community is getting organized to face the data analysis challenges, both in software development and in operational data processing matters. It ends with a more specific account of some of the main contributions of the Swiss Science Data Center (SDC-CH).},
	booktitle = {Proceedings of the {International} {Astronomical} {Union}},
	author = {Dubath, Pierre and Apostolakos, Nikolaos and Bonchi, Andrea and Belikov, Andrey and Brescia, Massimo and Cavuoti, Stefano and Capak, Peter and Coupon, Jean and Dabin, Christophe and Degaudenzi, Hubert and Desai, Shantanu and Dubath, Florian and Fontana, Adriano and Fotopoulou, Sotiria and Frailis, Marco and Galametz, Audrey and Hoar, John and Holliman, Mark and Hoyle, Ben and Hudelot, Patrick and Ilbert, Olivier and Kuemmel, Martin and Melchior, Martin and Mellier, Yannick and Mohr, Joe and Morisset, Nicolas and Paltani, Stéphane and Pello, Roser and Pilo, Stefano and Polenta, Gianluca and Poncet, Maurice and Saglia, Roberto and Salvato, Mara and Sauvage, Marc and Schefer, Marc and Serrano, Santiago and Soldati, Marco and Tramacere, Andrea and Williams, Rees and Zacchei, Andrea},
	year = {2016},
	note = {arXiv: 1701.08158
Issue: S325
ISSN: 17439221},
	keywords = {galaxies: distances and redshifts, galaxies: fundamental parameters, methods: data analysis},
	pages = {73--82},
}

@article{Ruiz-Rube2013,
	title = {Uses and applications of {Software} \& {Systems} {Process} {Engineering} {Meta}-{Model} process models. {A} systematic mapping study},
	volume = {25},
	issn = {2047-7481},
	url = {http://dx.doi.org/10.1002/smr.1594},
	doi = {10.1002/smr.1594},
	abstract = {Software process engineering is a discipline, which aims to study and improve software development and maintenance processes. The explicit definition of software processes is essential. To this end, the Object Management Group consortium proposed the Software \& Systems Process Engineering Meta-Model (SPEM) that exploits the benefits of the Model Driven Architecture paradigm applied to software process models, instead of software specification models. The aim of this study is to discover evidence clusters and evidence deserts in the use and application of SPEM from a business process management point of view. To reach the proposed objective, we have undertaken a systematic mapping study of the existing scientific literature.The reviewed literature deals mainly with process modeling and, to a lesser extent, with process adaptability, verification, and validation, enactment and evaluation. Wide agreement exists in using the SPEM meta-model to develop different types of methods and processes. Further research efforts are needed in areas related to enactment and evaluation of software processes. There is a need to evolve to a new version of the meta-model that incorporates the improvements proposed by different authors. Copyright © 2013 John Wiley \& Sons, Ltd.},
	number = {9},
	journal = {Journal of Software: Evolution and Process},
	author = {Ruiz-Rube, Iván and Dodero, Juan Manuel and Palomo-Duarte, Manuel and Ruiz, Mercedes and Gawn, David},
	year = {2013},
	pmid = {67195556},
	note = {arXiv: 1408.1293
ISBN: 9781450330565},
	keywords = {SPEM, model-driven engineering, software process engineering},
	pages = {999--1025},
}

@inproceedings{Kiryanov2015a,
	title = {{FTS3} / {WebFTS} - {A} {Powerful} {File} {Transfer} {Service} for {Scientific} {Communities}},
	volume = {66},
	doi = {10.1016/j.procs.2015.11.076},
	abstract = {FTS3, the service responsible for globally distributing the majority of the LHC data across the WLCG infrastructure, is now available to everybody. Already integrated into LHC experiment frameworks, a new web interface now makes the FTS3's transfer technology directly available to end users. In this article we describe this intuitive new interface, "WebFTS", which allows users to easily schedule and manage large data transfers right from the browser, profiting from a service which has been proven at the scale of petabytes per month. We will shed light on new development activities to extend FTS3 transfers capabilities outside Grid boundaries with support of non-Grid endpoints like Dropbox and S3. We also describe the latest changes integrated into the transfer engine itself, such as new data management operations like deletions and staging files from archive, all of which may be accessed through our standards-compliant REST API. For the Service Managers, we explain such features as the service's horizontal scalability, advanced monitoring and its "zero configuration" approach to deployment made possible by specialised transfer optimisation logic. For the Data Managers, we will present new tools for management of FTS3 transfer parameters like limits for bandwidth and max active file transfers per endpoint and VO, user and endpoint banning and powerful command line tools. We finish by describing our effort to extend WebFTS's captivating graphical interface with support of Federated Identity technologies, thus demonstrating the use of grid resources without the burden of certificate management. In this manner we show how FTS3 can cover the needs of wide range of parties from casual users to high-load services. The evolution of FTS3 is addressing technical and performance requirements and challenges for LHC Run 2, moreover, its simplicity, generic design, web portal and REST interface makes it an ideal file transfer scheduler both inside and outside of HEP community.},
	booktitle = {Procedia {Computer} {Science}},
	author = {Kiryanov, A. and Ayllon, A.A. and Keeble, O.},
	year = {2015},
	note = {ISSN: 18770509},
	keywords = {Data storage, FTS3, Federated identity, File transfer, HPC},
}

@article{Lassnig2014,
	title = {The {DMLite} {Rucio} plugin: {ATLAS} data in a filesystem},
	volume = {513},
	issn = {17426596},
	doi = {10.1088/1742-6596/513/4/042030},
	abstract = {Rucio is the next-generation data management system of the ATLAS experiment. Historically, clients interacted with the data management system via specialised tools, but in Rucio additional methods are provided. To support filesystem-like interaction with all ATLAS data, a plugin to the DMLite software stack has been developed. It is possible to mount Rucio as a filesystem, and execute regular filesystem operations in a POSIX fashion. This is exposed via various protocols, for example, WebDAV or NFS, which then removes any dependency on Rucio for client software. The main challenge for this work is the mapping of the set-like ATLAS namespace into a hierarchical filesystem, whilst preserving the high performance features of the former. This includes listing and searching for data, creation of files, datasets and containers, and the aggregation of existing data-all within directories with potentially millions of entries. This contribution details the design and implementation of the plugin. Furthermore, an evaluation of the performance characteristics is given, to show that this approach can scale to the requirements of ATLAS physics analysis. © Published under licence by IOP Publishing Ltd.},
	number = {TRACK 4},
	journal = {Journal of Physics: Conference Series},
	author = {Lassnig, M. and Van Dongen, D. and Da Rocha, R.B. and Ayllon, A.A. and Calfayan, P.},
	year = {2014},
}

@inproceedings{Furano2013,
	title = {The dynamic federations: {Federate} storage on the fly using {HTTP}/{WebDAV} and {DMLite}},
	abstract = {Recently the importance of clustering storage nodes across site boundaries is becoming more clear, thanks also to the recent ongoing initiatives in the CMS and ATLAS experiments. These approaches are supposed to promote simplicity in accessing the data and offering new possibilities for resilience and data placement strategies, that may also lead to a better utilization of the available CPU slots. Here we report on work that seeks to exploit the federation potential of redirectable protocols like HTTP/WebDAV and build a dynamic, scalable, persistency-free system that offers a unique view of the storage and metadata ensemble and the possibility of integration of other compatible resources such as those from cloud providers. The challenge, here undertaken by the providers of dCache and DPM, partially in the context of EMI-data, and pragmatically open to any other Grid and Cloud storage solutions, is to build such a system while being able to accommodate name translations from existing catalogues (e.g. LFCs), experiment- based metadata catalogues, or stateless algorithmic name translations, also known as "trivial file catalogues". Other technical challenges that will determine the success of this initiative include performance, latency and scalability, and the ability to create worldwide storage federations that are able to redirect clients to repositories that they can efficiently access, for instance trying to choose the endpoints that are closer or applying other criteria. One of the key requirements is to use standard clients (provided by OS'es or open source distributions, e.g. Web browsers) to access an already aggregated system. This was accomplished by exploiting the possibilities offered by the DMLite framework, in order to integrate with existing frontends like the Apache server. We believe that the features of a loosely coupled federation of open-protocols-based storage elements will open many possibilities of evolving the current computing models without disrupting them, and, at the same time, can operate with the existing infrastructures, follow their evolution path and add storage centers that can be acquired as a third-party service.},
	booktitle = {Proceedings of {Science}},
	author = {Furano, F. and Da Rocha, R. and Devresse, A. and Keeble, O. and Ayllón, A. and Fuhrmann, P.},
	year = {2013},
	note = {ISSN: 18248039},
}

@article{Ayllon2012,
	title = {Web enabled data management with {DPM} \&amp; {LFC}},
	volume = {396},
	issn = {17426588},
	doi = {10.1088/1742-6596/396/5/052006},
	abstract = {The Disk Pool Manager (DPM) and LCG File Catalog (LFC) are two grid data management components currently used in production with more than 240 endpoints. Together with a set of grid client tools they give the users a unified view of their data, hiding most details concerning data location and access. Recently we've put a lot of effort in developing a reliable and high performance HTTP/WebDAV frontend to both our grid catalog and storage components, exposing the existing functionality to users accessing the services via standard clients - e.g. web browsers, curl - present in all operating systems, giving users a simple and straight-forward way of interaction. In addition, as other relevant grid storage components (like dCache) expose their data using the same protocol, for the first time we had the opportunity of attempting a unified view of all grid storage using HTTP. We describe the HTTP redirection mechanism used to integrate the grid catalog(s) with the multiple storage components, including details on some assumptions made to allow integration with other implementations. We describe the way we hide the details regarding site availability or catalog inconsistencies by switching the standard HTTP client automatically between multiple replicas. We also present measurements of access performance, and the relevant factors regarding replica selection - current throughput and load, geographic proximity, etc. Finally, we report on some additional work done to have this system as a viable alternative to GridFTP, providing multi-stream transfers and exploiting some additional features of WebDAV to enable third party copies - essential for managing data movements between storage systems - with equivalent performance..},
	number = {PART 5},
	journal = {Journal of Physics: Conference Series},
	author = {Ayllon, A.A. and Beche, A. and Furano, F. and Hellmich, M. and Keeble, O. and Da Rocha, R.B.},
	year = {2012},
}

@article{Riahi2015,
	title = {{FTS3}: {Quantitative} monitoring},
	volume = {664},
	issn = {17426596},
	doi = {10.1088/1742-6596/664/6/062051},
	abstract = {The overall success of LHC data processing depends heavily on stable, reliable and fast data distribution. The Worldwide LHC Computing Grid (WLCG) relies on the File Transfer Service (FTS) as the data movement middleware for moving sets of files from one site to another. This paper describes the components of FTS3 monitoring infrastructure and how they are built to satisfy the common and particular requirements of the LHC experiments. We show how the system provides a complete and detailed cross-virtual organization (VO) picture of transfers for sites, operators and VOs. This information has proven critical due to the shared nature of the infrastructure, allowing a complete view of all transfers on shared network links between various workflows and VOs using the same FTS transfer manager. We also report on the performance of the FTS service itself, using data generated by the aforementioned monitoring infrastructure both during the commissioning and the first phase of production. We also explain how this monitoring information and network metrics produced can be used both as a starting point for troubleshooting data transfer issues, but also as a mechanism to collect information such as transfer efficiency between sites, achieved throughput and its evolution over time, most common errors, etc, and take decision upon them to further optimize transfer workflows. The service setup is subject to sites policies to control the network resource usage, as well as all the VOs making use of the Grid resources at the site to satisfy their requirements. FTS3 is the new version of FTS and has been deployed in production in August 2014.},
	number = {6},
	journal = {Journal of Physics: Conference Series},
	author = {Riahi, H. and Salichos, M. and Keeble, O. and Andreeva, J. and Ayllon, A.A. and Di Girolamo, A. and Magini, N. and Roiser, S. and Simon, M.K.},
	year = {2015},
}

@inproceedings{Kiryanov2015,
	title = {{FTS3} -{A} file transfer service for {Grids}, {HPCs} and {Clouds}},
	volume = {15-20-Marc},
	abstract = {FTS3, the service responsible for globally distributing the majority of the LHC data across the WLCG infrastructure, is now available to everybody. Already integrated into LHC experiment frameworks, a new Web interface now makes the FTS3's transfer technology directly available to end users. In this contribution we describe this intuitive new interface, "WebFTS", which allows users to easily schedule and manage large data transfers right from the browser, profiting from a service which has been proven at the scale of petabytes per month. We will shed light on new development activities to extend FTS3 transfers capabilities outside Grid boundaries with support of non-Grid endpoints like Dropbox and S3. We also describe the latest changes integrated into the transfer engine itself, such as new data management operations like deletions and staging files from archive, all of which may be accessed through our standardscompliant REST API. For the Service Manager, we explain such features as the service's horizontal scalability, advanced monitoring and its "zero configuration" approach to deployment made possible by specialised transfer optimisation logic. For the Data Manager, we will present new tools for management of FTS3 transfer parameters like limits for bandwidth and max active file transfers per endpoint and VO, user and endpoint banning and powerful command line tools. We finish by describing the impact of extending WebFTS's captivating graphical interface with support of Federated Identity technologies, thus demonstrating the use of Grid resources without the burden of X.509 certificate management. In this manner we show how FTS3 can cover the needs of wide range of parties from casual users to high-load services.},
	booktitle = {Proceedings of {Science}},
	author = {Kiryanov, A. and Ayllon, A.A. and Salichos, M. and Keeble, O.},
	year = {2015},
	note = {ISSN: 18248039},
}

@misc{Evans2011,
	title = {The {Internet} of {Things}},
	publisher = {Cisco Internet Business Solutions Group (IBSG)},
	author = {Evans, Dave},
	year = {2011},
	note = {Pages: 11},
}

@article{Reinsel2017,
	title = {Data {Age} 2025: {The} {Evolution} of {Data} to {Life}-{Critical}},
	abstract = {IDC White Paper © 2017 IDC. www.idc.com {\textbar} Page 2 Data Age 2025: The Evolution of Data to Life-Critical Don't Focus on Big Data; Focus on the Data That's Big Sponsored by Seagate Data Age 2025: The Evolution of Data to Life-Critical Don't Focus on Big Data; Focus on the Data That's Big EXECUTIVE SUMMARY We are fast approaching a new era of the Data Age. From autonomous cars to humanoid robots and from intelligent personal assistants to smart home devices, the world around us is undergoing a fundamental change, transforming the way we live, work, and play. Imagine being awoken and tended to by a virtual personal assistant that advises you on what clothing from your wardrobe is best suited to the weather report and your schedule for the day or being transported by your self-driving car. Or perhaps you won't need to commute to an office at all as technology will allow you to conjure workspaces out of thin air using interactive surfaces, and holographic teleconferencing becomes the norm for communicating virtually with colleagues. Weekends may involve browsing new furniture through an augmented reality app and seeing how a sofa looks in your living room before placing an order. As you relax on the new sofa, Saturday night's takeout will be a pizza made by a robot and delivered in record time by a drone. Data has become critical to all aspects of human life over the course of the past 30 years; it's changed how we're educated and entertained, and it informs the way we experience people, business, and the wider world around us. It is the lifeblood of our rapidly growing digital existence. This digital existence, as defined by the sum of all data created, captured, and replicated on our planet in any given year is growing rapidly, and we call it the " global datasphere " . In just the past 10 years society has witnessed the transition of analog to digital. What the next decade will bring using the power of data is virtually limitless. While we as consumers will enjoy the benefits of a digital existence, enterprises around the globe will be embracing new and unique business opportunities, powered by this wealth of data and the insight it provides. Extracting and delivering simplicity and convenience from the complexity of many billions of bytes – be it through},
	number = {April},
	journal = {IDC White Paper; Sponsored by Seagate},
	author = {Reinsel, David and Gantz, John and Rydning, John},
	year = {2017},
	note = {Publisher: Seagate},
	pages = {1--25},
}

@techreport{Bird2016,
	title = {{WLCG}: {Report} on {Project} {Status}, {Resources} and {Financial} {Plan}},
	abstract = {This status report covers the period from April 2016 – October 2016. Further details on progress, planning and resources, including accounting and reliability data, and detailed quarterly progress reports, can be found in the documents linked to the Progress Reports section on the WLCG web site},
	institution = {CERN},
	author = {Bird, Ian},
	year = {2016},
	pages = {24},
}

@inproceedings{Krzewicki2014,
	address = {Aix-Les Bains, France},
	title = {{HL}-{LHC} {Computing}},
	booktitle = {{ECFA} {High} {Luminosity} {LHC} {Experiments} {Workshop}},
	author = {Krzewicki, Mikołaj},
	year = {2014},
}

@inproceedings{Bolton2016,
	title = {{SKA} background},
	url = {https://indico.cern.ch/event/570594/contributions/2308016/},
	booktitle = {{SKA}/{GridPP}},
	author = {Bolton, Rosie and Scaife, Anna},
	year = {2016},
}

@inproceedings{Petersen2009,
	title = {Context in industrial software engineering research},
	isbn = {978-1-4244-4841-8},
	doi = {10.1109/ESEM.2009.5316010},
	abstract = {In order to draw valid conclusions when aggregating evidence it is important to describe the context in which industrial studies were conducted. This paper structures the context for empirical industrial studies and provides a checklist. The aim is to aid researchers in making informed decisions concerning which parts of the context to include in the descriptions. Furthermore, descriptions of industrial studies were surveyed.},
	booktitle = {2009 3rd {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}, {ESEM} 2009},
	author = {Petersen, Kai and Wohlin, Claes},
	year = {2009},
	note = {arXiv: 1101.4101v1
ISSN: 1938-6451},
	keywords = {★},
	pages = {401--404},
}

@article{Oakley2003,
	title = {Research {Evidence}, {Knowledge} {Management} and {Educational} {Practice}: early lessons from a systematic approach},
	volume = {1},
	issn = {1474-8460},
	url = {http://www.ingentaconnect.com/contentone/ioep/clre/2003/00000001/00000001/art00004},
	doi = {10.1080/14748460306693},
	abstract = {'Evidence-informed' education is part of a general move internationally to found policy and practice on a more reliable scientific base. This paper describes a five-year initiative in this area funded by the English Department for Education and Skills in 2000. The paper describes the background to the initiative, the infrastructure developed within it for supporting research synthesis, and some of the challenges experienced by the first review groups.},
	number = {1},
	journal = {London Review of Education},
	author = {Oakley, Ann},
	year = {2003},
	note = {ISBN: 1474-8460},
	keywords = {Déterminant, Praticiens, Savoirs, Social, Stratégie},
	pages = {21--33},
}

@inproceedings{Jalali2012,
	title = {Systematic {Literature} {Studies}: {Database} {Searches} vs. {Backward} {Snowballing}},
	isbn = {978-1-4503-1056-7},
	url = {http://dl.acm.org/citation.cfm?id=2372257},
	doi = {10.1145/2372251.2372257},
	abstract = {Systematic studies of the literature can be done in different ways. In particular, different guidelines propose different first steps in their recommendations, e.g. start with search strings in different databases or start with the reference lists of a starting set of papers. In software engineering, the main recommended first step is using search strings in a number of databases, while in information systems, snowballing has been recommended as the first step. This paper compares the two different search approaches for conducting literature review studies. The comparison is conducted by searching for articles addressing "Agile practices in global software engineering". The focus of the paper is on evaluating the two different search approaches. Despite the differences in the included papers, the conclusions and the patterns found in both studies are quite similar. The strengths and weaknesses of each first step are discussed separately and in comparison with each other. It is concluded that none of the first steps is outperforming the other, and the choice of guideline to follow, and hence the first step, may be context-specific, i.e. depending on the area of study.},
	booktitle = {{ESEM}’12: {Proceedings} of the {ACM}-{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	author = {Jalali, Samireh and Wohlin, Claes},
	year = {2012},
	note = {ISSN: 1938-6451},
	keywords = {Systematic Reviews, database management systems, global software engineering, information retrieval, snowballing, software prototyping},
	pages = {29--38},
}

@incollection{Victor2016,
	address = {Cham},
	title = {{NOSQL} {Design} for {Analytical} {Workloads}: {Variability} {Matters}},
	isbn = {978-3-319-46396-4},
	url = {http://link.springer.com/10.1007/978-3-319-46397-1_4},
	abstract = {Big Data has recently gained popularity and has strongly questioned relational databases as universal storage systems, especially in the presence of analytical workloads. As result, co-relational alternatives, commonly known as NOSQL (Not Only SQL) databases, are extensively used for Big Data. As the primary focus of NOSQL is on performance, NOSQL databases are directly designed at the physical level, and consequently the resulting schema is tailored to the dataset and access patterns of the problem in hand. However, we believe that NOSQL design can also benefit from traditional design approaches. In this paper we present a method to design databases for analytical workloads. Starting from the conceptual model and adopting the classical 3-phase design used for relational databases, we propose a novel design method considering the new features brought by NOSQL and encompassing relational and co-relational design altogether.},
	booktitle = {Conceptual {Modeling}},
	publisher = {Springer International Publishing},
	author = {Herrero, Victor and Abelló, Alberto and Romero, Oscar},
	editor = {Comyn-Wattiau, Isabelle and Tanaka, Katsumi and Song, Il-Yeol and Yamamoto, Shuichiro and Saeki, Motoshi},
	year = {2016},
	doi = {10.1007/978-3-319-46397-1_4},
	note = {ISSN: 16113349},
	pages = {50--64},
}

@inproceedings{Yasir2012a,
	title = {Enhanced {Query}-by-{Object} approach for information requirement elicitation in large databases},
	volume = {7678 LNCS},
	isbn = {978-3-642-35541-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-35542-4_4},
	doi = {10.1007/978-3-642-35542-4_4},
	abstract = {Information Requirement Elicitation (IRE) recommends a framework for developing interactive interfaces, which allow users to access database systems without having prior knowledge of a query language. An approach called ‘Query-by-Object’ (QBO) has been proposed in the literature for IRE by exploiting simple calculator like operations. However, the QBO approach was proposed by assuming that the underlying database is simple and contains few tables of small size. In this paper, we propose an enhanced QBO approach called Query-by-Topics (QBT), for designing calculator like user interfaces for large databases. We use methodologies for clustering database entities and discovering topical structures to represent objects at a higher level of abstraction. The QBO approach is then enhanced to allow users to query by topics (QBT). We developed a prototype system based on QBT and conducted experimental studies to show effectiveness of the proposed approach.},
	booktitle = {Big {Data} {Analytics}},
	author = {Yasir, Ammar and Swamy, Mittapally Kumara and Reddy, Polepalli Krishna and Bhalla, Subhash},
	year = {2012},
	note = {ISSN: 03029743},
	keywords = {Information requirement elicitation, Information systems, Query-by-Object, User interfaces, UserStudy:yes, cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {26--41},
}

@inproceedings{Chenghao2017,
	title = {An {Adaptive} {Data} {Partitioning} {Scheme} for {Accelerating} {Exploratory} {Spark} {SQL} {Queries}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-55753-3_8},
	doi = {10.1007/978-3-319-55753-3_8},
	abstract = {For data analysis, it’s useful to explore the data set with a sequence of queries, frequently using the results from the previous queries to shape the next queries. Thus, data used in the previous queries are often reused, at least in part, in the next queries. This fact may be used to accelerate queries with data partitioning, a widely used technique that enables skipping the irrelevant data for better I/O performance. For getting effective partitions which are likely to cover the query workload in the future, we propose an adaptive partitioning scheme, combining the data-driven metrics and user-driven metrics to guide the data partitioning as well as a heuristic model using the metric plugin system to support different exploratory patterns. For partition storage and management, we propose an effective partition index structure for quickly searching for appropriate partitions to answer queries. The system is quite helpful in improving the performance of exploratory queries.},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	author = {Guo, Chenghao and Wu, Zhigang and He, Zhenying and Wang, X. Sean},
	year = {2017},
	keywords = {UserStudy:yes, cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
}

@inproceedings{Kantere2016,
	address = {Cham},
	title = {Query {Similarity} for {Approximate} {Query} {Answering}},
	volume = {9828},
	isbn = {978-3-319-44405-5},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-44406-2_29},
	doi = {10.1007/978-3-319-44406-2_29},
	abstract = {Query rewriting in heterogeneous environments assumes mappings that are complete. In reality and especially in the Big Data era it is rarely the case that such complete sets of mappings exist between sources, and the presence of partial mappings is the norm rather than the exception. So, practically, existing rewriting algorithms fail in the majority of cases. The solution is to approximate original queries with others that can be answered by existing mappings. Approximate queries bear some similarity to original ones in terms of structure and semantics. In this paper we investigate the notion of such query similarity and we introduce the use of query similarity functions to this end. We also present a methodology for the construction of such functions. We employ exemplary similarity functions created with the proposed methodology into recent algorithms for approximate query answering and show experimental results for the influence of the similarity function to the efficiency of the algorithms.},
	booktitle = {Database and {Expert} {Systems} {Applications}},
	publisher = {Springer International Publishing},
	author = {Kantere, Verena},
	editor = {Hartmann, Sven and Ma, Hui},
	year = {2016},
	note = {ISSN: 16113349},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution, type:Validation Research},
	pages = {355--367},
}

@inproceedings{Chen:2016:SSA:2882903.2882908,
	address = {New York, NY, USA},
	title = {A {Study} of {Sorting} {Algorithms} on {Approximate} {Memory}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882908},
	doi = {10.1145/2882903.2882908},
	abstract = {Hardware evolution has been one of the driving factors for the redesign of database systems. Recently, approximate storage emerges in the area of computer architecture. It trades off precision for better performance and/or energy consumption. Previous studies have demonstrated the benefits of approximate storage for applications that are tolerant to imprecision such as image processing. However, it is still an open question whether and how approximate storage can be used for applications that do not expose such intrinsic tolerance. In this paper, we study one of the most basic operations in database–sorting on a hybrid storage system with both precise storage and approximate storage. Particularly, we start with a study of three common sorting algorithms on approximate storage. Experimental results show that a 95\% sorted sequence can be obtained with up to 40\% reduction in total write latencies. Thus, we propose an approx-refine execution mechanism to improve the performance of sorting algorithms on the hybrid storage system to produce precise results. Our optimization gains the performance benefits by offloading the sorting operation to approximate storage, followed by an efficient refinement to resolve the unsortedness on the output of the approximate storage. Our experiments show that our approx-refine can reduce the total memory access time by up to 11\%. These studies shed light on the potential of approximate hardware for improving the performance of applications that require precise results.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data} - {SIGMOD} '16},
	publisher = {ACM},
	author = {Chen, Shuang and Jiang, Shunning and He, Bingsheng and Tang, Xueyan},
	year = {2016},
	note = {Series Title: SIGMOD '16
ISSN: 07308078},
	keywords = {database, hybrid storage, phase change memory},
	pages = {647--662},
}

@article{Park:2015:NH:2850583.2850589,
	title = {Neighbor-sensitive hashing},
	volume = {9},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2850583.2850589},
	doi = {10.14778/2850583.2850589},
	abstract = {Approximate kNN (k-nearest neighbor) techniques using binary hash functions are among the most commonly used approaches for overcoming the prohibitive cost of performing exact kNN queries. However, the success of these techniques largely depends on their hash functions' ability to distinguish kNN items; that is, the kNN items retrieved based on data items' hashcodes, should include as many true kNN items as possible. A widely-adopted principle for this process is to ensure that similar items are assigned to the same hashcode so that the items with the hashcodes similar to a query's hashcode are likely to be true neighbors. In this work, we abandon this heavily-utilized principle and pursue the opposite direction for generating more effective hash functions for kNN tasks. That is, we aim to increase the distance between similar items in the hashcode space, instead of reducing it. Our contribution begins by providing theoretical analysis on why this revolutionary and seemingly counter-intuitive approach leads to a more accurate identification of kNN items. Our analysis is followed by a proposal for a hashing algorithm that embeds this novel principle. Our empirical studies confirm that a hashing algorithm based on this counter-intuitive idea significantly improves the efficiency and accuracy of state-of-the-art techniques.},
	number = {3},
	journal = {Proceedings of the VLDB Endowment},
	author = {Park, Yongjoo and Cafarella, Michael and Mozafari, Barzan},
	month = nov,
	year = {2015},
	note = {Publisher: VLDB Endowment},
	pages = {144--155},
}

@inproceedings{Qin:2015:SAT:2799562.2799563,
	address = {New York, NY, USA},
	title = {Speculative {Approximations} for {Terascale} {Distributed} {Gradient} {Descent} {Optimization}},
	isbn = {978-1-4503-3724-3},
	url = {http://doi.acm.org/10.1145/2799562.2799563},
	doi = {10.1145/2799562.2799563},
	abstract = {Model calibration is a major challenge faced by the plethora of statistical analytics packages that are increasingly used in Big Data applications. Identifying the optimal model parameters is a time-consuming process that has to be executed from scratch for every dataset/model combination even by experienced data scientists. We argue that the incapacity to evaluate multiple parameter configurations simultaneously and the lack of support to quickly identify sub-optimal configurations are the principal causes. In this paper, we develop two database-inspired techniques for efficient model calibration. Speculative parameter testing applies advanced parallel multi-query processing methods to evaluate several configurations concurrently. Online aggregation is applied to identify sub-optimal configurations early in the processing by incrementally sampling the training dataset and estimating the objective function corresponding to each configuration. We design concurrent online aggregation estimators and define halting conditions to accurately and timely stop the execution. We apply the proposed techniques to distributed gradient descent optimization -- batch and incremental -- for support vector machines and logistic regression models. We implement the resulting solutions in GLADE PF-OLA -- a state-of-the-art Big Data analytics system -- and evaluate their performance over terascalesize synthetic and real datasets. The results confirm that as many as 32 configurations can be evaluated concurrently almost as fast as one, while sub-optimal configurations are detected accurately in as little as a 1/20th fraction of the time.},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Data} {Analytics} in the {Cloud}},
	publisher = {ACM},
	author = {Qin, Chengjie and Rusu, Florin},
	year = {2015},
	note = {Series Title: DanaC'15},
	pages = {1:1--1:10},
}

@inproceedings{Zeng2014,
	address = {New York, NY, USA},
	title = {The {Analytical} {Bootstrap}: {A} {New} {Method} for {Fast} {Error} {Estimation} in {Approximate} {Query} {Processing}},
	isbn = {978-1-4503-2376-5},
	url = {http://doi.acm.org/10.1145/2588555.2588579},
	doi = {10.1145/2588555.2588579},
	abstract = {Sampling is one of the most commonly used techniques in Approximate Query Processing (AQP)-an area of research that is now made more critical by the need for timely and cost-effective analytics over "Big Data". Assessing the quality (i.e., estimating the error) of approximate answers is essential for meaningful AQP, and the two main approaches used in the past to address this problem are based on either (i) analytic error quantification or (ii) the bootstrap method. The first approach is extremely efficient but lacks generality, whereas the second is quite general but suffers from its high computational overhead. In this paper, we introduce a probabilistic relational model for the bootstrap process, along with rigorous semantics and a unified error model, which bridges the gap between these two traditional approaches. Based on our probabilistic framework, we develop efficient algorithms to predict the distribution of the approximation results. These enable the computation of any bootstrap-based quality measure for a large class of SQL queries via a single-round evaluation of a slightly modified query. Extensive experiments on both synthetic and real-world datasets show that our method has superior prediction accuracy for bootstrap-based quality measures, and is several orders of magnitude faster than bootstrap.},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Zeng, Kai and Gao, Shi and Mozafari, Barzan and Zaniolo, Carlo},
	year = {2014},
	note = {Series Title: SIGMOD '14},
	keywords = {cluster:Query Approximation, error estimation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution, ★},
	pages = {277--288},
}

@inproceedings{Kim2015,
	address = {New York, NY, USA},
	title = {Flying {KIWI}: {Design} of {Approximate} {Query} {Processing} {Engine} for {Interactive} {Data} {Analytics} at {Scale}},
	isbn = {978-1-4503-3846-2},
	url = {http://doi.acm.org/10.1145/2837060.2837096},
	doi = {10.1145/2837060.2837096},
	abstract = {This paper introduces the design of hybrid SQL-on-Hadoop system, which supports dual-mode (interactive and deep) analytics. We present an architecture of approximate query processing engine using horizontal and vertical sampling of the original database for interactive big data analytics. A key novelty of our approach is that we can support interactive analytics as well as deep analytics at scale in the unified manner.},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {Big} {Data} {Applications} and {Services}},
	publisher = {ACM},
	author = {Kim, Sung-Soo and Lee, Taewhi and Chung, Moonyoung and Won, Jongho},
	year = {2015},
	note = {Series Title: BigDAS '15},
	keywords = {Big Data, RDistributed: Yes, RInteractive: Yes, Random Sampling, SQL-on-Hadoop, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {206--207},
}

@article{7904748,
	title = {Predictive {Intelligence} in {Analytics} {Aggregation} of {Partial} {Ordered} {Subsets}},
	volume = {PP},
	issn = {2168-2216},
	url = {http://ieeexplore.ieee.org/document/7904748/},
	doi = {10.1109/TSMC.2017.2690364},
	abstract = {Nowadays, the increased amount of users' devices produce huge volumes of data that should be efficiently managed by modern applications. Streams are adopted to deliver data that, usually, are stored into a number of partitions. Splitting the data offers a lot of advantages as applications can process them in parallel, thus, they increase the speed of processing. Progressive analytics are also adopted to deliver partial responses, during processing, thus, saving time in the execution of applications. Data exploration and analytics queries are very significant for future applications. Usually, such queries demand for an ordered set of objects as a response and require intelligent predictive schemes to deliver the responses on top of the partial results retrieved by the distributed data partitions. A finite set of query processors are adopted to produce these partial results. Processors are placed in front of each partition and report progressive analytics to a central entity. In this paper, we envision the query controller (QC) as the central entity that collects progressive analytics and return the final response to users/applications. The QC receives partial ordered sets of objects and aggregates them to derive the final outcome. We focus on a QC that applies time-optimized techniques and aggregation operators to deliver every response, i.e., ordered sets, over streams of partial ordered subsets. We perform a comprehensive performance assessment with synthetic data and report on the performance of the QC. Our experimental evaluation reveals the pros and cons of the proposed model and a comparison assessment places this paper in the respective literature.},
	number = {99},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Kolomvatsos, K and Hadjiefthymiades, S},
	year = {2017},
	keywords = {Decision making, Distributed},
	pages = {1--12},
}

@inproceedings{Zeng:2016:IMU:2882903.2915240,
	address = {New York, NY, USA},
	title = {{iOLAP}: {Managing} {Uncertainty} for {Efficient} {Incremental} {OLAP}},
	isbn = {978-1-4503-3531-7},
	url = {http://doi.acm.org/10.1145/2882903.2915240},
	doi = {10.1145/2882903.2915240},
	abstract = {The size of data and the complexity of analytics continue to grow along with the need for timely and cost-effective analysis. However, the growth of computation power cannot keep up with the growth of data. This calls for a paradigm shift from traditional batch OLAP processing model to an incremental OLAP processing model. In this paper, we propose iOLAP, an incremental OLAP query engine that provides a smooth trade-off between query accuracy and latency, and fulfills a full spectrum of user requirements from approximate but timely query execution to a more traditional accurate query execution. iOLAP enables interactive incremental query processing using a novel mini-batch execution model---given an OLAP query, iOLAP first randomly partitions the input dataset into smaller sets (mini-batches) and then incrementally processes through these mini-batches by executing a delta update query on each mini-batch, where each subsequent delta update query computes an update based on the output of the previous one. The key idea behind iOLAP is a novel delta update algorithm that models delta processing as an uncertainty propagation problem, and minimizes the recomputation during each subsequent delta update by minimizing the uncertainties in the partial (including intermediate) query results. We implement iOLAP on top of Apache Spark and have successfully demonstrated it at scale on over 100 machines. Extensive experiments on a multitude of queries and datasets demonstrate that iOLAP can deliver approximate query answers for complex OLAP queries orders of magnitude faster than traditional OLAP engines, while continuously delivering updates every few seconds.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Zeng, Kai and Agarwal, Sameer and Stoica, Ion},
	year = {2016},
	note = {Series Title: SIGMOD '16},
	keywords = {OLAP, cluster:Query Approximation, incremental, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1347--1361},
}

@inproceedings{Kandula:2016:QLA:2882903.2882940,
	address = {New York, NY, USA},
	title = {Quickr: {Lazily} {Approximating} {Complex} {AdHoc} {Queries} in {BigData} {Clusters}},
	isbn = {978-1-4503-3531-7},
	url = {http://doi.acm.org/10.1145/2882903.2882940},
	doi = {10.1145/2882903.2882940},
	abstract = {We present a system that approximates the answer to complex ad-hoc queries in big-data clusters by injecting samplers on-the-fly and without requiring pre-existing samples. Improvements can be substantial when big-data queries take multiple passes over data and when samplers execute early in the query plan. We present a new, universe, sampler which is able to sample multiple join inputs. By incorporating samplers natively into a cost-based query optimizer, we automatically generate plans with appropriate samplers at appropriate locations. We devise an accuracy analysis method using which we ensure that query plans with samplers will not miss groups and that aggregate values are within a small ratio of their true value. An implementation on a cluster with tens of thousands of machines shows that queries in the TPC-DS benchmark use a median of 2X fewer resources. In contrast, approaches that construct input samples even when given 10X the size of the input to store samples improve only 22\% of the queries, i.e., a median speed up of 0X.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Kandula, Srikanth and Shanbhag, Anil and Vitorovic, Aleksandar and Olma, Matthaios and Grandl, Robert and Chaudhuri, Surajit and Ding, Bolin},
	year = {2016},
	note = {Series Title: SIGMOD '16},
	keywords = {Big Data, Clustering, cluster:Query Approximation, data-parallel, layer:Middleware, parallel, query optimization, relational, sampling, sampling joins, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {631--646},
}

@inproceedings{7113363,
	title = {Dynamic interaction graphs with probabilistic edge decay},
	url = {http://ieeexplore.ieee.org/document/7113363/},
	doi = {10.1109/ICDE.2015.7113363},
	abstract = {A large scale network of social interactions, such as mentions in Twitter, can often be modeled as a “dynamic interaction graph” in which new interactions (edges) are continually added over time. Existing systems for extracting timely insights from such graphs are based on either a cumulative “snapshot” model or a “sliding window” model. The former model does not sufficiently emphasize recent interactions. The latter model abruptly forgets past interactions, leading to discontinuities in which, e.g., the graph analysis completely ignores historically important influencers who have temporarily gone dormant. We introduce TIDE, a distributed system for analyzing dynamic graphs that employs a new “probabilistic edge decay” (PED) model. In this model, the graph analysis algorithm of interest is applied at each time step to one or more graphs obtained as samples from the current “snapshot” graph that comprises all interactions that have occurred so far. The probability that a given edge of the snapshot graph is included in a sample decays over time according to a user specified decay function. The PED model allows controlled trade-offs between recency and continuity, and allows existing analysis algorithms for static graphs to be applied to dynamic graphs essentially without change. For the important class of exponential decay functions, we provide efficient methods that leverage past samples to incrementally generate new samples as time advances. We also exploit the large degree of overlap between samples to reduce memory consumption from O(N) to O(logN) when maintaining N sample graphs. Finally, we provide bulk-execution methods for applying graph algorithms to multiple sample graphs simultaneously without requiring any changes to existing graph-processing APIs. Experiments on a real Twitter dataset demonstrate the effectiveness and efficiency of our TIDE prototype, which is built on top of- the Spark distributed computing framework.},
	booktitle = {2015 {IEEE} 31st {International} {Conference} on {Data} {Engineering}},
	author = {Xie, W and Tian, Y and Sismanis, Y and Balmin, A and Haas, P J},
	month = apr,
	year = {2015},
	note = {ISSN: 1063-6382},
	keywords = {Internet, graph theory, probability},
	pages = {1143--1154},
}

@inproceedings{Ramnarayan:2016:SHT:2882903.2899408,
	address = {New York, NY, USA},
	title = {{SnappyData}: {A} {Hybrid} {Transactional} {Analytical} {Store} {Built} {On} {Spark}},
	isbn = {978-1-4503-3531-7},
	url = {http://doi.acm.org/10.1145/2882903.2899408},
	doi = {10.1145/2882903.2899408},
	abstract = {In recent years, our customers have expressed frustration in the traditional approach of using a combination of disparate products to handle their streaming, transactional and analytical needs. The common practice of stitching heterogeneous environments in custom ways has caused enormous production woes by increasing development complexity and total cost of ownership. With SnappyData, an open source platform, we propose a unified engine for real-time operational analytics, delivering stream analytics, OLTP and OLAP in a single integrated solution. We realize this platform through a seamless integration of Apache Spark (as a big data computational engine) with GemFire (as an in-memory transactional store with scale-out SQL semantics). In this demonstration, after presenting a few use case scenarios, we exhibit SnappyData as our our in-memory solution for delivering truly interactive analytics (i.e., a couple of seconds), when faced with large data volumes or high velocity streams. We show that SnappyData can exploit state-of-the-art approximate query processing techniques and a variety of data synopses. Finally, we allow the audience to define various high-level accuracy contracts (HAC), to communicate their accuracy requirements with SnappyData in an intuitive fashion.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Ramnarayan, Jags and Mozafari, Barzan and Wale, Sumedh and Menon, Sudhir and Kumar, Neeraj and Bhanawat, Hemant and Chakraborty, Soubhik and Mahajan, Yogesh and Mishra, Rishitesh and Bachhav, Kishor},
	year = {2016},
	note = {Series Title: SIGMOD '16},
	keywords = {OLAP, OLTP, in-memory database, stream analytics, stream processing},
	pages = {2153--2156},
}

@inproceedings{Moritz2017a,
	address = {New York, NY, USA},
	title = {What {Users} {Don}'t {Expect} about {Exploratory} {Data} {Analysis} on {Approximate} {Query} {Processing} {Systems}},
	isbn = {978-1-4503-5029-7},
	url = {http://dx.doi.org/10.1145/3077257.3077258},
	doi = {10.1145/3077257.3077258},
	abstract = {Pangloss implements " Optimistic Visualization " , a method that gives analysts confidence to use approximate results for exploratory data analysis. In this paper, we outline how analysts' experience with an approximate visualization system did not match their intuitions. These observations have implications for the design of future data exploration systems that expose uncertainty. We also describe requirements for approximate query engines to enable the next generation of exploratory visualization systems.},
	booktitle = {Proceedings of the {2Nd} {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	publisher = {ACM},
	author = {Moritz, Dominik and Fisher, Danyel},
	year = {2017},
	note = {Series Title: HILDA'17},
	keywords = {KEYWORDS approximate query processing, Visualization, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Validation Research},
	pages = {9:1--9:4},
}

@article{Rusu:2015:WAC:2838914.2818178,
	title = {Workload-{Driven} {Antijoin} {Cardinality} {Estimation}},
	volume = {40},
	issn = {03625915},
	url = {https://dl.acm.org/citation.cfm?id=2818178},
	doi = {10.1145/2818178},
	abstract = {Antijoin cardinality estimation is among a handful of problems that has eluded accurate efficient solutions amenable to implementation in relational query optimizers. Given the widespread use of antijoin and subset-based queries in analytical workloads and the extensive research targeted at join cardinality estimation-a seemingly related problem-the lack of adequate solutions for antijoin cardinality estimation is intriguing. In this article, we introduce a novel sampling-based estimator for antijoin cardinality that (unlike existent estimators) provides sufficient accuracy and efficiency to be implemented in a query optimizer. The proposed estimator incorporates three novel ideas. First, we use prior workload information when learning a mixture superpopulation model of the data offline. Second, we design a Bayesian statistics framework that updates the superpopulation model according to the live queries, thus allowing the estimator to adapt dynamically to the online workload. Third, we develop an efficient algorithm for sampling from a hypergeometric distribution in order to generate Monte Carlo trials, without explicitly instantiating either the population or the sample. When put together, these ideas form the basis of an efficient antijoin cardinality estimator satisfying the strict requirements of a query optimizer, as shown by the extensive experimental results over synthetically generated as well as massive TPC-H data. ABSTRACT FROM AUTHOR]; Copyright of ACM Transactions on Database Systems is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	number = {3},
	journal = {ACM Transactions on Database Systems},
	author = {RUSU, FLORIN and ZHUANG, ZIXUAN and MINGXI, W U and JERMAINE, CHRIS},
	month = oct,
	year = {2015},
	note = {Publisher: ACM
Place: New York, NY, USA},
	keywords = {ESTIMATION theory, MATHEMATICAL optimization, MONTE Carlo method, Monte Carlo, SAMPLING (Statistics), cluster:Sampling, layer:Database Layer, query optimization, sampling, supercluster:Data Storage, type:Proposal of Solution},
	pages = {16:1--16:41},
}

@inproceedings{Kalyvianaki:2016:TFF:2882903.2882943,
	address = {New York, NY, USA},
	title = {Themis},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882943},
	doi = {10.1145/2882903.2882943},
	abstract = {Federated stream processing systems, which utilise nodes from multiple independent domains, can be found increasingly in multi-provider cloud deployments, internet-of-things systems, collaborative sensing applications and large-scale grid systems. To pool resources from several sites and take advantage of local processing, submitted queries are split into query fragments, which are executed collaboratively by different sites. When supporting many concurrent users, however, queries may exhaust available processing resources, thus requiring constant load shedding. Given that individual sites have autonomy over how they allocate query fragments on their nodes, it is an open challenge how to ensure global fairness on processing quality experienced by queries in a federated scenario. We describe THEMIS, a federated stream processing system for resource-starved, multi-site deployments. It executes queries in a globally fair fashion and provides users with constant feedback on the experienced processing quality for their queries. THEMIS associates stream data with its source information content (SIC), a metric that quantifies the contribution of that data towards the query result, based on the amount of source data used to generate it. We provide the BALANCE-SIC distributed load shedding algorithm that balances the SIC values of result data. Our evaluation shows that the BALANCE-SIC algorithm yields balanced SIC values across queries, as measured by Jain's Fairness Index. Our approach also incurs a low execution time overhead.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data} - {SIGMOD} '16},
	publisher = {ACM},
	author = {Kalyvianaki, Evangelia and Fiscato, Marco and Salonidis, Theodoros and Pietzuch, Peter},
	year = {2016},
	note = {Series Title: SIGMOD '16},
	keywords = {fairness, federated data stream processing, tuple shedding},
	pages = {541--553},
}

@article{Kolomvatsos2015,
	title = {An {Efficient} {Time} {Optimized} {Scheme} for {Progressive} {Analytics} in {Big} {Data}},
	volume = {2},
	issn = {22145796},
	url = {http://www.sciencedirect.com/science/article/pii/S2214579615000106},
	doi = {10.1016/j.bdr.2015.02.001},
	abstract = {Big data analytics is the key research subject for future data driven decision making applications. Due to the large amount of data, progressive analytics could provide an efficient way for querying big data clusters. Each cluster contains only a piece of the examined data. Continuous queries over these data sources require intelligent mechanisms to result the final outcome (query response) in the minimum time with the maximum performance. A Query Controller (QC) is responsible to manage continuous/sequential queries and return the final outcome to users or applications. In this paper, we propose a mechanism that can be adopted by the QC. The proposed mechanism is capable of managing partial results retrieved by a number of processors each one responsible for each cluster. Each processor executes a query over a specific cluster of data. Our mechanism adopts two sequential decision making models for handling the incoming partial results. The first model is based on a finite horizon time-optimized model and the second one is based on an infinite horizon optimally scheduled model. We provide mathematical formulations for solving the discussed problem and present simulation results. Through a large number of experiments, we reveal the advantages of the proposed models and give numerical results comparing them with a deterministic model. These results indicate that the proposed models can efficiently reduce the required time for returning the final outcome to the user/application while keeping the quality of the aggregated result at high levels.},
	number = {4},
	journal = {Big Data Research},
	author = {Kolomvatsos, Kostas and Anagnostopoulos, Christos and Hadjiefthymiades, Stathes},
	year = {2015},
	note = {ISBN: 2214-5796},
	keywords = {Big Data, Continuous queries, Progressive analytics, Sequential time-optimized models, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {155--165},
}

@incollection{Kumari2017,
	address = {Singapore},
	title = {Challenges of modern query processing},
	volume = {507},
	isbn = {978-981-10-2470-2},
	url = {https://doi.org/10.1007/978-981-10-2471-9_41},
	abstract = {Efficient query processing plays a critical role in numerous settings especially in case of data centric applications. Starting from the architecture to the final stage of result compilation a query processing system has to undertake several challenges. In this paper, we compare different architectures and existing approaches of query processing. In modern days, database systems have to deal with data distribution. To make challenges more complex, the participating databases might be heterogeneous in nature. In this paper, we discuss various challenges of query processing systems in centralized, distributed, and multidatabase backgrounds. We also analyze various parameters and metrics that directly impact query processing.},
	booktitle = {Advances in {Intelligent} {Systems} and {Computing}},
	publisher = {Springer Singapore},
	author = {Kumari, A. and Singh, V.},
	editor = {Satapathy, Suresh Chandra and Prasad, V Kamakshi and Rani, B Padmaja and Udgata, Siba K and Raju, K Srujan},
	year = {2017},
	doi = {10.1007/978-981-10-2471-9_41},
	keywords = {★},
	pages = {423--432},
}

@article{DBLP:journals/corr/SparksTFJK15,
	title = {{TuPAQ}: {An} {Efficient} {Planner} for {Large}-scale {Predictive} {Analytic} {Queries}},
	volume = {abs/1502.0},
	url = {http://arxiv.org/abs/1502.0006},
	abstract = {The proliferation of massive datasets combined with the development of sophisticated analytical techniques have enabled a wide variety of novel applications such as improved product recommendations, automatic image tagging, and improved speech-driven interfaces. These and many other applications can be supported by Predictive Analytic Queries (PAQs). A major obstacle to supporting PAQs is the challenging and expensive process of identifying and training an appropriate predictive model. Recent efforts aiming to automate this process have focused on single node implementations and have assumed that model training itself is a black box, thus limiting the effectiveness of such approaches on largescale problems. In this work, we build upon these recent efforts and propose an integrated PAQ planning architecture that combines advanced model search techniques, bandit resource allocation via runtime algorithm introspection, and physical optimization via batching. The result is TUPAQ, a component of the MLbase system, which solves the PAQ planning problem with comparable quality to exhaustive strategies but an order of magnitude more efficiently than the standard baseline approach, and can scale to models trained on terabytes of data across hundreds of machines.},
	number = {Ml},
	journal = {arXiv preprint arXiv:1502.00068},
	author = {Sparks, Evan R. and Talwalkar, Ameet and Franklin, Michael J. and Jordan, Michael I. and Kraska, Tim},
	year = {2015},
	note = {arXiv: 1502.0006},
}

@inproceedings{8026934,
	title = {Logic-{Partition} {Based} {Gaussian} {Sampling} for {Online} {Aggregation}},
	url = {http://ieeexplore.ieee.org/document/8026934/},
	doi = {10.1109/CBD.2017.39},
	abstract = {Online aggregation is a commonly used technology to return approximate query results over random samples, which provides a fast way for users to obtain a trade-off between time and accuracy. The key issue of online aggregation is how to guarantee the efficiency and effectiveness of random sample collection. However, the state-of-the-art approaches either adopt the random sampling method or adopt the sequential sampling with preprocessing to obtain the uniform samples. The former one suffers from the inefficient random access to the whole dataset especially for skewed data distribution, and the later one is limited by the time-consuming preprocessing. To make the sampling more efficient, we propose a scalable sampling algorithm called logic-partition based Gaussian sampling. The basic idea of our solution is convert the random sampling into a near-sequential sampling without any extra preprocessing, and achieve a balance between the sampling efficiency and sample quality. Extensive experiments using the TPC-H benchmark for skewed data distribution have demonstrated the superior performance of our solution.},
	booktitle = {2017 {Fifth} {International} {Conference} on {Advanced} {Cloud} and {Big} {Data} ({CBD})},
	author = {Zhang, L and Wang, Y and Xu, X},
	month = aug,
	year = {2017},
	keywords = {Gaussian processes, cluster:Sampling, layer:Database Layer, query processing, sampling metho, supercluster:Data Storage, type:Proposal of Solution},
	pages = {182--187},
}

@inproceedings{Sparks:2015:AMS:2806777.2806945,
	address = {New York, NY, USA},
	title = {Automating model search for large scale machine learning},
	isbn = {978-1-4503-3651-2},
	url = {http://dl.acm.org/citation.cfm?doid=2806777.2806945},
	doi = {10.1145/2806777.2806945},
	abstract = {The proliferation of massive datasets combined with the development of sophisticated analytical techniques has enabled a wide variety of novel applications such as improved product recommendations, automatic image tagging, and improved speech-driven interfaces. A major obstacle to supporting these predictive applications is the challenging and expensive process of identifying and training an appropriate predictive model. Recent efforts aiming to automate this process have focused on single node implementations and have assumed that model training itself is a black box, limiting their usefulness for applications driven by large-scale datasets. In this work, we build upon these recent efforts and propose an architecture for automatic machine learning at scale comprised of a cost-based cluster resource allocation estimator, advanced hyper-parameter tuning techniques, bandit resource allocation via runtime algorithm introspection, and physical optimization via batching and optimal resource allocation. The result is TuPAQ, a component of the MLbase system that automatically finds and trains models for a user's predictive application with comparable quality to those found using exhaustive strategies, but an order of magnitude more efficiently than the standard baseline approach. TuPAQ scales to models trained on Terabytes of data across hundreds of machines.},
	booktitle = {Proceedings of the {Sixth} {ACM} {Symposium} on {Cloud} {Computing} - {SoCC} '15},
	publisher = {ACM},
	author = {Sparks, Evan R. and Talwalkar, Ameet and Haas, Daniel and Franklin, Michael J. and Jordan, Michael I. and Kraska, Tim},
	year = {2015},
	note = {Series Title: SoCC '15},
	pages = {368--380},
}

@article{DBLP:journals/corr/YuS16,
	title = {Hippo: {A} {Fast}, yet {Scalable}, {Database} {Indexing} {Approach}},
	volume = {abs/1604.0},
	url = {http://arxiv.org/abs/1604.03234},
	abstract = {Even though existing database indexes (e.g., B+-Tree) speed up the query execution, they suffer from two main drawbacks: (1) A database index usually yields 5\% to 15\% additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on modern storage devices like Solid State Disk (SSD) or Non-Volatile Memory (NVM). (2) Maintaining a database index incurs high latency because the DBMS has to find and update those index pages affected by the underlying table changes. This paper proposes Hippo a fast, yet scalable, database indexing approach. Hippo only stores the pointers of disk pages along with light weight histogram-based summaries. The proposed structure significantly shrinks index storage and maintenance overhead without compromising much on query execution performance. Experiments, based on real Hippo implementation inside PostgreSQL 9.5, using the TPC-H benchmark show that Hippo achieves up to two orders of magnitude less storage space and up to three orders of magnitude less maintenance overhead than traditional database indexes, i.e., B+-Tree. Furthermore, the experiments also show that Hippo achieves comparable query execution performance to that of the B+-Tree for various selectivity factors.},
	number = {1},
	journal = {arXiv preprint arXiv:1604.03234},
	author = {Yu, Jia and Sarwat, Mohamed},
	year = {2016},
	note = {arXiv: 1604.03234},
}

@inproceedings{Wang2017,
	title = {{AQP}++: {A} {Hybrid} {Approximate} {Query} {Processing} {Framework} for {Generalized} {Aggregation} {Queries}},
	isbn = {978-1-5090-3677-6},
	url = {http://ieeexplore.ieee.org/document/7815186/},
	doi = {10.1109/CBD.2016.020},
	abstract = {A sampling-based approximate query processing (AQP) method provides a fast way for users to obtain a trade-off between accuracy and time consumption by executing the query on a sample of data rather than the whole dataset. There are two major AQP methods: the (1) central limit theorem (CLT)-based online aggregation; and the (2) bootstrap method. The former is very efficient but is only suitable for simple aggregation queries, while the latter is quite general but has relatively high computational overhead. Both methods suffer from the possible estimation failure. However, there is no technology that can both support simple/complex queries within an acceptable time coupled with carefully considering the estimation failure. To make the current AQP method much more general and efficient, we propose a hybrid approximate query framework called AQP++ to combine the advantages of both methods and eliminate the limitations as far as possible. According to this hybrid framework, an estimation parameters adjustment method is presented for CLT-based online aggregation to improve its usability for much more complex aggregation queries. Then, an execution cost model is proposed to describe the computational overhead of the two AQP methods, which can be used to support our dynamic scheduling mechanism of AQP++ and make the whole system more efficient and flexible. Moreover, we have implemented our AQP++ prototype and conducted extensive experiments on the TPC-H benchmark for skewed data distribution. Our results demonstrate that our AQP++ can produce acceptable approximate results for both simple and complex queries within a much shorter time compared with the original CLT-based online aggregation and bootstrap method.},
	booktitle = {2016 {International} {Conference} on {Advanced} {Cloud} and {Big} {Data} ({CBD})},
	author = {Wang, Yuxiang and Xu, Xiaoliang and Xia, Yixing and Fang, Qiming},
	year = {2017},
	note = {ISSN: 18777503},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {56--62},
}

@article{Cheng2017a,
	title = {{OLA}-{RAW} : {Scalable} {Exploration} over {Raw} {Data}},
	url = {https://arxiv.org/abs/1702.00358},
	abstract = {In-situ processing has been proposed as a novel data exploration solution in many domains generating massive amounts of raw data, e.g., astronomy, since it provides immediate SQL querying over raw files. The performance of in-situ processing across a query workload is, however, limited by the speed of full scan, tokenizing, and parsing of the entire data. Online aggregation (OLA) has been introduced as an efficient method for data exploration that identifies uninteresting patterns faster by continuously estimating the result of a computation during the actual processing---the computation can be stopped as early as the estimate is accurate enough to be deemed uninteresting. However, existing OLA solutions have a high upfront cost of randomly shuffling and/or sampling the data. In this paper, we present OLA-RAW, a bi-level sampling scheme for parallel online aggregation over raw data. Sampling in OLA-RAW is query-driven and performed exclusively in-situ during the runtime query execution, without data reorganization. This is realized by a novel resource-aware bi-level sampling algorithm that processes data in random chunks concurrently and determines adaptively the number of sampled tuples inside a chunk. In order to avoid the cost of repetitive conversion from raw data, OLA-RAW builds and maintains a memory-resident bi-level sample synopsis incrementally. We implement OLA-RAW inside a modern in-situ data processing system and evaluate its performance across several real and synthetic datasets and file formats. Our results show that OLA-RAW chooses the sampling plan that minimizes the execution time and guarantees the required accuracy for each query in a given workload. The end result is a focused data exploration process that avoids unnecessary work and discards uninteresting data.},
	journal = {arXiv.org},
	author = {Cheng, Yu and Zhao, Weijie and Rusu, Florin},
	year = {2017},
	note = {arXiv: 1702.00358},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage},
	pages = {1--23},
}

@inproceedings{7264331,
	title = {Interactive and {Scalable} {Exploration} of {Big} {Spatial} {Data} -- {A} {Data} {Management} {Perspective}},
	volume = {1},
	url = {http://ieeexplore.ieee.org/document/7264331/},
	doi = {10.1109/MDM.2015.67},
	abstract = {Recently, the volume of available spatial data increased tremendously. For instance, in November 2013 NASA announced the release of hundreds of Terabytes of its earth remote sensing dataset. Such data includes but not limited to: weather maps, socioeconomic data, vegetation indices, geological maps, and more. Making sense of such spatial data will be beneficial for several applications that may transform science and society -- For example: (1) Space Science: that allows astronomers to study and probably discover new features of both the earth and the outer space, (2) Socio-Economic Analysis: that includes for example climate change analysis, study of deforestation, population migration, and variation in sea levels, (3) Urban Planning: assisting government in city planning, road network design, and transportation engineering, (4) Disaster Planning: that helps in assessing the impact of natural disasters. The main aim of this paper is to investigate novel data management techniques that enable interactive and scalable exploration of big spatial data. The paper envisions novel system architectures that provide support for interactive and spatial data exploration, as follows: (1) The paper suggests extending data analytics frameworks, e.g., Apache Spark, to support spatial data types and operations at scale. The resulting framework will serve as a scalable backbone for processing spatial data exploration tasks. (2) It also sketches novel structures and algorithms that leverage modern hardware, e.g., SSDs, and in-memory data processing techniques to efficiently store and access spatial data. Second, the paper proposes extending spatial database systems to support an exploration-aware spatial query evaluation paradigm through three novel components: (1) Spatial Query Steering: that allows the user to slightly modify the query conditions online (zooming in/out) and retrieve the new results in very low latency. (2) Recommendation-Aware Spatial Querying: that injects the re- ommendation functionality inside classical spatial query executors to support spatial data recommendation. It leverages recommendation algorithms to predict what spatial objects/areas the user would like based on her past interactions with the system. (3) Spatial Query Approximation: That aims at achieving interactive performance by studying the tradeoff between approximate spatial data exploration and query response time.},
	booktitle = {2015 16th {IEEE} {International} {Conference} on {Mobile} {Data} {Management}},
	author = {Sarwat, M},
	month = jun,
	year = {2015},
	note = {ISSN: 1551-6245},
	keywords = {Big Data, cluster:Spatial Query, geophysical prospecting, layer:Database Layer, query processing, supercluster:Indexes, type:Philosophical Paper},
	pages = {263--270},
}

@article{Venkataraman2014,
	title = {The power of choice in data-aware cluster scheduling},
	abstract = {Providing timely results in the face of rapid growth in data volumes has become important for analytical frame-works. For this reason, frameworks increasingly operate on only a subset of the input data. A key property of such sampling is that combinatorially many subsets of the input are present. We present KMN, a system that leverages these choices to perform data-aware scheduling, i.e., minimize time taken by tasks to read their inputs, for a DAG of tasks. KMN not only uses choices to colocate tasks with their data but also percolates such combinatorial choices to downstream tasks in the DAG by launching a few additional tasks at every upstream stage. Evaluations using workloads from Facebook and Conviva on a 100-machine EC2 cluster show that KMN reduces average job duration by 81\% using just 5\% ad-ditional resources.},
	journal = {USENIX conference on Operating Systems Design and Implementation},
	author = {Venkataraman, Shivaram and Panda, Aurojit and Franklin, Michael J and Stoica, Ion and Ananthanarayanan, Ganesh and Franklin, Michael J and Stoica, Ion},
	year = {2014},
	note = {ISBN: 9781931971164},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {301--316},
}

@article{DBLP:journals/corr/QinR15,
	title = {Speculative {Approximations} for {Terascale} {Analytics}},
	volume = {abs/1501.0},
	url = {http://arxiv.org/abs/1501.0255},
	doi = {10.1145/2799562.2799563},
	abstract = {Model calibration is a major challenge faced by the plethora of statistical analytics packages that are increasingly used in Big Data applications. Identifying the optimal model parameters is a time-consuming process that has to be executed from scratch for every dataset/model combination even by experienced data scientists. We argue that the incapacity to evaluate multiple parameter configurations simultaneously and the lack of support to quickly identify sub-optimal configurations are the principal causes. In this paper, we develop two database-inspired techniques for efficient model calibration. Speculative parameter testing applies advanced parallel multi-query processing methods to evaluate several configurations concurrently. The number of configurations is determined adaptively at runtime, while the configurations themselves are extracted from a distribution that is continuously learned following a Bayesian process. Online aggregation is applied to identify sub-optimal configurations early in the processing by incrementally sampling the training dataset and estimating the objective function corresponding to each configuration. We design concurrent online aggregation estimators and define halting conditions to accurately and timely stop the execution. We apply the proposed techniques to distributed gradient descent optimization -- batch and incremental -- for support vector machines and logistic regression models. We implement the resulting solutions in GLADE PF-OLA -- a state-of-the-art Big Data analytics system -- and evaluate their performance over terascale-size synthetic and real datasets. The results confirm that as many as 32 configurations can be evaluated concurrently almost as fast as one, while sub-optimal configurations are detected accurately in as little as a \$1/20{\textasciicircum}\{{\textbackslash}text\{th\}\}\$ fraction of the time.},
	journal = {CoRR},
	author = {Qin, Chengjie and Rusu, Florin},
	year = {2015},
	note = {arXiv: 1501.0255
ISBN: 9781450337243},
	pages = {1--7},
}

@article{DBLP:journals/corr/RansfordC15,
	title = {{SAP}: an {Architecture} for {Selectively} {Approximate} {Wireless} {Communication}},
	volume = {abs/1510.0},
	url = {http://arxiv.org/abs/1510.03955},
	abstract = {Integrity checking is ubiquitous in data networks, but not all network traffic needs integrity protection. Many applications can tolerate slightly damaged data while still working acceptably, trading accuracy versus efficiency to save time and energy. Such applications should be able to receive damaged data if they so desire. In today's network stacks, lower-layer integrity checks discard damaged data regardless of the application's wishes, violating the End-to-End Principle. This paper argues for optional integrity checking and gently redesigns a commodity network architecture to support integrity-unprotected data. Our scheme, called Selective Approximate Protocol (SAP), allows applications to coordinate multiple network layers to accept potentially damaged data. Unlike previous schemes that targeted video or media streaming, SAP is generic. SAP's improved throughput and decreased retransmission rate is a good match for applications in the domain of approximate computing. Implemented atop WiFi as a case study, SAP works with existing physical layers and requires no hardware changes. SAP's benefits increase as channel conditions degrade. In tests of an error-tolerant file-transfer application over WiFi, SAP sped up transmission by about 30\% on average.},
	journal = {CoRR},
	author = {Ransford, Benjamin and Ceze, Luis},
	year = {2015},
	note = {arXiv: 1510.03955},
}

@article{DBLP:journals/corr/QuocBBCFS17,
	title = {Privacy {Preserving} {Stream} {Analytics}: {The} {Marriage} of {Randomized} {Response} and {Approximate} {Computing}},
	volume = {abs/1701.0},
	url = {http://arxiv.org/abs/1701.05403},
	abstract = {How to preserve users' privacy while supporting high-utility analytics for low-latency stream processing? To answer this question: we describe the design, implementation, and evaluation of PRIVAPPROX, a data analytics system for privacy-preserving stream processing. PRIVAPPROX provides three properties: (i) Privacy: zero-knowledge privacy guarantees for users, a privacy bound tighter than the state-of-the-art differential privacy; (ii) Utility: an interface for data analysts to systematically explore the trade-offs between the output accuracy (with error-estimation) and query execution budget; (iii) Latency: near real-time stream processing based on a scalable "synchronization-free" distributed architecture. The key idea behind our approach is to marry two existing techniques together: namely, sampling (used in the context of approximate computing) and randomized response (used in the context of privacy-preserving analytics). The resulting marriage is complementary - it achieves stronger privacy guarantees and also improves performance, a necessary ingredient for achieving low-latency stream analytics.},
	journal = {CoRR},
	author = {Quoc, Do Le and Beck, Martin and Bhatotia, Pramod and Chen, Ruichuan and Fetzer, Christof and Strufe, Thorsten},
	year = {2017},
	note = {arXiv: 1701.05403},
}

@inproceedings{Cheng2017,
	address = {New York, NY, USA},
	title = {Bi-{Level} {Online} {Aggregation} on {Raw} {Data}},
	isbn = {978-1-4503-5282-6},
	url = {http://dl.acm.org/citation.cfm?doid=3085504.3085514},
	doi = {10.1145/3085504.3085514},
	abstract = {In-situ processing has been proposed as a novel data exploration solution in many domains generating massive amounts of raw data, e.g., astronomy, since it provides immediate SQL querying over raw files. The performance of in-situ processing across a query workload is, however, limited by the speed of full scan, tokenizing, and parsing of the entire data. Online aggregation (OLA) has been introduced as an efficient method for data exploration that identifies uninteresting patterns faster by continuously estimating the result of a computation during the actual processing---the computation can be stopped as early as the estimate is accurate enough to be deemed uninteresting. However, existing OLA solutions have a high upfront cost of randomly shuffling and/or sampling the data. In this paper, we present OLA-RAW, a bi-level sampling scheme for parallel online aggregation over raw data. Sampling in OLA-RAW is query-driven and performed exclusively in-situ during the runtime query execution, without data reorganization. This is realized by a novel resource-aware bi-level sampling algorithm that processes data in random chunks concurrently and determines adaptively the number of sampled tuples inside a chunk. In order to avoid the cost of repetitive conversion from raw data, OLA-RAW builds and maintains a memory-resident bi-level sample synopsis incrementally. We implement OLA-RAW inside a modern in-situ data processing system and evaluate its performance across several real and synthetic datasets and file formats. Our results show that OLA-RAW chooses the sampling plan that minimizes the execution time and guarantees the required accuracy for each query in a given workload. The end result is a focused data exploration process that avoids unnecessary work and discards uninteresting data.},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {ACM},
	author = {Cheng, Yu and Zhao, Weijie and Rusu, Florin},
	year = {2017},
	note = {Series Title: SSDBM '17},
	keywords = {Astronomy, RRaw: Yes, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution, ★},
	pages = {1--12},
}

@inproceedings{Rajan:2016:PEP:2987550.2987566,
	address = {New York, NY, USA},
	title = {{PerfOrator}: {Eloquent} {Performance} {Models} for {Resource} {Optimization}},
	isbn = {978-1-4503-4525-5},
	url = {http://doi.acm.org/10.1145/2987550.2987566},
	doi = {10.1145/2987550.2987566},
	abstract = {Query Optimization focuses on finding the best query execution plan, given fixed hardware resources. In BigData settings, both pay-as-you-go clouds and on-prem shared clusters, a complementary challenge emerges: Resource Optimization: find the best hardware resources, given an execution plan. In this world, provisioning is almost instantaneous and time-varying resources can be acquired on a per-query basis. This allows us to optimize allocations for completion time, resource usage, dollar cost, etc. These optimizations have a huge impact on performance and cost, and pivot around a core challenge: faithful resource-to-performance models for arbitrary BigData queries. This task is challenging for users and tools alike due to lack of good statistics (high-velocity, unstructured data), frequent use of UDFs, impact on performance of different hardware types and a lack of understanding of parallel execution at such a scale. We address this with PerfOrator, a novel approach to resource-to-performance modeling. PerfOrator employs nonlinear regression on profile runs to model arbitrary UDFs, calibration queries to generalize across hardware platforms, and analytical framework models to account for parallelism. The resulting estimates are orders of magnitude more accurate than existing approaches (e.g, Hive's optimizer), and have been successfully employed in two resource optimization scenarios: 1) optimize provisioning of clusters in cloud settings---with decisions within 1\% of optimal, 2) reserve skyline of resources for SLA jobs---with accuracies over 10x better than human experts.},
	booktitle = {Proceedings of the {Seventh} {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Rajan, Kaushik and Kakadia, Dharmesh and Curino, Carlo and Krishnan, Subru},
	year = {2016},
	note = {Series Title: SoCC '16},
	pages = {415--427},
}

@article{Kolomvatsos2017,
	title = {Learning the engagement of query processors for intelligent analytics},
	volume = {46},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-016-0821-z},
	doi = {10.1007/s10489-016-0821-z},
	abstract = {Current applications require the processing of huge amounts of data produced by applications or end users personal devices. In such settings, intelligent analytics on top of large scale data are the key research subject for future data driven decision making. Due to the huge amount of data, analytics should be based on an efficient technique for querying big data partitions. Each partition contains only a part of the data and a processor is dedicated to execute queries for the corresponding partition. A Query Controller (QC) is responsible for managing continuous queries and returning the final outcome to users / applications by using the underlying processors. In this paper, we propose a learning scheme to be adopted by the QC for allocating each query to the available processors. We adopt the Q-learning algorithm to calculate the reward that the QC obtains for every allocation between queries and processors. The outcome is an efficient model that derives the optimal allocation for the incoming queries. We provide mathematical formulations for solving the discussed problem and present our simulation results. Through a large number of simulations, we reveal the advantages of the proposed model and give numerical results while comparing our framework with a baseline model.},
	number = {1},
	journal = {Applied Intelligence},
	author = {Kolomvatsos, Kostas and Hadjiefthymiades, Stathes},
	month = jan,
	year = {2017},
	pages = {96--112},
}

@article{DBLP:journals/corr/abs-1209-3686,
	title = {Active {Learning} for {Crowd}-{Sourced} {Databases}},
	volume = {abs/1209.3},
	url = {http://arxiv.org/abs/1209.3686},
	abstract = {Crowd-sourcing has become a popular means of acquiring labeled data for many tasks where humans are more accurate than comput- ers, such as image tagging, entity resolution, or sentiment analysis. However, due to the time and cost of human labor, solutions that solely rely on crowd-sourcing are often limited to small datasets (i.e., a few thousand items). This paper proposes algorithms for integrating machine learning into crowd-sourced databases in or- der to combine the accuracy of human labeling with the speed and cost-effectiveness of machine learning classifiers. By using active learning as our optimization strategy for labeling tasks in crowd- sourced databases, we can minimize the number of questions asked to the crowd, allowing crowd-sourced applications to scale (i.e, la- bel much larger datasets at lower costs). Designing active learning algorithms for a crowd-sourced database poses many practical challenges: such algorithms need to be generic, scalable, and easy-to-use for a broad range of practitioners, even those who are not machine learning experts. We draw on the theory of nonparametric bootstrap to design, to the best of our knowledge, the first active learning algorithms that meet all these requirements. Our results, on 3 real-world datasets collected with Amazon’s Mechanical Turk, and on 15 UCI datasets, show that our meth- ods on average ask 1–2 orders of magnitude fewer questions than the baseline, and 4.5–44×fewer than existing active learning algo- rithms.},
	journal = {CoRR},
	author = {Mozafari, Barzan and Sarkar, Purnamrita and Franklin, Michael J and Jordan, Michael I and Madden, Samuel},
	year = {2012},
}

@article{Kolomvatsos2016,
	title = {An intelligent, uncertainty driven aggregation scheme for streams of ordered sets},
	volume = {45},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-016-0789-8},
	doi = {10.1007/s10489-016-0789-8},
	abstract = {Data streams management has attracted the attention of many researchers during the recent years. The reason is that numerous devices generate huge amounts of data demanding an efficient processing scheme for delivering high quality applications. Data are reported through streams and stored into a number of partitions. Separation techniques facilitate the parallel management of data while intelligent methods are necessary to manage these multiple instances of data. Progressive analytics over huge amounts of data could be adopted to deliver partial responses and, possibly, to save time in the execution of applications. An interesting research domain is the efficient management of queries over multiple partitions. Usually, such queries demand responses in the form of ordered sets of objects (e.g., top-k queries). These ordered sets include objects in a ranked order and require novel mechanisms for deriving responses based on partial results. In this paper, we study a setting of multiple data partitions and propose an intelligent, uncertainty driven decision making mechanism that aims to respond to streams of queries. Our mechanism delivers an ordered set of objects over a number of partial ordered subsets retrieved by each partition of data. We envision that a number of query processors are placed in front of each partition and report progressive analytics to a Query Controller (QC). The QC receives queries, assigns the task to the underlying processors and decides the right time to deliver the final ordered set to the application. We propose an aggregation model for deriving the final ordered set of objects and a Fuzzy Logic (FL) inference process. We present a Type-2 FL system that decides when the QC should stop aggregating partial subsets and return the final response to the application. We report on the performance of the proposed mechanism through the execution of a large set of experiments. Our results deal with the throughput of the QC, the quality of the final ordered set of objects and the time required for delivering the final response.},
	number = {3},
	journal = {Applied Intelligence},
	author = {Kolomvatsos, Kostas},
	month = oct,
	year = {2016},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {713--735},
}

@inproceedings{7363913,
	title = {{QueRIE} reloaded: {Using} matrix factorization to improve database query recommendations},
	url = {http://ieeexplore.ieee.org/document/7363913/},
	doi = {10.1109/BigData.2015.7363913},
	abstract = {Interactive database exploration is a key task in information mining. Relational databases have been long used as a critical infrastructure component to access and analyze large volumes of data in a variety of applications, including ad-hoc analytics over big data, large-scale data warehouses that support business-intelligence tools, and services for scientific-data exploration. To aid the users of such databases, we developed the QueRIE system for personalized query recommendations. Similarly to traditional recommender systems, QueRIE continuously monitors the user's querying behavior and finds matching patterns in the system's query log, identifying "similar" users. Subsequently, these users and their queries are being used to recommend queries that the current user may find useful. We have previously shown that when employing different neighborhood-based collaborative filtering techniques, there exists a trade-off between computational efficiency and accuracy. In this paper we extend our previous work on the QueRIE framework, to address scalability, the most desirable characteristic of applications that rely on the mining of big data. Latent factor collaborative filtering models have been shown to address the scalability problem in traditional rating-based recommender systems, without much compromise to the recommender system's accuracy. In this work, we explore the use of latent factor models when, instead of ratings, the input consists of database-query log data. We show through experimentation that, as in the case of rating-based recommender systems, such techniques offer both scalability and prediction accuracy in the database query recommendations domain, outperforming the neighborhood-based approaches.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Eirinaki, M and Patel, S},
	month = oct,
	year = {2015},
	keywords = {cluster:Assisted Query Formulation, data mining, layer:User Interaction, matrix decompo, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1500--1508},
}

@article{Sellam2016,
	title = {Cluster-{Driven} {Navigation} of the {Query} {Space}},
	volume = {28},
	issn = {10414347},
	url = {http://ieeexplore.ieee.org/abstract/document/7374727/},
	doi = {10.1109/TKDE.2016.2515590},
	abstract = {How can users who know neither programming nor statistics explore large databases? We present a novel interface, designed to guide explorers through their data: Blaeu. Blaeu is a database front-end, 'boosted' with unsupervised learning primitives. Thanks to these primitives, it can summarize and recommend queries. Our first contribution is Blaeu's interaction model. With Blaeu, users explore the data through data maps. A data map is an interactive set of clusters, which users navigate with zooms and projections. Our second contribution is Blaeu's engine. We present three mapping algorithms, for three different settings. The first algorithm deals with small to medium databases, the second one targets high dimensional spaces, and the last one focuses on speed and interaction. We then present an optimization strategy based on sampling. Our experiments reveal that Blaeu can cluster millions of tuples with hundreds of columns in a few seconds on commodity hardware. © 2015 IEEE.},
	number = {5},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Sellam, Thibault and Kersten, Martin},
	year = {2016},
	keywords = {Clustering, Interactive data exploration and discovery, RInteractive: Yes, cluster:Novel Query Interfaces, layer:User Interaction, query languages, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1118--1131},
}

@inproceedings{Sellam:2015:SED:2806416.2806538,
	address = {New York, NY, USA},
	title = {Semi-{Automated} {Exploration} of {Data} {Warehouses}},
	isbn = {978-1-4503-3794-6},
	url = {http://doi.acm.org/10.1145/2806416.2806538},
	doi = {10.1145/2806416.2806538},
	abstract = {Exploratory data analysis tries to discover novel dependencies and unexpected patterns in large databases. Traditionally, this process is manual and hypothesis-driven. However, analysts can come short of patience and imagination. In this paper, we introduce Claude, a hypothesis generator for data warehouses. Claude follows a 2-step approach: (1) It detects interesting views, by exploiting non-linear statistical dependencies between the dimensions and the measure. (2) To explain its findings, it detects local patterns in these views and describes them with SQL queries. Technically, we derive a model of interestingness from fundamental information theory. To exploit this model, we present aggressive approximations and heuristics, allowing Claude to be fast and more accurate than state-of-art view selection algorithms.},
	booktitle = {Proceedings of the 24th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Sellam, Thibault and Müller, Emmanuel and Kersten, Martin},
	year = {2015},
	note = {Series Title: CIKM '15},
	keywords = {cluster:Assisted Query Formulation, data exploration, feature selec-, layer:User Interaction, query recommendation, subgroup discovery, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1321--1330},
}

@inproceedings{Sellam:2016:FEV:2949689.2949692,
	address = {New York, NY, USA},
	title = {Fast, {Explainable} {View} {Detection} to {Characterize} {Exploration} {Queries}},
	isbn = {978-1-4503-4215-5},
	url = {http://dl.acm.org/citation.cfm?doid=2949689.2949692},
	doi = {10.1145/2949689.2949692},
	abstract = {The aim of data exploration is to get acquainted with an unfamiliar database. Typically, explorers operate by trial and error: they submit a query, study the result, and refine their query subsequently. In this paper, we investigate how to help them understand their query results. In particular, we focus on medium to high dimension spaces: if the database contains dozens or hundreds of columns, which variables should they inspect? We propose to detect subspaces in which the users' selection is different from the rest of the database. From this idea, we built Ziggy, a tuple description engine. Ziggy can detect informative subspaces, and it can explain why it recommends them, with visualizations and natural language. It can cope with mixed data, missing values, and it penalizes redundancy. Our experiments reveal that it is up to an order of magnitude faster than state-of-the-art feature selection algorithms, at minimal accuracy costs.},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {ACM},
	author = {Sellam, Thibault and Kersten, Martin},
	year = {2016},
	note = {Series Title: SSDBM '16},
	keywords = {Data exploration, cluster:Assisted Query Formulation, data description, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1--12},
}

@inproceedings{Dhankar2017,
	address = {Singapore},
	title = {User search intention in interactive data exploration: {A} brief review},
	volume = {721},
	isbn = {978-981-10-5426-6},
	url = {https://doi.org/10.1007/978-981-10-5427-3_44},
	doi = {10.1007/978-981-10-5427-3_44},
	abstract = {Data exploration finds relevant data efficiently even if a user doesn't know exactly what he/she is aiming for. These exploratory search paradigms are not well-supported in traditional database systems, thus interactive data exploratory (IDE) system evolved. A naïve data user exhibits various kinds of search behavior while formulating exploratory queries. In IDE system, each user interaction leads to more relevant results, due to its highly interactive and user-centric approach. To understand why and what user is searching for, more efficient data exploration systems need to be designed. This paper aims to discuss various factors affecting the user search behavior, how an IDE system support user. Various practices for measuring system support's effectiveness are also highlighted. Finally, proposed a strategy for modeling the user search intentions for exploratory queries in IDE systems.},
	booktitle = {Advances in {Computing} and {Data} {Sciences}},
	publisher = {Springer Singapore},
	author = {Dhankar, Archana and Singh, Vikram},
	editor = {Singh, Mayank and Gupta, P K and Tyagi, Vipin and Sharma, Arun and Ören, Tuncer and Grosky, William},
	year = {2017},
	note = {ISSN: 18650929},
	keywords = {Exploratory search, Interactive Data Exploration, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Evaluation Research},
	pages = {409--419},
}

@article{Cheung2016,
	title = {Computer-{Assisted} {Query} {Formulation}},
	volume = {8},
	issn = {1935-8237},
	doi = {10.1561/XXXXXXXXXX},
	abstract = {Database management systems (DBMS) typically provide an application programming interface for users to issue queries using query languages such as SQL. Many such languages were originally designed for business data processing applications. While these applications are still relevant, two other classes of applications have become important users of data management systems: (a) web applications that issue queries programmatically to the DBMS, and (b) data analytics involving complex queries that allow data scientists to better understand their datasets. Unfortunately, existing query languages provided by database management systems are often far from ideal for these application do- mains. In this tutorial, we describe a set of technologies that assist users in specifying database queries for different application domains. The goal of such systems is to bridge the gap between current query interfaces provided by database management systems and the needs of di erent usage scenarios that are not well served by existing query languages. We discuss the different interaction modes that such systems provide and the algorithms used to infer user queries. In particular, we focus on a new class of systems built using program synthesis techniques, and furthermore discuss opportunities in combining synthesis and other methods used in prior systems to infer user queries.},
	number = {1-2},
	journal = {Foundations and Trends in Signal Processing},
	author = {Cheung, Alvin and {Armando Solar-Lezama}},
	year = {2016},
	note = {arXiv: 1408.0952v2
ISBN: 9781680830},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Evaluation Research},
	pages = {1--126},
}

@article{LiuW2017,
	title = {An {Analysis} of {Query}-{Agnostic} {Sampling} for {Interactive} {Data} {Exploration}},
	volume = {0},
	issn = {0361-0926},
	url = {http://dx.doi.org/10.1080/03610926.2017.1363231},
	doi = {10.1080/03610926.2017.1363231},
	abstract = {Data analysts often explore a large database to identify the data of interest, but may not be able to specify the exact query to send to the database. A manual data exploration process is labor-intensive and time-consuming. In the new paradigm of system-aided interactive data exploration, the Database Management System (DBMS) presents a few samples to the user in each iteration of the exploration process, and asks the user for relevance feedback. Based on the user feedback, the system automatically builds a model of the user interest and selects a new set of samples for user feedback in the next iteration, until the user interest model con-verges. As pointed out in a recent study, a large number of samples must be labeled by the user before the DBMS can obtain the first positive sample for each user interest area. Since the DBMS does not have a priori knowledge of the user interest area, how to select the initial sets of samples to maximize the chance of having at least one positive sample in each user interest area is a hard problem in interactive data exploration. In this paper, we examine a number of sampling tech-niques used for interactive data exploration, including equi-width and equi-depth stratified sampling and their progres-sive sampling versions. We derive rigorous probability lower bounds of these sampling techniques to enable a theoretical comparison. We further compare them empirically to show that the tradeoffs captured by our probability lower bounds reflect well the tradeoffs actually observed in DBMS execu-tion of these techniques, hence demonstrating the practical value of our theoretical results.},
	number = {ja},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Liu, Wenzhao and Diao, Yanlei and Liu, Anna},
	year = {2017},
	note = {Publisher: Taylor \& Francis},
	keywords = {cluster:Automatic Exploration, layer:User Interaction, supercluster:Exploration Interfaces, type:Evaluation Research},
	pages = {26},
}

@article{Akande201553,
	title = {Towards an efficient storage and retrieval mechanism for large unstructured grids},
	volume = {45},
	issn = {0167-739X},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X14002180},
	doi = {https://doi.org/10.1016/j.future.2014.10.024},
	abstract = {The size of spatial scientific datasets is steadily increasing due to improvements in instruments and availability of computational resources. However, much of the research on efficient storage and access to spatial datasets has focused on large multidimensional arrays. In contrast, unstructured grids consisting of collections of simplices (e.g. triangles or tetrahedra) present special challenges that have received less attention. Data values found at the vertices of the simplices may be dispersed throughout a datafile, producing especially poor disk locality. Our previous work has focused on addressing this locality problem. In this paper, we reorganize the unstructured grid to improve locality of disk access by maintaining the spatial neighborhood relationships inherent in the unstructured grid. This reorganization produces significant gains in performance by reducing the number of accesses made to the data file. We also examine the effects of different chunking configurations on data retrieval performance. A major motivation for reorganizing the unstructured grid is to allow the application of iteration aware prefetching. Applying this prefetching method to unstructured grids produces further performance gains over and above the gains seen from reorganization alone. The work presented in this journal contains at least 40\% new material not included in our conference paper (Akande and Rhodes 2013).},
	journal = {Future Generation Computer Systems},
	author = {Akande, Oyindamola O and Rhodes, Philip J},
	year = {2015},
	keywords = {Prefetching, Scientific Computing, Spatial data, cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {53--69},
}

@inproceedings{Mozafari:2015:CPF:2723372.2749454,
	address = {New York, NY, USA},
	title = {{CliffGuard}: {A} {Principled} {Framework} for {Finding} {Robust} {Database} {Designs}},
	isbn = {978-1-4503-2758-9},
	url = {http://doi.acm.org/10.1145/2723372.2749454},
	doi = {10.1145/2723372.2749454},
	abstract = {A fundamental problem in database systems is choosing the best physical design, i.e., a small set of auxiliary structures that enable the fastest execution of future queries. Almost all commercial databases come with designer tools that create a number of indices or materialized views (together comprising the physical design) that they exploit during query processing. Existing designers are what we call nominal; that is, they assume that their input parameters are precisely known and equal to some nominal values. For instance, since future workload is often not known a priori, it is common for these tools to optimize for past workloads in hopes that future queries and data will be similar. In practice, however, these parameters are often noisy or missing. Since nominal designers do not take the influence of such uncertainties into account, they find designs that are sub-optimal and remarkably brittle. Often, as soon as the future workload deviates from the past, their overall performance falls off a cliff, leading to customer discontent and expensive redesigns. Thus, we propose a new type of database designer that is robust against parameter uncertainties, so that overall performance degrades more gracefully when future workloads deviate from the past. Users express their risk tolerance by deciding on how much nominal optimality they are willing to trade for attaining their desired level of robustness against uncertain situations. To the best of our knowledge, this paper is the first to adopt the recent breakthroughs in the theory of robust optimization to build a practical framework for solving some of the most fundamental problems in databases, replacing today's brittle designs with a principled world of robust designs that can guarantee predictable and consistent performance.},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Mozafari, Barzan and Goh, Eugene Zhen Ye and Yoon, Dong Young},
	year = {2015},
	note = {Series Title: SIGMOD '15},
	keywords = {cluster:Flexible Engines, database performance, layer:Database Layer, physical design, robust optimization, supercluster:Indexes, type:Proposal of Solution},
	pages = {1167--1182},
}

@inproceedings{Braun:2015:AMH:2723372.2742783,
	address = {New York, NY, USA},
	title = {Analytics in {Motion}: {High} {Performance} {Event}-{Processing} {AND} {Real}-{Time} {Analytics} in the {Same} {Database}},
	isbn = {978-1-4503-2758-9},
	url = {http://doi.acm.org/10.1145/2723372.2742783},
	doi = {10.1145/2723372.2742783},
	abstract = {Modern data-centric flows in the telecommunications industry require real time analytical processing over a rapidly changing and large dataset. The traditional approach of separating OLTP and OLAP workloads cannot satisfy this requirement. Instead, a new class of integrated solutions for handling hybrid workloads is needed. This paper presents an industrial use case and a novel architecture that integrates key-value-based event processing and SQL-based analytical processing on the same distributed store while minimizing the total cost of ownership. Our approach combines several well-known techniques such as shared scans, delta processing, a PAX-fashioned storage layout, and an interleaving of scanning and delta merging in a completely new way. Performance experiments show that our system scales out linearly with the number of servers. For instance, our system sustains event streams of 100,000 events per second while simultaneously processing 100 ad-hoc analytical queries per second, using a cluster of 12 commodity servers. In doing so, our system meets all response time goals of our telecommunication customers; that is, 10 milliseconds per event and 100 milliseconds for an ad-hoc analytical query. Moreover, our system beats commercial competitors by a factor of 2.5 in analytical and two orders of magnitude in update performance.},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Braun, Lucas and Etter, Thomas and Gasparis, Georgios and Kaufmann, Martin and Kossmann, Donald and Widmer, Daniel and Avitzur, Aharon and Iliopoulos, Anthony and Levy, Eliezer and Liang, Ning},
	year = {2015},
	note = {Series Title: SIGMOD '15},
	keywords = {cluster:Adaptive Storage, event-processing, layer:Database Layer, oltp/olap engine, supercluster:Data Storage, type:Proposal of Solution},
	pages = {251--264},
}

@inproceedings{Kester:2017:APS:3035918.3064049,
	address = {New York, NY, USA},
	title = {Access {Path} {Selection} in {Main}-{Memory} {Optimized} {Data} {Systems}: {Should} {I} {Scan} or {Should} {I} {Probe}?},
	isbn = {978-1-4503-4197-4},
	url = {http://dl.acm.org/citation.cfm?doid=3035918.3064049},
	doi = {10.1145/3035918.3064049},
	abstract = {The advent of columnar data analytics engines fueled a series of optimizations on the scan operator. New designs include column-group storage, vectorized execution, shared scans, working directly over compressed data, and operating using SIMD and multi-core execution. Larger main memories and deeper cache hierarchies increase the efficiency of modern scans, prompting a revisit of the question of access path selection. In this paper, we compare modern sequential scans and secondary index scans. Through detailed analytical modeling and experimentation we show that while scans have become useful in more cases than before, both access paths are still useful, and so, access path selection (APS) is still required to achieve the best performance when considering variable workloads. We show how to perform access path selection. In particular, contrary to the way traditional systems choose between scans and secondary indexes, we find that in addition to the query selectivity, the underlying hardware, and the system design, modern optimizers also need to take into account query concurrency. We further discuss the implications of integrating access path selection in a modern analytical data system. We demonstrate, both theoretically and experimentally, that using the proposed model a system can quickly perform access path selection, outperforming solutions that rely on a single access path or traditional access path models. We outline a light-weight mechanism to integrate APS into main-memory analytical systems that does not interfere with low latency queries. We also use the APS model to explain how the division between sequential scan and secondary index scan has historically changed due to hardware and workload changes, which allows for future projections based on hardware advancements.},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Kester, Michael S. and Athanassoulis, Manos and Idreos, Stratos},
	year = {2017},
	note = {Series Title: SIGMOD '17
ISSN: 07308078},
	keywords = {cluster:Flexible Engines, cluster:Indexes, cost based optimization, layer:Database Layer, main memory system, supercluster:Indexes, type:Evaluation Research},
	pages = {715--730},
}

@article{Chernishev2017,
	title = {The design of an adaptive column-store system},
	volume = {4},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-017-0069-4},
	doi = {10.1186/s40537-017-0069-4},
	abstract = {A fully self-managed DBMS which does not require administrator intervention is the ultimate goal of database developers. This system should automate deploying, configuration, administration, monitoring, and tuning tasks. Although there are some advances in this field, self-managed technology is largely not ready for industrial use and remains an active area of research. One of the most crucial tasks for such a system is automated physical design tuning. A self-managed approach for this task implies that the physical design of a database should be automatically adapted to changing workloads. The problems of materialized view and index selection, data allocation, horizontal and vertical partitioning were studied for a long time, and hundreds of approaches were developed. However, most of these approaches were static, thus, unsuitable for self-managed systems. In this paper we discuss the prospects of an adaptive distributed relational column-store. We show that the column-store approach holds a great promise for construction of an efficient self-managed database. At first, we present a short survey of existing physical design studies and provide a classification of approaches. In the survey, we highlight the self-managed aspects. Then, we provide some views on the organization of a self-managed distributed column-store system. We discuss its three core components: an alerter, a reorganization controller and a set of physical design options (actions) available to such a system. We present possible approaches for each of these components and evaluate them. Several physical design problems are formulated and discussed. This study is the first step towards a creation of an adaptive distributed column-store system.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Chernishev, George},
	month = mar,
	year = {2017},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Evaluation Research},
	pages = {5},
}

@inproceedings{372516,
	address = {Bordeaux, France},
	title = {Designing {Access} {Methods}: {The} {RUM} {Conjecture}},
	isbn = {978-3-89318-070-7},
	doi = {10.5441/002/edbt.2016.42},
	abstract = {The database research community has been building methods to store, access, and update data for more than four decades. Throughout the evolution of the structures and techniques used to access data, access methods adapt to the ever changing hardware and workload requirements. Today, even small changes in the workload or the hardware lead to a redesign of access methods. The need for new designs has been increasing as data generation and workload diversification grow exponentially, and hardware advances introduce increased complexity. New workload requirements are introduced by the emergence of new applications, and data is managed by large systems composed of more and more complex and heterogeneous hardware. As a result, it is increasingly important to develop application-aware and hardware-aware access methods. The fundamental challenges that every researcher, systems architect, or designer faces when designing a new access method are how to minimize, i) read times (R), ii) update cost (U), and iii) memory (or storage) overhead (M). In this paper, we conjecture that when optimizing the read-update-memory overheads, optimizing in any two areas negatively impacts the third. We present a simple model of the RUM overheads, and we articulate the RUM Conjecture. We show how the RUM Conjecture manifests in state-of-the-art access methods, and we envision a trend toward RUM-aware access methods for future data systems.\&nbsp;},
	booktitle = {Under {Review}},
	author = {Athanassoulis, Manos and Kester, Michael S. and Maas, Lukas and Stoica, Radu and Idreos, Stratos and Ailamaki, Anastasia and Callaghan, Mark},
	year = {2015},
	keywords = {★},
	pages = {461--466},
}

@article{DBLP:journals/corr/SouleG14,
	title = {Optimized {Disk} {Layouts} for {Adaptive} {Storage} of {Interaction} {Graphs}},
	volume = {abs/1410.5},
	url = {http://arxiv.org/abs/1410.5290},
	abstract = {We are living in an ever more connected world, where data recording the interactions between people, software systems, and the physical world is becoming increasingly prevalent. This data often takes the form of a temporally evolving graph, where entities are the vertices and the interactions between them are the edges. We call such graphs interaction graphs. Various application domains, including telecommunications, transportation, and social media, depend on analytics performed on interaction graphs. The ability to efficiently support historical analysis over interaction graphs require effective solutions for the problem of data layout on disk. This paper presents an adaptive disk layout called the railway layout for optimizing disk block storage for interaction graphs. The key idea is to divide blocks into one or more sub-blocks, where each sub-block contains a subset of the attributes, but the entire graph structure is replicated within each sub-block. This improves query I/O, at the cost of increased storage overhead. We introduce optimal ILP formulations for partitioning disk blocks into sub-blocks with overlapping and non-overlapping attributes. Additionally, we present greedy heuristic approaches that can scale better compared to the ILP alternatives, yet achieve close to optimal query I/O. To demonstrate the benefits of the railway layout, we provide an extensive experimental study comparing our approach to a few baseline alternatives.},
	author = {Soulé, Robert and Gedik, Bügra},
	year = {2014},
	note = {arXiv: 1410.5290},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {1--18},
}

@inproceedings{Bugiotti2015a,
	title = {Toward {Scalable} {Hybrid} {Stores}},
	isbn = {978-1-5108-1087-7},
	abstract = {Data centric applications often use heterogeneous datasets: some very large while others of moderate size, some highly structured (e.g., relations) while others complex structured (e.g., graphs) or little structured (e.g., log data). Facing them is a variety of storage systems but none of which is the best for all, at all times. We present Estocada, an architecture we are currently developing to efficiently handle highly heterogeneous datasets based on a dynamic set of potentially very different data stores. Estocada provides to the ap- plication layer access to each dataset in its native format, while hosting them internally in a set of potentially overlapping fragments, possibly distributed across heterogeneous stores. At the core of Estocada lie powerful view-based rewriting and view selection algorithms to marry correctness with high performance.},
	booktitle = {{SEBD} {Italian} {Symposium} on {Advanced} {Database} {Systems}},
	author = {Bugiotti, Francesca and Bursztyn, Damian and Deutsch, Alin and Ileana, Ioana and Manolescu, Ioana},
	year = {2015},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes},
}

@inproceedings{Pavlo2017,
	title = {Self-{Driving} {Database} {Management} {Systems}},
	abstract = {In the last two decades, both researchers and vendors have built advisory tools to assist database administrators (DBAs) in various aspects of system tuning and physical design. Most of this previous work, however, is incomplete because they still require humans to make the final decisions about any changes to the database and are reactionary measures that fix problems after they occur. What is needed for a truly " self-driving " database management system (DBMS) is a new architecture that is designed for autonomous operation. This is different than earlier attempts because all aspects of the system are controlled by an integrated planning component that not only optimizes the system for the current workload, but also predicts future workload trends so that the system can prepare itself accordingly. With this, the DBMS can support all of the previous tuning techniques without requiring a human to determine the right way and proper time to deploy them. It also enables new optimiza-tions that are important for modern high-performance DBMSs, but which are not possible today because the complexity of managing these systems has surpassed the abilities of human experts. This paper presents the architecture of Peloton, the first self-driving DBMS. Peloton's autonomic capabilities are now possible due to algorithmic advancements in deep learning, as well as im-provements in hardware and adaptive database architectures.},
	booktitle = {{CIDR}'17},
	author = {Pavlo, Andrew and Angulo, Gustavo and Arulraj, Joy and Lin, Haibin and Lin, Jiexi and Ma, Lin and Menon, Prashanth and Mowry, Todd C and Perron, Matthew and Quah, Ian and Santurkar, Siddharth and Tomasic, Anthony and Toor, Skye and Aken, Dana Van and Wang, Ziqi and Wu, Yingjun and Xian, Ran and Zhang, Tieying},
	year = {2017},
	keywords = {cluster:Adaptive Indexing, cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, supercluster:Indexes, type:Proposal of Solution},
}

@article{DBLP:journals/corr/SadoghiBBC16,
	title = {L-{Store}: {A} {Real}-time {OLTP} and {OLAP} {System}},
	volume = {abs/1601.0},
	url = {http://arxiv.org/abs/1601.04084},
	abstract = {Arguably data is the new natural resource in the enterprise world with an unprecedented degree of proliferation. But to derive real-time actionable insights from the data, it is important to bridge the gap between managing the data that is being updated at a high velocity (i.e., OLTP) and analyzing a large volume of data (i.e., OLAP). However, there has been a divide where specialized solutions were often deployed to support either OLTP or OLAP workloads but not both; thus, limiting the analysis to stale and possibly irrelevant data. In this paper, we present Lineage-based Data Store (L-Store) that combines the real-time processing of transactional and analytical workloads within a single unified engine by introducing a novel lineage-based storage architecture. By exploiting the lineage, we develop a contention-free and lazy staging of columnar data from a write-optimized form (suitable for OLTP) into a read-optimized form (suitable for OLAP) in a transactionally consistent approach that also supports querying and retaining the current and historic data. Our working prototype of L-Store demonstrates its superiority compared to state-of-the-art approaches under a comprehensive experimental evaluation.},
	author = {Sadoghi, Mohammad and Bhattacherjee, Souvik and Bhattacharjee, Bishwaranjan and Canim, Mustafa},
	year = {2016},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
}

@article{Bugiotti2015,
	title = {Invisible {Glue} : {Scalable} {Self}-{Tuning} {Multi}-{Stores}},
	abstract = {Next-generation data centric applications often involve diverse datasets, some very large while others may be of moderate size, some highly structured (e.g., relations) while others may have more complex structure (e.g., graphs) or little structure (e.g., text or log data). Facing them is a variety of storage systems, each of which can host some of the datasets (possibly after some data migration), but none of which is likely to be best for all, at all times. Deploying and efficiently running data-centric applications in such a complex setting is very challenging. We propose Estocada, an architecture for efficiently han- dling highly heterogeneous datasets based on a dynamic set of potentially very different data stores. Estocada pro- vides to the application/programming layer access to each data set in its native format, while hosting them internally in a set of potentially overlapping fragments, possibly dis- tributing (fragments of) each dataset across heterogeneous stores. Given workload information, Estocada self-tunes for performance, i.e., it automatically choses the fragments of each data set to be deployed in each store so as to op- timize performance. At the core of Estocada lie powerful view-based rewriting and view selection algorithms, required in order to correctly handle the features (nesting, keys, con- straints etc.) of the diverse data models involved, and thus to marry correctness with high performance.},
	journal = {CIDR'15},
	author = {Bugiotti, Francesca and Bursztyn, Damian and Diego, U C San and Ileana, Ioana},
	year = {2015},
	keywords = {RRaw: Yes, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
}

@inproceedings{7930131,
	title = {Are {Databases} {Fit} for {Hybrid} {Workloads} on {GPUs}? {A} {Storage} {Engine}'s {Perspective}},
	url = {http://ieeexplore.ieee.org/document/7930131/},
	doi = {10.1109/ICDE.2017.237},
	abstract = {Employing special-purpose processors (e.g., GPUs) in database systems has been studied throughout the last decade. Research on heterogeneous database systems that use both general-and special-purpose processors has addressed either transaction-or analytic processing, but not the combination of them. Support for hybrid transaction-and analytic processing (HTAP) has been studied exclusively for CPU-only systems. In this paper we ask the question whether current systems are ready for HTAP workload management with cooperating general- and special-purpose processors. For this, we take the perspective of the backbone of database systems: the storage engine. We propose a unified terminology and a comprehensive taxonomy to compare state-of-the-art engines from both domains. We show similarities and differences, and determine a necessary set of features for engines supporting HTAP workload on CPUs and GPUs. Answering our research question, our findings yield a resolute: not yet.},
	booktitle = {2017 {IEEE} 33rd {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Pinnecke, M and Broneske, D and Durand, G C and Saake, G},
	month = apr,
	year = {2017},
	keywords = {distributed databases, graphics processing units, mi},
	pages = {1599--1606},
}

@inproceedings{Psaroudakis2015,
	address = {Cham},
	title = {Scaling {Up} {Mixed} {Workloads}: {A} {Battle} of {Data} {Freshness}, {Flexibility}, and {Scheduling}},
	isbn = {978-3-319-15350-6},
	url = {https://link.springer.com/chapter/10.1007%2F978-3-319-15350-6_7},
	doi = {10.1007/978-3-319-15350-6_7},
	abstract = {The common ``one size does not fit all'' paradigm isolates transactional and analytical workloads into separate, specialized database systems. Operational data is periodically replicated to a data warehouse for analytics. Competitiveness of enterprises today, however, depends on real-time reporting on operational data, necessitating an integration of transactional and analytical processing in a single database system. The mixed workload should be able to query and modify common data in a shared schema. The database needs to provide performance guarantees for transactional workloads, and, at the same time, efficiently evaluate complex analytical queries. In this paper, we share our analysis of the performance of two main-memory databases that support mixed workloads, SAP HANA and HyPer, while evaluating the mixed workload CH-benCHmark. By examining their similarities and differences, we identify the factors that affect performance while scaling the number of concurrent transactional and analytical clients. The three main factors are (a) data freshness, i.e., how recent is the data processed by analytical queries, (b) flexibility, i.e., restricting transactional features in order to increase optimization choices and enhance performance, and (c) scheduling, i.e., how the mixed workload utilizes resources. Specifically for scheduling, we show that the absence of workload management under cases of high concurrency leads to analytical workloads overwhelming the system and severely hurting the performance of transactional workloads.},
	booktitle = {Performance {Characterization} and {Benchmarking}. {Traditional} to {Big} {Data}},
	publisher = {Springer International Publishing},
	author = {Psaroudakis, Iraklis and Wolf, Florian and May, Norman and Neumann, Thomas and Böhm, Alexander and Ailamaki, Anastasia and Sattler, Kai-Uwe},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	year = {2015},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Evaluation Research},
	pages = {97--112},
}

@article{Soule2016,
	title = {{RailwayDB}: adaptive storage of interaction graphs},
	volume = {25},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-015-0407-0},
	doi = {10.1007/s00778-015-0407-0},
	abstract = {We are living in an ever more connected world, where data recording the interactions between people, software systems, and the physical world is becoming increasingly prevalent. These data often take the form of a temporally evolving graph, where entities are the vertices and the interactions between them are the edges. We call such graphs interaction graphs. Various domains, including telecommunications, transportation, and social media, depend on analytics performed on interaction graphs. The ability to efficiently support historical analysis over interaction graphs requires effective solutions for the problem of data layout on disk. This paper presents an adaptive disk layout called the railway layout for optimizing disk block storage for interaction graphs. The key idea is to divide blocks into one or more sub-blocks. Each sub-block contains the entire graph structure, but only a subset of the attributes. This improves query I/O, at the cost of increased storage overhead. We introduce optimal integer linear program (ILP) formulations for partitioning disk blocks into sub-blocks with overlapping and nonoverlapping attributes. Additionally, we present greedy heuristics that can scale better compared to the ILP alternatives, yet achieve close to optimal query I/O. We provide an implementation of the railway layout as part of RailwayDB---an open-source graph database we have developed. To demonstrate the benefits of the railway layout, we provide an extensive experimental evaluation, including model-based as well as empirical results comparing our approach to baseline alternatives.},
	number = {2},
	journal = {The VLDB Journal},
	author = {Soulé, Robert and Gedik, Bugra},
	month = apr,
	year = {2016},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {151--169},
}

@incollection{Basu2016,
	address = {Berlin, Heidelberg},
	title = {Regularized {Cost}-{Model} {Oblivious} {Database} {Tuning} with {Reinforcement} {Learning}},
	isbn = {978-3-662-53455-7},
	url = {https://link.springer.com/chapter/10.1007%2F978-3-662-53455-7_5},
	abstract = {In this paper, we propose a learning approach to adaptive performance tuning of database applications. The objective is to validate the opportunity to devise a tuning strategy that does not need prior knowledge of a cost model. Instead, the cost model is learned through reinforcement learning. We instantiate our approach to the use case of index tuning. We model the execution of queries and updates as a Markov decision process whose states are database configurations, actions are configuration changes, and rewards are functions of the cost of configuration change and query and update evaluation. During the reinforcement learning process, we face two important challenges: the unavailability of a cost model and the size of the state space. To address the former, we iteratively learn the cost model, in a principled manner, using regularization to avoid overfitting. To address the latter, we devise strategies to prune the state space, both in the general case and for the use case of index tuning. We empirically and comparatively evaluate our approach on a standard OLTP dataset. We show that our approach is competitive with state-of-the-art adaptive index tuning, which is dependent on a cost model.},
	booktitle = {Transactions on {Large}-{Scale} {Data}- and {Knowledge}-{Centered} {Systems} {XXVIII}: {Special} {Issue} on {Database}- and {Expert}-{Systems} {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Basu, Debabrota and Lin, Qian and Chen, Weidong and Vo, Hoang Tam and Yuan, Zihong and Senellart, Pierre and Bressan, Stéphane},
	editor = {Hameurlain, Abdelkader and Küng, Josef and Wagner, Roland and Chen, Qimin},
	year = {2016},
	doi = {10.1007/978-3-662-53455-7_5},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {96--132},
}

@inproceedings{Arulraj:2016:BAR:2882903.2915231,
	address = {New York, NY, USA},
	title = {Bridging the {Archipelago} between {Row}-{Stores} and {Column}-{Stores} for {Hybrid} {Workloads}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2915231},
	doi = {10.1145/2882903.2915231},
	abstract = {Data-intensive applications seek to obtain trill insights in real-time by analyzing a combination of historical data sets alongside recently collected data. This means that to support such hybrid workloads, database management systems (DBMSs) need to handle both fast ACID transactions and complex analytical queries on the same database. But the current trend is to use specialized systems that are optimized for only one of these workloads, and thus require an organization to maintain separate copies of the database. This adds additional cost to deploying a database application in terms of both storage and administration overhead. To overcome this barrier, we present a hybrid DBMS architecture that efficiently supports varied workloads on the same database. Our approach differs from previous methods in that we use a single execution engine that is oblivious to the storage layout of data without sacrificing the performance benefits of the specialized systems. This obviates the need to maintain separate copies of the database in multiple independent systems. We also present a technique to continuously evolve the database’s physical storage layout by analyzing the queries’ access patterns and choosing the optimal layout for different segments of data within the same table. To evaluate this work, we implemented our architecture in an in-memory DBMS. Our results show that our approach delivers up to 3×higher throughput compared to static storage layouts across different workloads. We also demonstrate that our continuous adaptation mechanism allows the DBMS to achieve a near-optimal layout for an arbitrary workload without requiring any manual tuning.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Arulraj, Joy and Pavlo, Andrew and Menon, Prashanth},
	year = {2016},
	note = {Series Title: SIGMOD '16
ISSN: 07308078},
	keywords = {HTAP, cluster:Flexible Engines, hybrid workloads, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {583--598},
}

@inproceedings{Bian:2017:WTL:3035918.3035930,
	address = {New York, NY, USA},
	title = {Wide {Table} {Layout} {Optimization} based on {Column} {Ordering} and {Duplication}},
	isbn = {978-1-4503-4197-4},
	url = {http://doi.acm.org/10.1145/3035918.3035930},
	doi = {10.1145/3035918.3035930},
	abstract = {Modern data analytical tasks often witness very wide tables, from a few hundred columns to a few thousand. While it is commonly agreed that column stores are an appropriate data format for wide tables and analytical workloads, the physical order of columns has not been investigated. Column ordering plays a critical role in I/O performance, because in wide tables accessing the columns in a single horizontal partition may involve multiple disk seeks. An optimal column ordering will incur minimal cumulative disk seek costs for the set of queries applied to the data. In this paper, we aim to find such an optimal column layout to maximize I/O performance. Specifically, we study two problems for column stores on HDFS: column ordering and column duplication. Column ordering seeks an approximately optimal order of columns; column duplication complements column ordering in that some columns may be duplicated multiple times to reduce contention among the queries' diverse requirements on the column order. We consider an actual fine-grained cost model for column accesses and propose algorithms that take a query workload as input and output a column ordering strategy with or without storage redundancy that significantly improves the overall I/O performance. Experimental results over real-life data and production query workloads confirm the effectiveness of the proposed algorithms in diverse settings.},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Bian, Haoqiong and Yan, Ying and Tao, Wenbo and Chen, Liang Jeff and Chen, Yueguo and Du, Xiaoyong and Moscibroda, Thomas},
	year = {2017},
	note = {Series Title: SIGMOD '17
ISSN: 07308078},
	keywords = {Data analysis, cluster:Adaptive Storage, data layout optimization, disk-based storage system, hdfs, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {299--314},
}

@article{Mishra2017,
	title = {Host managed contention avoidance storage solutions for {Big} {Data}},
	volume = {4},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-017-0080-9},
	doi = {10.1186/s40537-017-0080-9},
	abstract = {The performance gap between compute and storage is fairly considerable. This results in a mismatch between the application needs from storage and what storage can deliver. The full potential of storage devices cannot be harnessed till all layers of I/O hierarchy function efficiently. Despite advanced optimizations applied across various layers along the odyssey of data access, the I/O stack still remains volatile. The problems associated due to the inefficiencies in data management get amplified in Big Data shared resource environments. The Linux OS (host) block layer is the most critical part of the I/O hierarchy, as it orchestrates the I/O requests from different applications to the underlying storage. Unfortunately, despite it's significance, the block layer, essentially the block I/O scheduler, hasn't evolved to meet the needs of Big Data. We have designed and developed two contention avoidance storage solutions, collectively known as ``BID: Bulk I/O Dispatch'' in the Linux block layer specifically to suit multi-tenant, multi-tasking shared Big Data environments. Hard disk drives (HDDs) form the backbone of data center storage. The data access time in HDDs is majorly governed by disk arm movements, which usually occurs when data is not accessed sequentially. Big Data applications exhibit evident sequentiality but due to the contentions amongst other I/O submitting applications, the I/O accesses get multiplexed which leads to higher disk arm movements. BID schemes aim to exploit the inherent I/O sequentiality of Big Data applications to improve the overall I/O completion time by reducing the avoidable disk arm movements. In the first part, we propose a dynamically adaptable block I/O scheduling scheme BID-HDD for disk based storage. BID-HDD tries to recreate the sequentiality in I/O access in order to provide performance isolation to each I/O submitting process. Through trace driven simulation based experiments with cloud emulating MapReduce benchmarks, we show the effectiveness of BID-HDD which results in 28--52\% lesser time for all I/O requests than the best performing Linux disk schedulers. In the second part, we propose a hybrid scheme BID-Hybrid to exploit SCM's (SSDs) superior random performance to further avoid contentions at disk based storage. BID-Hybrid is able to efficiently offload non-bulky interruptions from HDD request queue to SSD queue using BID-HDD for disk request processing and multi-q FIFO architecture for SSD. This results in performance gain of 6--23\% for MapReduce workloads when compared to BID-HDD and 33--54\% over best performing Linux scheduling scheme. BID schemes as a whole is aimed to avoid contentions for disk based storage I/Os following system constraints without compromising SLAs.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Mishra, Pratik and Somani, Arun K},
	month = jun,
	year = {2017},
	keywords = {layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {18},
}

@inproceedings{Desmeurs2015,
	title = {Event-{Driven} {Application} {Brownout}: {Reconciling} {High} {Utilization} and {Low} {Tail} {Response} {Times}},
	isbn = {0-7695-5636-1},
	url = {http://ieeexplore.ieee.org/document/7312136/},
	doi = {10.1109/ICCAC.2015.25},
	abstract = {Data centers currently waste a lot of energy, due to lack of energy proportionality and low resource utilization, the latter currently being necessary to ensure application responsiveness. To address the second concern we propose a novel application-level technique that we call event-driven Brownout. For each request, i.e., in an event-driven manner, the application can execute some optional code that is not required for correct operation but desirable for user experience, and does so only if the number of pending client requests is below a given threshold. We propose several autonomic algorithms, based on control theory and machine learning, to automatically tune this threshold based on measured application 95th percentile response times. We evaluate our approach using the RUBiS benchmark which shows a 11-fold improvement in maintaining response-time close to a set-point at high utilization compared to competing approaches. Our contribution is opening the path to more energy efficient data-centers, by allowing applications to keep response times close to a set-point even at high resource utilization.},
	booktitle = {Proceedings - 2015 {International} {Conference} on {Cloud} and {Autonomic} {Computing}, {ICCAC} 2015},
	author = {Desmeurs, David and Klein, Cristian and Papadopoulos, Alessandro Vittorio and Tordsson, Johan},
	year = {2015},
	keywords = {event-driven control, graceful performance degradation, machine learning},
	pages = {1--12},
}

@inproceedings{Battle2016,
	address = {New York, NY, USA},
	title = {Dynamic {Prefetching} of {Data} {Tiles} for {Interactive} {Visualization}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882919},
	doi = {10.1145/2882903.2882919},
	abstract = {In this paper, we present ForeCache, a general-purpose tool for exploratory browsing of large datasets. ForeCache utilizes a client-server architecture, where the user interacts with a lightweight client-side interface to browse datasets, and the data to be browsed is re-trieved from a DBMS running on a back-end server. We assume a detail-on-demand browsing paradigm, and optimize the back-end support for this paradigm by inserting a separate middleware layer in front of the DBMS. To improve response times, the middleware layer fetches data ahead of the user as she explores a dataset. We consider two different mechanisms for prefetching: (a) learning what to fetch from the user's recent movements, and (b) using data characteristics (e.g., histograms) to find data similar to what the user has viewed in the past. We incorporate these mech-anisms into a single prediction engine that adjusts its prediction strategies over time, based on changes in the user's behavior. We evaluated our prediction engine with a user study, and found that our dynamic prefetching strategy provides: (1) significant improve-ments in overall latency when compared with non-prefetching sys-tems (430\% improvement); and (2) substantial improvements in both prediction accuracy (25\% improvement) and latency (88\% im-provement) relative to existing prefetching techniques.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Battle, Leilani and Chang, Remco and Stonebraker, Michael},
	year = {2016},
	note = {Series Title: SIGMOD '16
ISSN: 07308078},
	keywords = {RInteractive: Yes, UserStudy:yes, Visualization, cluster:Data Prefetching, data exploration, layer:Middleware, predictive caching, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1363--1375},
}

@inproceedings{Moritz2017,
	title = {Trust, but {Verify} : {Optimistic} {Visualizations} of {Approximate} {Queries} for {Exploring} {Big} {Data}},
	isbn = {978-1-4503-4655-9},
	url = {http://dl.acm.org/citation.cfm?doid=3025453.3025456},
	doi = {10.1145/3025453.3025456},
	abstract = {Analysts need interactive speed for exploratory analysis, but big data systems are often slow. With sampling, data systems can produce approximate answers fast enough for exploratory visualization, at the cost of accuracy and trust. We propose optimistic visualization, which approaches these issues from a user experience perspective. This method lets analysts explore approximate results interactively, and provides a way to detect and recover from errors later. Pangloss implements these ideas. We discuss design issues raised by optimistic visualization systems. We test this concept with five expert visualizers in a laboratory study and three case studies at Microsoft. Analysts reported that they felt more confident in their results, and used optimistic visualization to check that their preliminary results were correct.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Moritz, Dominik},
	year = {2017},
	keywords = {Data visualization, UserStudy:yes, cluster:Visual Optimizations, exploratory analysis, layer:User Interaction, optimistic visualization, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {2904--2915},
}

@inproceedings{WangYi2015,
	address = {New York, NY, USA},
	title = {Supporting online analytics with user-defined estimation and early termination in a {MapReduce}-like framework},
	isbn = {978-1-4503-3993-3},
	url = {http://dl.acm.org/citation.cfm?doid=2831244.2831247},
	doi = {10.1145/2831244.2831247},
	abstract = {Online analytics based on runtime approximation has been widely adopted for meeting time and/or resource constraints. Though MapReduce has been gaining its popularity in both scientific and commercial sectors, there are several obstacles in implementing online analytics in a MapReduce implementation. In this paper, we present a MapReduce-like framework for online analytics. Our system can process the input incrementally, provide fast estimates, and terminate the execution as soon as a user-defined termination state is reached. We have extended the MapReduce API by allowing the user to customize both the estimation method and termination condition. We also have shown both the functionality and efficiency of our system through three approximate applications. A comparison with a batch processing implementation shows a speedup of at least an order of magnitude.},
	booktitle = {Proceedings of the 2015 {International} {Workshop} on {Data}-{Intensive} {Scalable} {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Yi and Chen, Linchuan and Agrawal, Gagan},
	year = {2015},
	note = {Series Title: DISCS '15},
	keywords = {MapReduce, RDistributed: Yes, RInteractive: Yes, cluster:Query Approximation, early termination, layer:Middleware, online analytics, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1--8},
}

@inproceedings{Kelley2015,
	title = {Measuring and managing answer quality for online data-intensive services},
	isbn = {978-1-4673-6970-1},
	url = {http://ieeexplore.ieee.org/document/7266961/},
	doi = {10.1109/ICAC.2015.33},
	abstract = {Online data-intensive services parallelize query execution across distributed software components. Interactive response time is a priority, so online query executions return answers without waiting for slow running components to finish. However, data from these slow components could lead to better answers. We propose Ubora, an approach to measure the effect of slow running components on the quality of answers. Ubora randomly samples online queries and executes them twice. The first execution elides data from slow components and provides fast online answers, the second execution waits for all components to complete. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of platforms, including Hadoop/Yarn, Apache Lucene, the Easy Rec Recommendation Engine, and the Open Ephyra question answering system. Ubora computes answer quality much faster than competing approaches that do not use memoization. With Ubora, we show that answer quality can and should be used to guide online admission control. Our adaptive controller processed 37\% more queries than a competing controller guided by the rate of timeouts.},
	booktitle = {Proceedings - {IEEE} {International} {Conference} on {Autonomic} {Computing}, {ICAC} 2015},
	author = {Kelley, Jaimie and Stewart, Christopher and Morris, Nathaniel and Tiwari, Devesh and He, Yuxiong and Elnikety, Sameh},
	year = {2015},
	note = {arXiv: 1506.05172v1},
	keywords = {Data-intensive, Memoization, Online services},
	pages = {167--176},
}

@article{Zhao2017,
	title = {Controlling {False} {Discoveries} {During} {Interactive} {Data} {Exploration}},
	url = {http://arxiv.org/abs/1612.01040},
	doi = {10.1145/3035918.3064019},
	abstract = {Recent tools for interactive data exploration significantly increase the chance that users make false discoveries. The crux is that these tools implicitly allow the user to test a large body of different hypotheses with just a few clicks thus incurring in the issue commonly known in statistics as the multiple hypothesis testing error. In this paper, we propose solutions to integrate multiple hypothesis testing control into interactive data exploration tools. A key insight is that existing methods for controlling the false discovery rate (such as FDR) are not directly applicable for interactive data exploration. We therefore discuss a set of new control procedures that are better suited and integrated them in our system called Aware. By means of extensive experiments using both real-world and synthetic data sets we demonstrate how Aware can help experts and novice users alike to efficiently control false discoveries.},
	journal = {Proceedings of the 2017 ACM International Conference on Management of Data},
	author = {Zhao, Zheguang and De Stefani, Lorenzo and Zgraggen, Emanuel and Binnig, Carsten and Upfal, Eli and Kraska, Tim},
	year = {2017},
	note = {arXiv: 1612.01040
ISBN: 9781450341974},
	keywords = {Error Estimation, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {527--540},
}

@inproceedings{Kahng2016a,
	title = {Visual exploration of machine learning results using data cube analysis},
	isbn = {978-1-4503-4207-0},
	url = {http://dl.acm.org/citation.cfm?doid=2939502.2939503},
	doi = {10.1145/2939502.2939503},
	abstract = {As complex machine learning systems become more widely adopted, it becomes increasingly challenging for users to understand models or interpret the results generated from the models. We present our ongoing work on developing interactive and visual approaches for exploring and understanding machine learning results using data cube analysis. We propose MLCube, a data cube inspired framework that enables users to define instance subsets using feature conditions and computes aggregate statistics and evaluation metrics over the subsets. We also design MLCube Explorer, an interactive visualization tool for comparing models' performances over the subsets. Users can interactively specify operations, such as drilling down to specific instance subsets, to perform more in-depth exploration. Through a usage scenario, we demonstrate how MLCube Explorer works with a public advertisement click log data set, to help a user build new advertisement click prediction models that advance over an existing model.},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	author = {Kahng, Minsuk and Fang, Dezhi and Chau, Duen Horng (Polo)},
	year = {2016},
	keywords = {RInteractive: Yes, cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1--6},
}

@article{Pahins2017,
	title = {Hashedcubes: {Simple}, {Low} {Memory}, {Real}-{Time} {Visual} {Exploration} of {Big} {Data}},
	volume = {23},
	issn = {10772626},
	url = {http://ieeexplore.ieee.org/document/7539326/},
	doi = {10.1109/TVCG.2016.2598624},
	abstract = {We propose Hashedcubes, a data structure that enables real-time visual exploration of large datasets that improves the state of the art by virtue of its low memory requirements, low query latencies, and implementation simplicity. In some instances, Hashedcubes notably requires two orders of magnitude less space than recent data cube visualization proposals. In this paper, we describe the algorithms to build and query Hashedcubes, and how it can drive well-known interactive visualizations such as binned scatterplots, linked histograms and heatmaps. We report memory usage, build time and query latencies for a variety of synthetic and real-world datasets, and find that although sometimes Hashedcubes offers slightly slower querying times to the state of the art, the typical query is answered fast enough to easily sustain a interaction. In datasets with hundreds of millions of elements, only about 2\% of the queries take longer than 40ms. Finally, we discuss the limitations of data structure, potential spacetime tradeoffs, and future research directions.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Pahins, Cícero A.L. and Stephens, Sean A. and Scheidegger, Carlos and Comba, João L.D.},
	year = {2017},
	keywords = {RInteractive: Yes, Scalability, cluster:Spatial Query, data cube, interactive exploration, layer:Database Layer, multidimensional data, supercluster:Indexes, type:Proposal of Solution},
	pages = {671--680},
}

@inproceedings{Aligon2014,
	title = {A {Holistic} {Approach} to {OLAP} {Sessions} {Composition}},
	isbn = {978-1-4503-0999-8},
	url = {http://dl.acm.org/citation.cfm?doid=2666158.2666179},
	doi = {10.1145/2666158.2666179},
	abstract = {OLAP is the main paradigm for flexible and effective exploration of multidimensional cubes in data warehouses. During an OLAP session the user analyzes the results of a query and determines a new query that will give her a better understanding of information. Given the huge size of the data space, this exploration process is often tedious and may leave the user disoriented and frustrated. This paper presents an OLAP tool 1 named Falseto (Former AnalyticaL Sessions for lEss Tedious Olap), that is meant to assist query and session composition, by letting the user summarize, browse, query, and reuse former analytical sessions. Falseto's implementation on top of a formal framework is detailed. We also report the experiments we run to obtain and analyze real OLAP sessions and assess Falseto with them. Finally, we discuss how Falseto can be seen as a starting point for bridging OLAP with exploratory search, a search paradigm centered on the user and the evolution of her knowledge.},
	booktitle = {Proceedings of the 17th {International} {Workshop} on {Data} {Warehousing} and {OLAP} - {DOLAP} '14},
	author = {Aligon, Julien and Boulil, Kamal and Marcel, Patrick and Peralta, Veronika},
	year = {2014},
	keywords = {RInteractive: Yes, UserStudy:yes, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {37--46},
}

@misc{Wang2015a,
	title = {Data {Management} and {Data} {Processing} {Support} on {Array}-{Based} {Scientific} {Data}},
	url = {http://rave.ohiolink.edu/etdc/view?acc_num=osu1436157356},
	abstract = {Scientific simulations are now being performed at finer temporal and spatial scales, leading to an explosion of the output data (mostly in array-based formats), and challenges in effectively storing, managing, querying, disseminating, analyzing, and visualizing these datasets. Many paradigms and tools used today for large-scale scientific data management and data processing are often too heavy-weight and have inherent limitations, making it extremely hard to cope with the `big data’ challenges in a variety of scientific domains. Our overall goal is to provide high-performance data management and data processing support on array-based scientific data, targeting data-intensive applications and various scientific array storages. We believe that such high-performance support can significantly reduce the prohibitively expensive costs of data translation, data transfer, data ingestion, data integration, data processing, and data storage involved in many scientific applications, leading to better performance, ease-of-use, and responsiveness. On one hand, we have investigated four data management topics as follows. First, we built a light-weight data management layer over scientific datasets stored in HDF5 format, which is one of the popular array formats. Unlike many popular data transport protocols such as OPeNDAP, which requires costly data translation and data transfer before accessing remote data, our implementation can support server-side flexible subsetting and aggregation, with high parallel efficiency. Second, to avoid the high upfront data ingestion costs of loading large-scale array data into array databases like SciDB, we designed a system referred to as SAGA, which can provide database-like support over native array storage. Specifically, we focused on implementing a number of structural (grid, sliding,hierarchical, and circular) aggregations, which are unique in array data model. Third, we proposed a novel approximate aggregation approach over array data using bitmap indexing. This approach can operate on the compact bitmap indices rather than the original raw datasets, and can support fast, accurate and flexible aggregations over any array or its subset without data reorganization. Fourth, we extended bitmap indexing to assist the data mining task subgroup discovery over array data. Like the aggregation approach, our algorithm can operate entirely on bitmap indices, and it can efficiently handle a key challenge associated with array data – a subgroup identified over array data can be described by value-based and/or dimension-based attributes. On the other hand, we focused on both offline and in-situ data processing paradigms in the context of MapReduce. To process disk-resident scientific data in various data formats, we developed a customizable MapReduce-like framework, SciMATE, which can be adapted to support transparent processing on any of the scientific data formats. Thus, unnecessary data integration and data reloading incurred by applying traditional MapReduce paradigm to scientific data processing can be avoided. We then designed another MapReduce-like framework, Smart, to support efficient in-situ scientific analytics in both time sharing and space sharing modes. In contrast to offline processing, our implementation can avoid, either completely or to a very large extent, both data transfer and data storage costs.},
	urldate = {2017-09-29},
	author = {Wang, Yi},
	year = {2015},
	keywords = {Scientific Computing, data management, data processing},
}

@article{Zoumpatianos2015,
	title = {Query {Workloads} for {Data} {Series} {Indexes}},
	url = {http://dl.acm.org/citation.cfm?doid=2783258.2783382},
	doi = {10.1145/2783258.2783382},
	abstract = {Data series are a prevalent data type that has attracted lots of interest in recent years. Most of the research has focused on how to e support similarity or nearest neighbor queries over large data series collections (an important data mining task), and several data series summarization and in-dexing methods have been proposed in order to solve this problem. Nevertheless, up to this point very little atten-tion has been paid to properly evaluating such index struc-tures, with most previous work relying solely on randomly selected data series to use as queries (with/without adding noise). In this work, we show that random workloads are inherently not suitable for the task at hand and we argue that there is a need for carefully generating a query work-load. We define measures that capture the characteristics of queries, and we propose a method for generating workloads with the desired properties, that is, effectively evaluating and comparing data series summarizations and indexes. In our experimental evaluation, with carefully controlled query workloads, we shed light on key factors affecting the performance of nearest neighbor search in large data series collec-tions.},
	journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Zoumpatianos, Kostas},
	year = {2015},
	note = {ISBN: 9781450336642},
	keywords = {cluster:Indexes, data series, indexing, layer:Database Layer, similarity search, supercluster:Indexes, type:Evaluation Research},
	pages = {1603--1612},
}

@inproceedings{Shaikh2016,
	title = {Efficient index computation for array based structured data},
	isbn = {978-1-4673-9257-0},
	url = {http://ieeexplore.ieee.org/document/7391930/},
	doi = {10.1109/EICT.2015.7391930},
	abstract = {Traditional computing techniques widely vary over large scale computing. Some techniques are useful for structured data while some other techniques are useful for non-structured data. To store and operation on the structured data, multidimensional array is widely used as basic data structure. But the performance of multidimensional array is very poor when the number of dimension is very high i.e. 32-dimension. Data scientist suggests linearization of dimensions for higher dimensional data. But linearized data are highly compute intensive for retrieving into original data format as well as operations are very expensive on that data. It suffers from the higher index computational cost and lower data locality. In this paper, we propose an index computation algorithm based on generalized two-dimensional structure. We showed that the accessing cost of array elements is higher for traditional multidimensional array over generalized two-dimensional representation. Though the most sequential access of main memory is itself very simple for any compiler yet experimental results showed a better performance. Both theoretical analysis and experiment are done to show its' effectiveness of the proposed algorithm.},
	booktitle = {2nd {International} {Conference} on {Electrical} {Information} and {Communication} {Technologies}, {EICT} 2015},
	author = {Shaikh, Md Abu Hanif and Omar, Mehnuma Tabassum and Azharul Hasan, K. M.},
	year = {2016},
	keywords = {Data locality, HPC, Higher dimensional array, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {101--105},
}

@inproceedings{Doraiswamy2016,
	title = {A {GPU}-based index to support interactive spatio-temporal queries over historical data},
	isbn = {978-1-5090-2019-5},
	url = {http://ieeexplore.ieee.org/document/7498315/},
	doi = {10.1109/ICDE.2016.7498315},
	abstract = {—There are increasing volumes of spatio-temporal data from various sources such as sensors, social networks and urban environments. Analysis of such data requires flexible exploration and visualizations, but queries that span multiple geographical regions over multiple time slices are expensive to compute, making it challenging to attain interactive speeds for large data sets. In this paper, we propose a new indexing scheme that makes use of modern GPUs to efficiently support spatio-temporal queries over point data. The index covers multiple dimensions, thus allowing simultaneous filtering of spatial and temporal attributes. It uses a block-based storage structure to speed up OLAP-type queries over historical data, and supports query processing over in-memory and disk-resident data. We present different query execution algorithms that we designed to allow the index to be used in different hardware configurations, including CPU-only, GPU-only, and a combination of CPU and GPU. To demonstrate the effectiveness of our techniques, we implemented them on top of MongoDB and performed an experimental evaluation using two real-world data sets: New York City's (NYC) taxi data – consisting of over 868 million taxi trips spanning a period of five years, and Twitter posts – over 1.1 billion tweets collected over a period of 14 months. Our results show that our GPU-based index obtains interactive, sub-second response times for queries over large data sets and leads to at least two orders of magnitude speedup over spatial indexes implemented in existing open-source and commercial database systems.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering}, {ICDE} 2016},
	author = {Doraiswamy, Harish and Vo, Huy T. and Silva, Claudio T. and Freire, Juliana},
	year = {2016},
	keywords = {RInteractive: Yes, cluster:Spatial Query, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1086--1097},
}

@article{Hong2016a,
	title = {{AQUAdexIM}: highly efficient in-memory indexing and querying of astronomy time series images},
	volume = {42},
	issn = {15729508},
	url = {https://link.springer.com/article/10.1007/s10686-016-9515-0},
	doi = {10.1007/s10686-016-9515-0},
	abstract = {Astronomy has always been, and will continue to be, a data-based science, and astronomers nowadays are faced with increasingly massive datasets, one key problem of which is to efficiently retrieve the desired cup of data from the ocean. AQUAdexIM, an innovative spatial indexing and querying method, performs highly efficient on-the-fly queries under users' request to search for Time Series Images from existing observation data on the server side and only return the desired FITS images to users, so users no longer need to download entire datasets to their local machines, which will only become more and more impractical as the data size keeps increasing. Moreover, AQUAdexIM manages to keep a very low storage space overhead and its specially designed in-memory index structure enables it to search for Time Series Images of a given area of the sky 10 times faster than using Redis, a state-of-the-art in-memory database.},
	number = {3},
	journal = {Experimental Astronomy},
	author = {Hong, Zhi and Yu, Ce and Wang, Jie and Xiao, Jian and Cui, Chenzhou and Sun, Jizhou},
	year = {2016},
	keywords = {Astronomy, FITS file, In-memory database, Pseudo-sphere index, Time series images, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {387--405},
}

@inproceedings{Wu2015a,
	title = {{PABIRS}: {A} data access middleware for distributed file systems},
	volume = {2015-May},
	isbn = {978-1-4799-7963-9},
	url = {http://ieeexplore.ieee.org/document/7113277/},
	doi = {10.1109/ICDE.2015.7113277},
	abstract = {Various big data management systems have emerged to handle different types of applications, which cast very different demands on storage, indexing and retrieval of large amount of data on distributed file system. Such diversity on demands has raised huge challenges to the design of new generation of data access service for big data. In this paper, we present PABIRS, a unified data access middleware to support mixed workloads. PABIRS encapsulates the underlying distributed file system (DFS) and provides a unified access interface to systems such as MapReduce and key-value stores. PABIRS achieves dramatic improvement on efficiency by employing a novel hybrid indexing scheme. Based on the data distribution, the indexing scheme adaptively builds bitmap index and Log Structured Merge Tree (LSM) index. Moreover, PABIRS distributes the computation to multiple index nodes and utilizes a Pregel-based algorithm to facilitate parallel data search and retrieval. We empirically evaluate PABIRS against other existing distributed data processing systems and verify the huge advantages of PABIRS on shorter response time, higher throughput and better scalability, over big data with real-life phone logs and TPC-H benchmark.},
	booktitle = {2015 {IEEE} 31st {International} {Conference} on {Data} {Engineering}},
	author = {Wu, Sai and Chen, Gang and Zhou, Xianke and Zhang, Zhenjie and Tung, Anthony K.H. and Winslett, Marianne},
	year = {2015},
	note = {ISSN: 10844627},
	keywords = {RDistributed: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {113--124},
}

@inproceedings{Wasay2017,
	address = {New York, NY, USA},
	title = {Data {Canopy}: {Accelerating} {Exploratory} {Statistical} {Analysis}},
	isbn = {978-1-4503-4197-4},
	url = {http://dl.acm.org/citation.cfm?doid=3035918.3064051},
	doi = {10.1145/3035918.3064051},
	abstract = {During exploratory statistical analysis, data scientists repeatedly compute statistics on data sets to infer knowledge. Moreover, statistics form the building blocks of core machine learning classification and filtering algorithms. Modern data systems, software libraries, and domain-specific tools provide support to compute statistics but lack a cohesive framework for storing, organizing, and reusing them. This creates a significant problem for exploratory statistical analysis as data grows: Despite existing overlap in exploratory workloads (which are repetitive in nature), statistics are always computed from scratch. This leads to repeated data movement and recomputation, hindering interactive data exploration. We address this challenge in Data Canopy, where descriptive and dependence statistics are synthesized from a library of basic aggregates. These basic aggregates are stored within an in-memory data structure, and and are reused for overlapping data parts and for various statistical measures. What this means for exploratory statistical analysis is that repeated requests to compute different statistics do not trigger a full pass over the data. We discuss in detail the basic design elements in Data Canopy, which address multiple challenges: (1) How to decompose statistics into basic aggregates for maximal reuse? (2) How to represent, store, maintain, and access these basic aggregates? (3) Under different scenarios, which basic aggregates to maintain? (4) How to tune Data Canopy in a hardware conscious way for maximum performance and how to maintain good performance as data grows and memory pressure increases? We demonstrate experimentally that Data Canopy results in an average speed-up of at least 10x after just 100 exploratory queries when compared with state-of-the-art systems used for exploratory statistical analysis.},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Wasay, Abdul and Wei, Xinding and Dayan, Niv and Idreos, Stratos},
	year = {2017},
	note = {Series Title: SIGMOD '17},
	keywords = {cluster:Data Prefetching, data exploration, data science, layer:Middleware, machine learning, statistical analysis, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {557--572},
}

@inproceedings{Yasunaga:2016:MST:2851613.2852002,
	address = {New York, NY, USA},
	title = {{MorphingAssist}: {Seamless} {Transformation} from {Keywords} to {Structured} {Queries}},
	isbn = {978-1-4503-3739-7},
	url = {http://doi.acm.org/10.1145/2851613.2852002},
	doi = {10.1145/2851613.2852002},
	abstract = {There is an increasing number of web data that consists of text and structured data, such as the combination of Wikipedia pages and DBpedia data. To issue queries to such data, we must choose one of the following: (1) submit keyword queries against textual data or (2) submit structured queries written in structured query languages such as SQL and SPARQL against structured data. This paper proposes MorphingAssist, a tool to assist users to gradually transform keywords into structured queries. A feature of our method is to use a hybrid query language that allows a variety of mixtures of keyword and structured query components in a query so that every intermediate step of transformation produces meaningful queries. However, assisting users to transform queries in such a way is a challenge. This paper explains how MorphingAssist generates and shows such hints to the user to facilitate his/her transforming simple keyword queries into more precise queries.},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Yasunaga, Yui and Morishima, Atsuyuki},
	year = {2016},
	note = {Series Title: SAC '16},
	keywords = {query languages},
	pages = {808--811},
}

@inproceedings{Qarabaqi2016,
	title = {Merlin: {Exploratory} {Analysis} with {Imprecise} {Queries}},
	volume = {28},
	url = {http://ieeexplore.ieee.org/document/7312990/},
	doi = {10.1109/TKDE.2015.2496270},
	abstract = {Merlin supports exploratory search in large databases. The user interacts with it by specifying probability distributions over attributes, which express imprecise conditions about the entities of interest. Merlin helps the user home in on the right query conditions by addressing three key challenges: (1) efficiently computing results for an imprecise query, (2) providing feedback about the sensitivity of the result to changes of individual conditions, and (3) suggesting new conditions. We formally introduce the notion of sensitivity and prove structural properties that enable efficient algorithms for quantifying the effect of uncertainty in user-specified conditions. To support interactive responses, we also develop techniques that can deliver probability estimates within a given realtime limit and are able to adapt automatically as interactive query refinement proceeds.},
	booktitle = {{IEEE} {Transactions} on {Knowledge} and {Data} {Engineering}},
	author = {Qarabaqi, Bahar and Riedewald, Mirek},
	year = {2016},
	note = {Issue: 2
ISSN: 10414347},
	keywords = {Interactive data exploration and discovery, RInteractive: Yes, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {342--355},
}

@inproceedings{Ge2016,
	title = {{REQUEST}: {A} scalable framework for interactive construction of exploratory queries},
	isbn = {978-1-4673-9004-0},
	url = {http://ieeexplore.ieee.org/abstract/document/7840657/},
	doi = {10.1109/BigData.2016.7840657},
	abstract = {Exploration over large datasets is a key first step in data analysis, as users may be unfamiliar with the underlying database schema and unable to construct precise queries that represent their interests. Such data exploration task usually involves executing numerous ad-hoc queries, which requires a considerable amount of time and human effort. In this paper, we present REQUEST, a novel framework that is designed to minimize the human effort and enable both effective and efficient data exploration. REQUEST supports the query-from-examples style of data exploration by integrating two key components: 1) Data Reduction, and 2) Query Selection. As instances of the REQUEST framework, we propose several highly scalable schemes, which employ active learning techniques and provide different levels of efficiency and effectiveness as guided by the user's preferences. Our results, on real-world datasets from Sloan Digital Sky Survey, show that our schemes on average require 1-2 orders of magnitude fewer feedback questions than the random baseline, and 3-16× fewer questions than the state-of-the-art, while maintaining interactive response time. Moreover, our schemes are able to construct, with high accuracy, queries that are often undetectable by current techniques.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Ge, Xiaoyu and Xue, Yanbing and Luo, Zhipeng and Sharaf, Mohamed A. and Chrysanthis, Panos K.},
	year = {2016},
	keywords = {RInteractive: Yes, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {646--655},
}

@inproceedings{Vasilyeva2016,
	title = {Why-query support in graph databases},
	isbn = {978-1-5090-2108-6},
	doi = {10.1109/ICDEW.2016.7495652},
	abstract = {Graph databases implementing a property graph model allow storing of heterogeneous information in the form of a graph and support complex graph-specific queries like shortest path, pattern matching, etc. Their flexibility and a rich spectrum of supported queries make it difficult for a user to create correct queries. As a consequence, a user can get unexpected results like too many, too few, or even empty answers. This research aims at providing a basic debugging functionality to a user in order to discover the reasons of a failure and to fix a query. The main goals of this thesis include (1) studying the reasons of a failure in terms of a graph with the focus on cardinality-based problems like too few, too many, and empty results; (2) developing methods for query refinement in order to derive expected answers with considering specifics of a property graph model, and (3) proposing a set of strategies for integrating user intention into the debugging process.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} {Workshops}, {ICDEW} 2016},
	author = {Vasilyeva, Elena},
	year = {2016},
	pages = {221--225},
}

@article{Mottin2016,
	title = {Exemplar queries: a new way of searching},
	volume = {25},
	issn = {0949877X},
	url = {https://link.springer.com/article/10.1007/s00778-016-0429-2},
	doi = {10.1007/s00778-016-0429-2},
	abstract = {Modern search engines employ advanced techniques that go beyond the structures that strictly satisfy the query conditions in an effort to better capture the user intentions. In this work, we introduce a novel query paradigm that considers a user query as an example of the data in which the user is interested. We call these queries exemplar queries. We provide a formal specification of their semantics and show that they are fundamentally different from notions like queries by example, approximate queries and related queries. We provide an implementation of these semantics for knowledge graphs and present an exact solution with a number of optimizations that improve performance without compromising the result quality. We study two different congruence relations, isomorphism and strong simulation, for identifying the answers to an exemplar query. We also provide an approximate solution that prunes the search space and achieves considerably better time performance with minimal or no impact on effectiveness. The effectiveness and efficiency of these solutions with synthetic and real datasets are experimentally evaluated, and the importance of exemplar queries in practice is illustrated.},
	number = {6},
	journal = {The VLDB Journal},
	author = {Mottin, Davide and Lissandrini, Matteo and Velegrakis, Yannis and Palpanas, Themis},
	year = {2016},
	keywords = {Exemplar query, Knowledge base, Knowledge graph, Query answering, cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {741--765},
}

@inproceedings{7498344,
	title = {{QPlain}: {Query} by explanation},
	url = {http://ieeexplore.ieee.org/document/7498344/?section=abstract},
	doi = {10.1109/ICDE.2016.7498344},
	abstract = {To assist non-specialists in formulating database queries, multiple frameworks that automatically infer queries from a set of input and output examples have been proposed. While highly useful, a shortcoming of the approach is that if users can only provide a small set of examples, many inherently different queries may qualify. We observe that additional information about the examples, in the form of their explanations, is useful in significantly focusing the set of qualifying queries. We propose to demonstrate QPlain, a system that learns conjunctive queries from examples and their explanations. We capture explanations of different levels of granularity and detail, by leveraging recently developed models for data provenance. Explanations are fed through an intuitive interface, are compiled to the appropriate provenance model, and are then used to derive proposed queries. We will demonstrate that it is feasible for non-specialists to provide examples with meaningful explanations, and that the presence of such explanations result in a much more focused set of queries which better match user intentions.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Deutch, D and Gilad, A},
	month = may,
	year = {2016},
	keywords = {cluster:Automatic Exploration, data handling, inference mechanisms, layer:User Interaction, query languages, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1358--1361},
}

@inproceedings{Mayer2015,
	title = {User {Interaction} {Models} for {Disambiguation} in {Programming} by {Example}},
	isbn = {978-1-4503-3779-3},
	url = {http://dl.acm.org/citation.cfm?doid=2807442.2807459},
	doi = {10.1145/2807442.2807459},
	abstract = {Programming by Examples (PBE) has the potential to revolutionize end-user programming by enabling end users, most of whom are non-programmers, to create small scripts for automating repetitive tasks. However, examples, though often easy to provide, are an ambiguous specification of the user's intent. Because of that, a key impedance in adoption of PBE systems is the lack of user confidence in the correctness of the program that was synthesized by the system. We present two novel user interaction models that communicate actionable information to the user to help resolve ambiguity in the examples. One of these models allows the user to effectively navigate between the huge set of programs that are consistent with the examples provided by the user. The other model uses active learning to ask directed example-based questions to the user on the test input data over which the user intends to run the synthesized program. Our user studies show that each of these models significantly reduces the number of errors in the performed task without any difference in completion time. Moreover, both models are perceived as useful, and the proactive active-learning based model has a slightly higher preference regarding the users' confidence in the result.},
	booktitle = {Proceedings of the 28th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} \& {Technology}},
	author = {Mayer, Mikaël and Soares, Gustavo and Grechkin, Maxim and Le, Vu and Marron, Mark and Polozov, Oleksandr and Singh, Rishabh and Zorn, Benjamin and Gulwani, Sumit},
	year = {2015},
	keywords = {UserStudy:yes, cluster:Assisted Query Formulation, flashprog, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {291--301},
}

@inproceedings{Abedjan2016,
	title = {{DataXFormer}: {A} robust transformation discovery system},
	isbn = {978-1-5090-2019-5},
	url = {http://ieeexplore.ieee.org/document/7498319/},
	doi = {10.1109/ICDE.2016.7498319},
	abstract = {In data integration, data curation, and other data analysis tasks, users spend a considerable amount of time converting data from one representation to another. For example US dates to European dates or airport codes to city names. In a previous vision paper, we presented the initial design of DataXFormer, a system that uses web resources to assist in transformation discovery. Specifically, DataXFormer discovers possible transformations from web tables and web forms and involves human feedback where appropriate. In this paper, we present the full fledged system along with several extensions. In particular, we present algorithms to find (i) transformations that entail multiple columns of input data, (ii) indirect transformations that are compositions of other transformations, (iii) transfor-mations that are not functions but rather relationships, and (iv) transformations from a knowledge base of public data. We report on experiments with a collection of 120 transformation tasks, and show our enhanced system automatically covers 101 of them by using openly available resources.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering}, {ICDE} 2016},
	author = {Abedjan, Ziawasch and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Stonebraker, Michael},
	year = {2016},
	pages = {1134--1145},
}

@inproceedings{6916939,
	title = {{ORange}: {Objective}-{Aware} {Range} {Query} {Refinement}},
	volume = {1},
	url = {http://ieeexplore.ieee.org/document/6916939/},
	doi = {10.1109/MDM.2014.48},
	abstract = {In this demo paper we present Orange, a system prototype for objective-aware range query refinement. Orange essentially refines a range query to meet a pre-specified cardinality constraint while taking into account the (dis)similarity between the initial query and its corresponding refined version. To achieve this goal, Orange employes the novel scheme SAQR for efficient similarity-aware query refinement. The main idea underlying SAQR is to utilize the pre-defined constraints on cardinality and similarity in order to bound the search space and quickly find a refined query, which meets the user's expectations. We showcase Orange in a web-based application which aims to guide planners in allocating service zones for police patrol units using real and historical dataset of crime incidents.},
	booktitle = {2014 {IEEE} 15th {International} {Conference} on {Mobile} {Data} {Management}},
	author = {Albarrak, A and Noboa, T and Khan, H A and Sharaf, M A and Zhou, X and Sadiq, S},
	month = jul,
	year = {2014},
	note = {ISSN: 1551-6245},
	keywords = {Internet, ORange, RInteractive: Yes, SAQR, cluster:Query Approximation, layer:Middleware, query processing, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {333--336},
}

@article{Khan2017,
	title = {Model-{Based} {Diversification} for {Sequential} {Exploratory} {Queries}},
	volume = {2},
	issn = {2364-1541},
	url = {https://link.springer.com/article/10.1007%2Fs41019-017-0038-0},
	doi = {10.1007/s41019-017-0038-0},
	abstract = {Today, data exploration platforms are widely used to assist users in locating interesting objects within large volumes of scientific and business data. In those platforms, users try to make sense of the underlying data space by iteratively posing numerous queries over large databases. While diversification of query results, like other data summarization techniques, provides users with quick insights into the huge query answer space, it adds additional complexity to an already computationally expensive data exploration task. To address this challenge, in this paper we propose a diversification scheme that targets the problem of efficiently diversifying the results of multiple queries within and across different data exploratory sessions. Our proposed scheme relies on a model-based diversification method and an ordered cache. In particular, we employ an adaptive regression model to estimate the diversity of a diverse subset. Such estimation of diversity value allows us to select diverse results without scanning all the query results. In order to further expedite the diversification process, we propose an order-based caching scheme to leverage the overlap between sequence of data exploration queries. Our extensive experimental evaluation on both synthetic and real data sets shows the significant benefits provided by our scheme as compared to the existing methods.},
	number = {2},
	journal = {Data Science and Engineering},
	author = {Khan, Hina A and Sharaf, Mohamed A},
	month = jun,
	year = {2017},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {151--168},
}

@inproceedings{Khan2015,
	title = {Progressive diversification for column-based data exploration platforms},
	volume = {2015-May},
	isbn = {978-1-4799-7963-9},
	url = {http://ieeexplore.ieee.org/document/7113295/},
	doi = {10.1109/ICDE.2015.7113295},
	abstract = {In Data Exploration platforms, diversification has become an essential method for extracting representative data, which provide users with a concise and meaningful view of the results to their queries. However, the benefits of diversification are achieved at the expense of an additional cost for the post-processing of query results. For high dimensional large result sets, the cost of diversification is further escalated due to massive distance computations required to evaluate the similarity between results. To address that challenge, in this paper we propose the Progressive Data Diversification (pDiverse) scheme. The main idea underlying pDiverse is to utilize partial distance computation to reduce the amount of processed data. Our extensive experimental results on both synthetic and real data sets show that our proposed scheme outperforms existing diversification methods in terms of both I/O and CPU costs.},
	booktitle = {2015 {IEEE} 31st {International} {Conference} on {Data} {Engineering}},
	author = {Khan, Hina A. and Sharaf, Mohamed A.},
	year = {2015},
	note = {ISSN: 10844627},
	keywords = {cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {327--338},
}

@inproceedings{Hussain:2015:DFR:2795218.2795225,
	address = {New York, NY, USA},
	title = {Diversifying with {Few} {Regrets}, {But} too {Few} to {Mention}},
	isbn = {978-1-4503-3740-3},
	url = {http://doi.acm.org/10.1145/2795218.2795225},
	doi = {10.1145/2795218.2795225},
	abstract = {Representative data provide users with a concise overview of their potentially large query results. Recently, diversity maximization has been adopted as one technique to generate representative data with high coverage and low redundancy. Orthogonally, regret minimization has emerged as another technique to generate representative data with high utility that satisfy the user's preference. In reality, however, users typically have some pre-specified preferences over some dimensions of the data, while expecting good coverage over the other dimensions. Motivated by that need, in this work we propose a novel scheme called ReDi, which aims to generate representative data that balance the tradeoff between regret minimization and diversity maximization. ReDi is based on a hybrid objective function that combines both regret and diversity. Additionally, it employs several algorithms that are designed to maximize that objective function. We perform extensive experimental evaluation to measure the tradeoff between the effectiveness and efficiency provided by the different ReDi algorithms.},
	booktitle = {Proceedings of the {Second} {International} {Workshop} on {Exploratory} {Search} in {Databases} and the {Web}},
	publisher = {ACM},
	author = {Hussain, Zaeem and Khan, Hina A and Sharaf, Mohamed A},
	year = {2015},
	note = {Series Title: ExploreDB '15},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {27--32},
}

@inproceedings{Djedaini2017,
	title = {Benchmarking exploratory {OLAP}},
	volume = {10080 LNCS},
	isbn = {978-3-319-54333-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-54334-5_5},
	doi = {10.1007/978-3-319-54334-5_5},
	abstract = {Supporting interactive database exploration (IDE) is a problem that attracts lots of attention these days. Exploratory OLAP (On-Line Analytical Processing) is an important use case where tools support navigation and analysis of the most interesting data, using the best possible perspectives. While many approaches were proposed (like query recommendation, reuse, steering, personalization or unexpected data recommendation), a recurrent problem is how to assess the effectiveness of an exploratory OLAP approach. In this paper we propose a benchmark framework to do so, that relies on an extensible set of user-centric metrics that relate to the main dimensions of exploratory analysis. Namely, we describe how to model and simulate user activity, how to formalize our metrics and how to build exploratory tasks to properly evaluate an IDE system under test (SUT). To the best of our knowledge, this is the first proposal of such a benchmark. Experiments are two-fold: first we evaluate the benchmark protocol and metrics based on synthetic SUTs whose behavior is well known. Second, we concentrate on two different recent SUTs from IDE literature that are evaluated and compared with our benchmark. Finally, potential extensions to produce an industry-strength benchmark are listed in the conclusion.},
	booktitle = {Performance {Evaluation} and {Benchmarking}. {Traditional} - {Big} {Data} - {Internet} of {Things}},
	author = {Djedaini, Mahfoud and Furtado, Pedro and Labroche, Nicolas and Marcel, Patrick and Peralta, Verónika},
	year = {2017},
	note = {ISSN: 16113349},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Validation Research, ★},
	pages = {61--77},
}

@article{Siddiqa2017,
	title = {On the analysis of big data indexing execution strategies},
	volume = {32},
	issn = {18758967},
	url = {https://eprints.soton.ac.uk/399946/},
	doi = {10.3233/JIFS-169269},
	abstract = {Efficient response to search queries is very crucial for data analysts to obtain timely results from big data spanned over heterogeneous machines. Currently, a number of big-data processing frameworks are available in which search operations are performed in distributed and parallel manner. However, implementation of indexing mechanism results in noticeable reduction of overall query processing time. There is an urge to assess the feasibility and impact of indexing towards query execution performance. This paper investigates the performance of state-of-the-art clustered indexing approaches over Hadoop framework which is de facto standard for big data processing. Moreover, this study leverages a comparative analysis of nonclustered indexing overhead in terms of time and space taken by indexing process for varying volume data sets with increasing Index Hit Ratio. Furthermore, the experiments evaluate performance of search operations in terms of data access and retrieval time for queries that use indexes. We then validated the obtained results using Petri net mathematical modeling. We used multiple data sets in our experiments to manifest the impact of growing volume of data on indexing and data search and retrieval performance. The results and highlighted challenges favorably lead researchers towards improved implication of indexing mechanism in perspective of data retrieval from big data. Additionally, this study advocates selection of a non-clustered indexing solution so that optimized search performance over big data is obtained. [ABSTRACT FROM AUTHOR]},
	number = {5},
	journal = {Journal of Intelligent and Fuzzy Systems},
	author = {Siddiqa, Aisha and Karim, Ahmad and Saba, Tanzila and Chang, Victor},
	year = {2017},
	keywords = {Big Data, cluster:Indexes, data retrieval, indexing, layer:Database Layer, supercluster:Indexes, type:Evaluation Research},
	pages = {3259--3271},
}

@article{Schuhknecht2016a,
	title = {{RUMA} has it : {Rewired} {User}-space {Memory} {Access} is {Possible} !},
	volume = {9},
	issn = {21508097},
	doi = {10.14778/2977797.2977803},
	abstract = {Memory management is one of the most boring topics in database research. It plays a minor role in tasks like free-space management or efficient space usage. Here and there we also realize its im-pact on database performance when worrying about NUMA-aware memory allocation, data compacting, snapshotting, and defragmen-tation. But, overall, let's face it: the entire topic sounds as exciting as 'garbage collection' or 'debugging a program for memory leaks'. What if there were a technique that would promote memory man-agement from a third class helper thingie to a first class citizen in algorithm and systems design? What if that technique turned the role of memory management in a database system (and any other data processing system) upside-down? What if that technique could be identified as a key for re-designing various core algorithms with the effect of outperforming existing state-of-the-art methods con-siderably? Then we would write this paper. We introduce RUMA: Rewired User-space Memory Access. It allows for physiological data management, i.e. we allow developers to freely rewire the mappings from virtual to physical memory (in user space) while at the same time exploiting the virtual memory support offered by hardware and operating system. We show that fundamental database building blocks such as array operations, par-titioning, sorting, and snapshotting benefit strongly from RUMA.},
	number = {10},
	journal = {Proceedings of the VLDB Endowment},
	author = {Schuhknecht, Felix Martin},
	year = {2016},
	pages = {768--779},
}

@inproceedings{Schuh2015,
	title = {{AIR}: {Adaptive} {Index} {Replacement} in {Hadoop}},
	isbn = {978-1-4799-8441-1},
	url = {http://ieeexplore.ieee.org/document/7129539/},
	doi = {10.1109/ICDEW.2015.7129539},
	abstract = {The Hadoop Distributed Filesystem has become the de-facto standard for storing large datasets in data management systems such as Hadoop MapReduce, Hive, and Stratosphere. Though HDFS was originally designed to support scan-oriented operations, recently several techniques for HDFS have been developed to allow for efficient indexing. One of these indexing techniques is aggressive indexing, i.e. HDFS replicas are immediately indexed at upload time before touching any disk – creating multiple clustered indexes almost for free on the way. A second technique is adaptive indexing, i.e. HDFS blocks are only indexed on demand as a side effect of query processing. Though these techniques provide impressive speed-ups in terms of query processing, they totally ignored the costs involved with storing a large number of replicas of a particular dataset. The HDFS-variants of adaptive indexing were already designed to leverage the natural redundancy that comes with HDFS, typically storing a dataset three times anyway. However, it is questionable whether storing an unlimited number of replicas for a dataset is a practical solution. Therefore, this paper is the first to analyze adaptive indexing under a space constraint, i.e. we assume that indexes are adaptively created and deleted. We coin this problem the Adaptive Index Replacement problem. We present a new algorithm to solve the online AIR problem called LeastExpectedBenefit-K and compare it with several existing state-of-the-art online Index Selection algorithms. We present a comprehensive study evaluating ten different algorithms. Our results show that our algorithm},
	booktitle = {2015 31st {IEEE} {International} {Conference} on {Data} {Engineering} {Workshops}},
	author = {Schuh, Stefan and Dittrich, Jens},
	year = {2015},
	note = {ISSN: 10844627},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {22--29},
}

@inproceedings{7828553,
	title = {Performance of {Point} and {Range} {Queries} for {In}-memory {Databases} {Using} {Radix} {Trees} on {GPUs}},
	url = {http://ieeexplore.ieee.org/document/7828553/},
	doi = {10.1109/HPCC-SmartCity-DSS.2016.0212},
	abstract = {In in-memory database systems augmented by hardware accelerators, accelerating the index searching operations can greatly increase the runtime performance of database queries. Recently, adaptive radix trees (ART) have been shown to provide very fast index search implementation on the CPU. Here, we focus on an accelerator-based implementation of ART. We present a detailed performance study of our GPU-based adaptive radix tree (GRT) implementation over a variety of key distributions, synthetic benchmarks, and actual keys from music and book data sets. The performance is also compared with other index-searching schemes on the GPU. GRT on modern GPUs achieves some of the highest rates of index searches reported in the literature. For point queries, a throughput of up to 106 million and 130 million lookups per second is achieved for sparse and dense keys, respectively. For range queries, GRT yields 600 million and 1000 million lookups per second for sparse and dense keys, respectively, on a large dataset of 64 million 32-bit keys.},
	booktitle = {2016 {IEEE} 18th {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 14th {International} {Conference} on {Smart} {City}; {IEEE} 2nd {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Alam, M and Yoginath, S B and Perumalla, K S},
	month = dec,
	year = {2016},
	keywords = {database management systems, graphics processing un},
	pages = {1493--1500},
}

@inproceedings{8029774,
	title = {Interactive {Data} {Exploration} as a {Service} for the {Smart} {Factory}},
	url = {http://ieeexplore.ieee.org/document/8029774/},
	doi = {10.1109/ICWS.2017.129},
	abstract = {In the era of Internet of Things and dynamically interconnected systems, real time data becomes a new industrial asset, used to create new opportunities for operations improvement and to increase industrial value through the capitalisation of immaterial assets. In the smart factory, big data acquisition, analysis and visualisation pave the way to the manufacturing servitization, defined as the strategic innovation of organisations capabilities and processes to shift from product offering to an integrated "product plus service" offering. According to this vision, interconnected physical systems are associated with a cyber twin, where innovative services for big data management should be provided. In this paper, we propose an interactive data exploration framework, that poses a service-oriented perspective on the smart factory. Large amounts of data are incrementally collected from physical systems, organized and analysed on the cloud and new services are provided to enable data exploration. Such services implement novel data summarisation techniques, based on clustering, to manage data abundance, and data relevance evaluation techniques, aimed to focus the attention on relevant data that is being explored. Services are based on a multi-dimensional model, that is suited for supporting the iterative and multi-step exploration of Big Data.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Bagozi, A and Bianchini, D and Antonellis, V D and Marini, A and Ragazzi, D},
	month = jun,
	year = {2017},
	keywords = {Big Data, Computer architecture, Data acquisition},
	pages = {293--300},
}

@inproceedings{Grossniklaus:2016:FDW:2933267.2933304,
	address = {New York, NY, USA},
	title = {Frames},
	isbn = {978-1-4503-4021-2},
	url = {http://dl.acm.org/citation.cfm?doid=2933267.2933304},
	doi = {10.1145/2933267.2933304},
	abstract = {Traditional Data Stream Management Systems (DSMS) segment data streams using windows that are defined either by a time interval or a number of tuples. Such windows are fixed---the definition unvarying over the course of a stream---and are defined based on external properties unrelated to the data content of the stream. However, streams and their content do vary over time---the rate of a data stream may vary or the data distribution of the content may vary. The mismatch between a fixed stream segmentation and a variable stream motivates the need for a more flexible, expressive and physically independent stream segmentation. We introduce a new stream segmentation technique, called frames. Frames segment streams based on data content. We present a theory and implementation of frames and show the utility of frames for a variety of applications.},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Distributed} and {Event}-based {Systems} - {DEBS} '16},
	publisher = {ACM},
	author = {Grossniklaus, Michael and Maier, David and Miller, James and Moorthy, Sharmadha and Tufte, Kristin},
	year = {2016},
	note = {Series Title: DEBS '16},
	keywords = {data streams, stream processing, stream segmentation},
	pages = {13--24},
}

@incollection{Bagozi2017,
	address = {Cham},
	title = {Summarisation and {Relevance} {Evaluation} {Techniques} for {Big} {Data} {Exploration}: {The} {Smart} {Factory} {Case} {Study}},
	isbn = {978-3-319-59536-8},
	url = {http://link.springer.com/10.1007/978-3-319-59536-8_17},
	abstract = {The increasing connections of systems that produce high volumes of real time data have raised the importance of addressing data abundance research challenges. In the Industry 4.0 application domain, for example, high volumes and velocity of data collected from machines, as well as value of data that declines very quickly, put Big Data issues among the new challenges also for the factory of the future. While many approaches have been developed to investigate data analysis, data visualisation, data collection and management, the impact of Big Data exploration is still under-estimated. In this paper, we propose an approach to support and ease exploration of real time data in a dynamic context of interconnected systems, such as the Industry 4.0 domain, where large amounts of data must be incrementally collected, organized and analysed on-the-fly. The approach relies on: (i) a multi-dimensional model, that is suited for supporting the iterative and multi-step exploration of Big Data; (ii) novel data summarisation techniques, based on clustering; (iii) a model of relevance, aimed at focusing the attention of the user only on relevant data that are being explored. We describe the application of the approach in the smart factory as a case study.},
	booktitle = {Advanced {Information} {Systems} {Engineering}: 29th {International} {Conference}, {CAiSE} 2017, {Essen}, {Germany}, {June} 12-16, 2017, {Proceedings}},
	publisher = {Springer International Publishing},
	author = {Bagozi, Ada and Bianchini, Devis and De Antonellis, Valeria and Marini, Alessandro and Ragazzi, Davide},
	editor = {Dubois, Eric and Pohl, Klaus},
	year = {2017},
	doi = {10.1007/978-3-319-59536-8_17},
	pages = {264--279},
}

@inproceedings{Terlecki:2015:IUR:2723372.2742799,
	address = {New York, NY, USA},
	title = {On {Improving} {User} {Response} {Times} in {Tableau}},
	isbn = {978-1-4503-2758-9},
	url = {http://doi.acm.org/10.1145/2723372.2742799},
	doi = {10.1145/2723372.2742799},
	abstract = {The rapid increase in data volumes and complexity of applied analytical tasks poses a big challenge for visualization solutions. It is important to keep the experience highly interactive, so that users stay engaged and can perform insightful data exploration. Query processing usually dominates the cost of visualization generation. Therefore, in order to achieve acceptable response times, one needs to utilize backend capabilities to the fullest and apply techniques, such as caching or prefetching. In this paper we discuss key data processing components in Tableau: the query processor, query caches, Tableau Data Engine [1, 2] and Data Server. Furthermore, we cover recent performance improvements related to the number and quality of remote queries, broader reuse of cached data, and application of inter and intra query parallelism.},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Terlecki, Pawel and Xu, Fei and Shaw, Marianne and Kim, Valeri and Wesley, Richard},
	year = {2015},
	note = {Series Title: SIGMOD '15
ISSN: 07308078},
	keywords = {cluster:Visual Optimizations, data server, data visualization, layer:User Interaction, query batching, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1695--1706},
}

@article{Christensen2015,
	title = {{STORM} : {Spatio}-{Temporal} {Online} {Reasoning} and {Management} of {Large} {Spatio}-{Temporal} {Data}},
	issn = {07308078},
	url = {https://dl.acm.org/citation.cfm?doid=2723372.2735373},
	doi = {10.1145/2723372.2735373},
	abstract = {We present the STORM system to enable spatio-temporal online reasoning and management of large spatio-temporal data. STORM supports interactive spatio-temporal analytics through novel spatial online sampling techniques. Online spatio-temporal aggregation and analytics are then derived based on the online samples, where approximate answers with approximation quality guarantees can be provided im-mediately from the start of query execution. The quality of these online approximations improve over time. This demon-stration proposal describes key ideas in the design of the STORM system, and presents the demonstration plan.},
	journal = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
	author = {Christensen, Robert and Wang, Lu and Li, Feifei and Yi, Ke and Tang, Jun and Villa, Natalee},
	year = {2015},
	note = {ISBN: 9781450327589},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1111--1116},
}

@inproceedings{Cong2016,
	title = {Querying and mining geo-textual data for exploration: {Challenges} and opportunities},
	isbn = {978-1-5090-2108-6},
	url = {http://ieeexplore.ieee.org/document/7495640/},
	doi = {10.1109/ICDEW.2016.7495640},
	abstract = {Abstract: Geo-textual data (e.g., geo-tagged tweets) is becoming increasingly available on the Web. This paper reviews recent studies on searching and mining geo-textual data for exploration, and discusses future directions along with open problems.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} {Workshops}, {ICDEW} 2016},
	author = {Cong, Gao and Feng, Kaiyu and Zhao, Kaiqi},
	year = {2016},
	pages = {165--168},
}

@article{Brucato2017,
	title = {A {Scalable} {Execution} {Engine} for {Package} {Queries}},
	volume = {46},
	issn = {01635808},
	url = {http://dl.acm.org/citation.cfm?doid=3093754.3093761},
	doi = {10.1145/3093754.3093761},
	abstract = {Many modern applications and real-world problems involve the design of item collections, or packages: from planning your daily meals all the way to mapping the universe. Despite the pervasive need for packages, traditional data management does not offer support for their definition and computation. This is because traditional database queries follow a powerful, but very simple model: a query defines constraints that each tuple in the result must satisfy. However, a system tasked with the design of packages cannot consider items independently; rather, the system needs to determine if a set of items collectively satisfy given criteria. In this paper, we present package queries, a new query model that extends traditional database queries to handle complex constraints and preferences over answer sets. We develop a full-fledged package query system, implemented on top of a traditional database engine. Our work makes several contributions. First, we design PaQL, a SQL-based query language that supports the declarative specification of package queries. Second, we present a fundamental strategy for evaluating package queries that combines the capabilities of databases and constraint optimization solvers. The core of our approach is a set of translation rules that transform a package query to an integer linear program. Third, we introduce an offline data partitioning strategy allowing query evaluation to scale to large data sizes. Fourth, we introduce SKETCHREFINE, an efficient and scalable algorithm for package evaluation, which offers strong approximation guarantees. Finally, we present extensive experiments over real-world data. Our results demonstrate that SKETCHREFINE is effective at deriving high-quality package results, and achieves runtime performance that is an order of magnitude faster than directly using ILP solvers over large datasets.},
	number = {1},
	journal = {SIGMOD Rec.},
	author = {Brucato, Matteo and Abouzied, Azza and Meliou, Alexandra},
	month = may,
	year = {2017},
	note = {Publisher: ACM
Place: New York, NY, USA},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {24--31},
}

@article{Albarrak2017,
	title = {Efficient schemes for similarity-aware refinement of aggregation queries},
	volume = {20},
	issn = {1386145X},
	url = {https://link.springer.com/article/10.1007/s11280-017-0434-4},
	doi = {10.1007/s11280-017-0434-4},
	abstract = {Interactive data exploration platforms in Web, business and scientific domains are becoming increasingly popular. Typically, users without prior knowledge of data interact with these platforms in an exploratory manner hoping they might retrieve the results they are looking for. One way to explore large-volume data is by posing aggregate queries which group values of multiple rows by an aggregate operator to form a single value: an aggregated value. Though, when a query fails, i.e., returns undesired aggregated value, users will have to undertake a frustrating trial-and-error process to refine their queries, until a desired result is attained. This data exploration process, however, is growing rather difficult as the underlying data is typically of large-volume and high-dimensionality. While heuristic-based techniques are fairly successful in generating refined queries that meet specified requirements on the aggregated values, they are rather oblivious to the (dis)similarity between the input query and its corresponding refined version. Meanwhile, enforcing a similarity-aware query refinement is rather a non-trivial challenge, as it requires a careful examination of the query space while maintaining a low processing cost. To address this challenge, we propose an innovative scheme for efficient Similarity-Aware Refinement of Aggregation Queries called (EAGER) which aims to balance the tradeoff between satisfying the aggregate and similarity constraints imposed on the refined query to maximize its overall benefit to the user. To achieve that goal, EAGER implements efficient strategies to minimize the costs incurred in exploring the available search space by utilizing similarity-based and monotonic-based pruning techniques to bound the search space and quickly find a refined query that meets users’ expectations. Our extensive experiments show the scalability exhibited by EAGER under various workload settings, and the significant benefits it provides.},
	number = {6},
	journal = {World Wide Web},
	author = {Albarrak, Abdullah M. and Sharaf, Mohamed A.},
	year = {2017},
	keywords = {Data exploration, Query refinement, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1237--1267},
}

@article{Zgraggen2017,
	title = {How {Progressive} {Visualizations} {Affect} {Exploratory} {Analysis}},
	volume = {23},
	issn = {10772626},
	url = {http://ieeexplore.ieee.org/document/7563865/},
	doi = {10.1109/TVCG.2016.2607714},
	abstract = {The stated goal for visual data exploration is to operate at a rate that matches the pace of human data analysts, but the ever increasing amount of data has led to a fundamental problem: datasets are often too large to process within interactive time frames. Progressive analytics and visualizations have been proposed as potential solutions to this issue. By processing data incrementally in small chunks, progressive systems provide approximate query answers at interactive speeds that are then refined over time with increasing precision.We study how progressive visualizations affect users in exploratory settings in an experiment where we capture user behavior and knowledge discovery through interaction logs and think-aloud protocols. Our experiment includes three visualization conditions and different simulated dataset sizes. The visualization conditions are: (1) blocking, where results are displayed only after the entire dataset has been processed; (2) instantaneous, a hypothetical condition where results are shown almost immediately; and (3) progressive, where approximate results are displayed quickly and then refined over time.We analyze the data collected in our experiment and observe that users perform equally well with either instantaneous or progressive visualizations in key metrics, such as insight discovery rates and dataset coverage, while blocking visualizations have detrimental effects.},
	number = {8},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zgraggen, Emanuel and Galakatos, Alex and Crotty, Andrew and Fekete, Jean Daniel and Kraska, Tim},
	year = {2017},
	keywords = {Exploratory analysis, UserStudy:yes, cluster:Query Approximation, insight-based evaluation, interactive visualization, layer:Middleware, progressive visualization, scalability, supercluster:Interactive Performance Optimizations, type:Validation Research},
	pages = {1977--1987},
}

@article{Dimitriadou2016,
	title = {{AIDE}: {An} {Active} {Learning}-{Based} {Approach} for {Interactive} {Data} {Exploration}},
	volume = {28},
	issn = {10414347},
	url = {http://ieeexplore.ieee.org/document/7539596/},
	doi = {10.1109/TKDE.2016.2599168},
	abstract = {In this paper, we argue that database systems be augmented with an automated data exploration service that methodically steers users through the data in a meaningful way. Such an automated system is crucial for deriving insights from complex datasets found in many big data applications such as scientific and healthcare applications as well as for reducing the human effort of data exploration. Towards this end, we present AIDE, an Automatic Interactive Data Exploration framework that assists users in discovering new interesting data patterns and eliminate expensive ad-hoc exploratory queries. AIDE relies on a seamless integration of classification algorithms and data management optimization techniques that collectively strive to accurately learn the user interests based on his relevance feedback on strategically collected samples. We present a number of exploration techniques as well as optimizations that minimize the number of samples presented to the user while offering interactive performance. AIDE can deliver highly accurate query predictions for very common conjunctive queries with small user effort while, given a reasonable number of samples, it can predict with high accuracy complex disjunctive queries. It provides interactive performance as it limits the user wait time per iteration of exploration to less than a few seconds.},
	number = {11},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Dimitriadou, Kyriaki and Papaemmanouil, Olga and Diao, Yanlei},
	year = {2016},
	note = {arXiv: 1510.08897},
	keywords = {Data exploration, RInteractive: Yes, UserStudy:yes, cluster:Sampling, data sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {2842--2856},
}

@inproceedings{Feng2016a,
	title = {Towards {Best} {Region} {Search} for {Data} {Exploration}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882960},
	doi = {10.1145/2882903.2882960},
	abstract = {The increasing popularity and growth of mobile devices and location- based services enable us to utilize large-scale geo-tagged data to support novel location-based applications. This paper introduces a novel problem called the best region search (BRS) problem and provides efficient solutions to it. Given a set O of spatial objects, a submodular monotone aggregate score function, and the size a×b of a query rectangle, the BRS problem aims to find a×b rectangular region such that the aggregate score of the spatial objects inside the region is maximized. This problem is fundamental to support sev- eral real-world applications such as most influential region search (e.g., the best location for a signage to attract most audience) and most diversified region search (e.g., region with most diverse facil- ities). We propose an efficient algorithm called SliceBRS to find the exact answer to the BRS problem. Furthermore, we propose an approximate solution called CoverBRS and prove that the answer found by it is bounded by a constant. Our experimental study with real-world datasets and applications demonstrates the effectiveness and superiority of our proposed algorithms.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	author = {Feng, Kaiyu and Cong, Gao and Bhowmick, Sourav S. and Peng, Wen-Chih and Miao, Chunyan},
	year = {2016},
	note = {ISSN: 07308078},
	keywords = {cluster:Spatial Query, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1055--1070},
}

@article{iet:/content/journals/10.1049/joe.2016.0068,
	title = {Adaptive indexing approach for main memory column store},
	volume = {2017},
	url = {http://digital-library.theiet.org/content/journals/10.1049/joe.2016.0068},
	doi = {10.1049/joe.2016.0068},
	abstract = {Owing to efficient query processing for random workload, the hybrid crack sort (HCS) has become an important adaptive indexing approach in main-memory column store. However, under sequential workload scenarios, the HCS does not obtain a good query execution performance, because of great reorganisation overhead imposed on the initial queries. The authors propose a hybrid radix crack sort (HRCS) approach to solve this problem. By the adoption of radix-based partition strategy, it divides the unsorted column into disjoint key ranges and then conducts data reorganisation in at most two key ranges for each query. For HRCS, only a small portion of the whole column needs to be touched for the processing of each query, thus reducing the reorganisation cost and improving the query execution performance. The final experiments show that the novel HRCS approach can obtain a higher query execution performance for not only random workload but also sequential workload, as compared with HCS.},
	number = {2},
	journal = {The Journal of Engineering},
	author = {Liu, Zhijing},
	month = jul,
	year = {2016},
	note = {Publisher: Institution of Engineering and Technology},
	keywords = {HRCS approach, cluster:Adaptive Indexing, layer:Database Layer, radix-based partition strategy, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {26--32},
}

@inproceedings{Petraki2015,
	title = {Holistic {Indexing} in {Main}-memory {Column}-stores},
	isbn = {978-1-4503-2758-9},
	url = {http://dl.acm.org/citation.cfm?doid=2723372.2723719},
	doi = {10.1145/2723372.2723719},
	abstract = {Great database systems performance relies heavily on index tun- ing, i.e., creating and utilizing the best indices depending on the workload. However, the complexity of the index tuning process has dramatically increased in recent years due to ad-hoc workloads and shortage of time and system resources to invest in tuning. This paper introduces holistic indexing, a new approach to au- tomated index tuning in dynamic environments. Holistic indexing requires zero set-up and tuning effort, relying on adaptive index creation as a side-effect of query processing. Indices are created incrementally and partially; they are continuously refined as we process more and more queries. Holistic indexing takes the state- of-the-art adaptive indexing ideas a big step further by introducing the notion of a system which never stops refining the index space, taking educated decisions about which index we should incremen- tally refine next based on continuous knowledge acquisition about the running workload and resource utilization. When the system detects idle CPU cycles, it utilizes those extra cycles by refining the adaptive indices which are most likely to bring a benefit for future queries. Such idle CPU cycles occur when the system can- not exploit all available cores up to 100\%, i.e., either because the workload is not enough to saturate the CPUs or because the current tasks performed for query processing are not easy to parallelize to the point where all available CPU power is exploited. In this paper, we present the design of holistic indexing for column- oriented database architectures and we discuss a detailed analysis against parallel versions of state-of-the-art indexing and adaptive indexing approaches. Holistic indexing is implemented in an open- source column-store DBMS. Our detailed experiments on both syn- thetic and standard benchmarks (TPC-H) and workloads (SkyServer) demonstrate that holistic indexing brings significant performance gains by being able to continuously refine the physical design in parallel to query processing, exploiting any idle CPU resources.},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Petraki, Eleni and Idreos, Stratos and Manegold, Stefan},
	year = {2015},
	note = {ISSN: 07308078},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1153--1166},
}

@article{Rompf2015,
	title = {Go {Meta}! {A} {Case} for {Generative} {Programming} and {DSLs} in {Performance} {Critical} {Systems}},
	issn = {18688969},
	doi = {10.4230/LIPIcs.SNAPL.2015.238},
	abstract = {Most performance critical software is developed using very low-level techniques. We argue that this needs to change, and that generative programming is an effective avenue to enable the use of high-level languages and programming techniques in many such circumstances.},
	journal = {1st Summit on Advances in Programming Languages},
	author = {Rompf, Tiark and Brown, Kevin J. and Lee, HyoukJoong and Sujeeth, Arvind K. and Jonnalagedda, Manohar and Amin, Nada and Ofenbeck, Georg and Stojanov, Alen and Klonatos, Yannis and Dashti, Mohammad and {others}},
	year = {2015},
	note = {ISBN: 9783939897804},
	pages = {238},
}

@inproceedings{Idreos2016,
	title = {Past and {Future} {Steps} for {Adaptive} {Storage} {Data} {Systems}: {From} {Shallow} to {Deep} {Adaptivity}},
	url = {https://stratos.seas.harvard.edu/publications/past-and-future-steps-adaptive-storage-data-systems-shallow-deep-adaptivity},
	abstract = {Datasystems with adaptive storage can autonomously change their behavior by altering how data is stored and accessed. Such systems have been studied primarily for the case of adaptive indexing to auto- matically create the right indexes at the right granularity. More recently work on adaptive loading and adaptive data layouts brought even more flexibility. We survey this work and describe the need for even deeper adaptivity that goes beyond adjusting knobs in a single architecture; instead it can adapt the fundamental architecture of a data system to drastically alter its behavior.},
	booktitle = {International {Workshop} on {Enabling} {Real}-{Time} {Business} {Intelligence}},
	author = {Idreos, Stratos and Athanassoulis, Manos and Dayan, Niv and Guo, Demi and Kester, Michael S and Maas, Lukas M and Zoumpatianos, Kostas},
	year = {2016},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Opinion Papers},
}

@article{Schuhknecht2016,
	title = {An experimental evaluation and analysis of database cracking},
	volume = {25},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-015-0397-y},
	doi = {10.1007/s00778-015-0397-y},
	abstract = {Database cracking has been an area of active research in recent years. The core idea of database cracking is to create indexes adaptively and incrementally as a side product of query processing. Several works have proposed different cracking techniques for different aspects including updates, tuple reconstruction, convergence, concurrency control, and robustness. Our 2014 VLDB paper ``The Uncracked Pieces in Database Cracking'' (PVLDB 7:97--108, 2013/VLDB 2014) was the first comparative study of these different methods by an independent group. In this article, we extend our published experimental study on database cracking and bring it to an up-to-date state. Our goal is to critically review several aspects, identify the potential, and propose promising directions in database cracking. With this study, we hope to expand the scope of database cracking and possibly leverage cracking in database engines other than MonetDB. We repeat several prior database cracking works including the core cracking algorithms as well as three other works on convergence (hybrid cracking), tuple reconstruction (sideways cracking), and robustness (stochastic cracking), respectively. Additionally to our conference paper, we now also look at a recently published study about CPU efficiency (predication cracking). We evaluate these works and show possible directions to do even better. As a further extension, we evaluate the whole class of parallel cracking algorithms that were proposed in three recent works. Altogether, in this work we revisit 8 papers on database cracking and evaluate in total 18 cracking methods, 6 sorting algorithms, and 3 full index structures. Additionally, we test cracking under a variety of experimental settings, including high selectivity (Low selectivity means that many entries qualify. Consequently, a high selectivity means, that only few entries qualify) queries, low selectivity queries, varying selectivity, and multiple query access patterns. Finally, we compare cracking against different sorting algorithms as well as against different main memory optimized indexes, including the recently proposed adaptive radix tree (ART). Our results show that: (1) the previously proposed cracking algorithms are repeatable, (2) there is still enough room to significantly improve the previously proposed cracking algorithms, (3) parallelizing cracking algorithms efficiently is a hard task, (4) cracking depends heavily on query selectivity, (5) cracking needs to catch up with modern indexing trends, and (6) different indexing algorithms have different indexing signatures.},
	number = {1},
	journal = {The VLDB Journal},
	author = {Schuhknecht, Felix Martin and Jindal, Alekh and Dittrich, Jens},
	month = feb,
	year = {2016},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Evaluation Research},
	pages = {27--52},
}

@inproceedings{Pirk2016,
	title = {Voodoo - {A} vector algebra for portable database performance on modern hardware},
	url = {https://dl.acm.org/citation.cfm?id=3007336},
	doi = {10.14778/3007328.3007336},
	abstract = {In-memory databases require careful tuning and many engineering tricks to achieve good performance. Such database performance engineering is hard: a plethora of data and hardware-dependent optimization techniques form a design space that is difficult to navigate for a skilled engineer – even more so for a query compiler. To facilitate performance- oriented design exploration and query plan compilation, we present Voodoo, a declarative intermediate algebra that abstracts the detailed architectural properties of the hardware, such as multi- or many-core architectures, caches and SIMD registers, without losing the ability to generate highly tuned code. Because it consists of a collection of declarative, vector-oriented operations, Voodoo is easier to reason about and tune than low-level C and related hardware-focused ex- tensions (Intrinsics, OpenCL, CUDA, etc.). This enables our Voodoo compiler to produce (OpenCL) code that rivals and even outperforms the fastest state-of-the-art in memory databases for both GPUs and CPUs. In addition, Voodoo makes it possible to express techniques as diverse as cache- conscious processing, predication and vectorization (again on both GPUs and CPUs) with just a few lines of code. Central to our approach is a novel idea we termed control vectors, which allows a code generating frontend to expose parallelism to the Voodoo compiler in a abstract manner, enabling portable performance across hardware platforms. We used Voodoo to build an alternative backend for Mon- etDB, a popular open-source in-memory database. Our back- end allows MonetDB to perform at the same level as highly tuned in-memory databases, including HyPeR and Ocelot. We also demonstrate Voodoo’s usefulness when investigat- ing hardware conscious tuning techniques, assessing their performance on different queries, devices and data.},
	booktitle = {{VLDB}},
	author = {Pirk, Holger and Moll, Oscar and Zaharia, Matei and Madden, Sam},
	year = {2016},
	note = {ISSN: 21508097},
	pages = {1707--1718},
}

@article{Idreos:2015:DDS:2783888.2783903,
	title = {{DASlab}: {The} {Data} {Systems} {Laboratory} at {Harvard} {SEAS}},
	volume = {44},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/2783888.2783903},
	doi = {10.1145/2783888.2783903},
	abstract = {DASlab is a new laboratory at the Harvard School of Engineering and Applied Sciences (SEAS). The lab was formed in January 2014 when Stratos Idreos joined Harvard SEAS. DASlab curently consists of 3 PhD students, 1 postdoctoral researcher and 9 undergraduate researchers while it is set to double its graduate student population in the next one to two years. The lab is part of a growing community of systems and computer science researchers at Harvard; computer science faculty is scheduled to grow by 50\% in the next few year. The main focus of DASlab is on designing data systems that (a) make it easy to extract knowledge out of increasingly diverse and growing data sets and (b) can stand the test of time.},
	number = {1},
	journal = {SIGMOD Rec.},
	author = {Idreos, Stratos},
	month = may,
	year = {2015},
	note = {Publisher: ACM
Place: New York, NY, USA},
	pages = {46--51},
}

@inproceedings{Pirk:2015:FSY:2771937.2771944,
	address = {New York, NY, USA},
	title = {By their fruits shall ye know them -- {A} {Data} {Analyst}'s {Perspective} on {Massively} {Parallel} {System} {Design}},
	isbn = {978-1-4503-3638-3},
	url = {http://doi.acm.org/10.1145/2771937.2771944},
	doi = {10.1145/2771937.2771944},
	abstract = {Increasingly parallel systems promise a remedy for the current stagnation of single-core performance. However, the battle to find the most appropriate architecture for the resulting massively parallel systems is still ongoing. Currently, there are two active contenders: Massively Parallel Single Instruction Multiple Threads (SIMT) systems such as GPGPUs and Many Core Single Instruction Multiple Data (SIMD) systems such as Intel's Xeon Phi. While the former is more versatile, the latter is an efficient, time-tested technology with a clear migration path. In this study, we provide a data management perspective to the debate: we study the implementation and performance of a set of common data management operations on an SIMT device (an Nvidia GTX 780) and compare it to a Many Core SIMD system (an Intel Xeon Phi). We interpret the results to pinpoint architectural decisions and tradeoffs that lead to suboptimal performance and point out potential areas for improvement in the next generation of these devices.},
	booktitle = {{DaMoN}},
	publisher = {ACM},
	author = {Pirk, Holger and Madden, Sam and Stonebraker, Mike},
	year = {2015},
	note = {Series Title: DaMoN'15},
	pages = {5:1--5:6},
}

@inproceedings{Broneske2017,
	title = {Accelerating multi-column selection predicates in main-memory -{The} {Elf} approach},
	isbn = {978-1-5090-6543-1},
	url = {http://ieeexplore.ieee.org/document/7930014/},
	doi = {10.1109/ICDE.2017.118},
	abstract = {Evaluating selection predicates is a data-intensive task that reduces intermediate results, which are the input for further operations. With analytical queries getting more and more complex, the number of evaluated selection predicates per query and table rises, too. This leads to numerous multi-column selection predicates. Recent approaches to increase the performance of main-memory databases for selection-predicate evaluation aim at optimally exploiting the speed of the CPU by using accelerated scans. However, scanning each column one by one leaves tuning opportunities open that arise if all predicates are considered together. To this end, we introduce Elf, an index structure that is able to exploit the relation between several selection predicates. Elf features cache sensitivity, an optimized storage layout, fixed search paths, and slight data compression. In our evaluation, we compare its query performance to state-of-the-art approaches and a sequential scan using SIMD capabilities. Our results indicate a clear superiority of our approach for multi-column selection predicate queries with a low combined selectivity. For TPC-H queries with multi-column selection predicates, we achieve a speed-up between a factor of five and two orders of magnitude, mainly depending on the selectivity of the predicates.},
	booktitle = {Proceedings - {International} {Conference} on {Data} {Engineering}},
	author = {Broneske, David and Köppen, Veit and Saake, Gunter and Schäler, Martin},
	year = {2017},
	note = {ISSN: 10844627},
	pages = {647--658},
}

@inproceedings{Gawade2016,
	title = {Multi-core {Column}-store {Parallelization} {Under} {Concurrent} {Workload}},
	isbn = {978-1-4503-4319-0},
	url = {http://doi.acm.org/10.1145/2933349.2933350},
	doi = {10.1145/2933349.2933350},
	abstract = {Columnar database systems, designed for an optimal OLAP workload performance, strive for maximum multi-core utilization under concurrent query executions. However, multi-core parallel plan generated for isolated execution leads to suboptimal performance during concurrent query execution. In this paper, we analyze the concurrent workload resource contention effects on multi-core plans using three intra-query parallelization techniques, static, adaptive, and cost model parallelization. We focus on a plan level comparison of selected TPC-H queries, using in-memory multi-core columnar systems. Excessive partitions in statically parallelized plans result into heavy L3 cache misses leading to memory contention, degrading query performance severely. Overall, adaptive plans show more robustness, less scheduling overheads, and an average 50\% execution time improvement compared to statically parallelized plans, and cost model based plans.},
	booktitle = {Proceedings of the 12th {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	author = {Gawade, Mrunal and Kersten, Martin and Simitsis, Alkis},
	year = {2016},
	pages = {1:1--1:10},
}

@inproceedings{Xi:2015:BWN:2771937.2771945,
	address = {New York, NY, USA},
	title = {Beyond the {Wall}: {Near}-{Data} {Processing} for {Databases}},
	isbn = {978-1-4503-3638-3},
	url = {http://doi.acm.org/10.1145/2771937.2771945},
	doi = {10.1145/2771937.2771945},
	abstract = {The continuous growth of main memory size allows modern data systems to process entire large scale datasets in memory. The increase in memory capacity, however, is not matched by proportional decrease in memory latency, causing a mismatch for in-memory processing. As a result, data movement through the memory hierarchy is now one of the main performance bottlenecks for main memory data systems. Database systems researchers have proposed several innovative solutions to minimize data movement and to make data access patterns hardware-aware. Nevertheless, all relevant rows and columns for a given query have to be moved through the memory hierarchy; hence, movement of large data sets is on the critical path. In this paper, we present JAFAR, a Near-Data Processing (NDP) accelerator for pushing selects down to memory in modern column-stores. JAFAR implements the select operator and allows only qualifying data to travel up the memory hierarchy. Through a detailed simulation of JAFAR hardware we show that it has the potential to provide 9x improvement for selects in column-stores. In addition, we discuss both hardware and software challenges for using NDP in database systems as well as opportunities for further NDP accelerators to boost additional relational operators.},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	publisher = {ACM},
	author = {Xi, Sam Likun and Babarinsa, Oreoluwa and Athanassoulis, Manos and Idreos, Stratos},
	year = {2015},
	note = {Series Title: DaMoN'15},
	keywords = {cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {2:1--2:10},
}

@inproceedings{Shaikhha2016,
	title = {How to {Architect} a {Query} {Compiler}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2915244},
	doi = {10.1145/2882903.2915244},
	abstract = {This paper studies architecting query compilers. The state of the art in query compiler construction is lagging behind that in the compilers field. We attempt to remedy this by exploring the key causes of technical challenges in need of well founded solutions, and by gathering the most relevant ideas and approaches from the PL and compilers communities for easy digestion by database researchers. All query compilers known to us are more or less monolithic template expanders that do the bulk of the compilation task in one large leap. Such systems are hard to build and maintain. We propose to use a stack of multiple DSLs on different levels of abstraction with lowering in multiple steps to make query compilers easier to build and extend, ultimately allowing us to create more convincing and sustainable compiler-based data management systems. We attempt to derive our advice for creating such DSL stacks from widely acceptable principles. We have also re-created a well-known query compiler following these ideas and report on this effort.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data} - {SIGMOD} '16},
	author = {Shaikhha, Amir and Klonatos, Yannis and Parreaux, Lionel and Brown, Lewis and Dashti, Mohammad and Koch, Christoph},
	year = {2016},
	note = {ISSN: 07308078},
	pages = {1907--1922},
}

@article{IdreosMK17,
	title = {Evolutionary {Data} {Systems}},
	volume = {abs/1706.0},
	url = {http://arxiv.org/abs/1706.05714},
	abstract = {Anyone in need of a data system today is confronted with numerous complex options in terms of system architectures, such as traditional relational databases, NoSQL and NewSQL solutions as well as several sub-categories like column-stores, row-stores etc. This overwhelming array of choices makes bootstrapping data-driven applications difficult and time consuming, requiring expertise often not accessible due to cost issues (e.g., to scientific labs or small businesses). In this paper, we present the vision of evolutionary data systems that free systems architects and application designers from the complex, cumbersome and expensive process of designing and tuning specialized data system architectures that fit only a single, static application scenario. Setting up an evolutionary system is as simple as identifying the data. As new data and queries come in, the system automatically evolves so that its architecture matches the properties of the incoming workload at all times. Inspired by the theory of evolution, at any given point in time, an evolutionary system may employ multiple competing solutions down at the low level of database architectures -- characterized as combinations of data layouts, access methods and execution strategies. Over time, "the fittest wins" and becomes the dominant architecture until the environment (workload) changes. In our initial prototype, we demonstrate solutions that can seamlessly evolve (back and forth) between a key-value store and a column-store architecture in order to adapt to changing workloads.},
	journal = {CoRR},
	author = {Idreos, Stratos and Maas, Lukas M. and Kester, Mike S.},
	year = {2017},
	note = {arXiv: 1706.05714},
	keywords = {★},
}

@inproceedings{Karras2016,
	title = {Adaptive {Indexing} over {Encrypted} {Numeric} {Data}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2882932},
	doi = {10.1145/2882903.2882932},
	abstract = {Today, outsourcing query processing tasks to remote cloud servers becomes a viable option; such outsourcing calls for encrypting data stored at the server so as to render it secure against eavesdropping adversaries and/or an honest-but-curious server itself. At the same time, to be efficiently managed, outsourced data should be indexed, and even adaptively so, as a side-effect of query processing. Computationally heavy encryption schemes render such outsourcing unattractive; an alternative, Order-Preserving Encryption Scheme (OPES), intentionally preserves and reveals the order in the data, hence is unattractive from the security viewpoint. In this paper, we propose and analyze a scheme for lightweight and indexable encryption, based on linear-algebra operations. Our scheme provides higher security than OPES and allows for range and point queries to be efficiently evaluated over encrypted numeric data, with decryption performed at the client side. We implement a prototype that performs incremental, query-triggered adaptive indexing over encrypted numeric data based on this scheme, without leaking order information in advance, and without prohibitive overhead, as our extensive experimental study demonstrates.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	author = {Karras, Panagiotis and Nikitin, Artyom and Saad, Muhammad and Bhatt, Rudrika and Antyukhov, Denis and Idreos, Stratos},
	year = {2016},
	note = {ISSN: 07308078},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {171--183},
}

@misc{Demiralp2016ClustrophileAT,
	title = {Clustrophile: {A} {Tool} for {Visual} {Clustering} {Analysis}},
	abstract = {While clustering is one of the most popular methods for data mining, analysts lack adequate tools for quick, iterative clustering anal- ysis, which is essential for hypothesis generation and data reasoning. We introduce Clustrophile, an interactive tool for iteratively computing discrete and continuous data clusters, rapidly explor- ing different choices of clustering parameters, and reasoning about clustering instances in relation to data dimensions. Clustrophile combines three basic visualizations – a table of raw datasets, a scatter plot of planar projections, and a matrix diagram (heatmap) of discrete clusterings – through interaction and intermediate vi- sual encoding. Clustrophile also contributes two spatial interaction techniques, forward projection and backward projection, and a visualization method, prolines, for reasoning about two-dimensional projections obtained through dimensionality reductions.},
	author = {Demiralp, Çagatay},
	year = {2016},
	keywords = {Clustering, Tukey, UserStudy:yes, Visualization, cluster:Visualization Tools, dimensionality reduction, experiment, forward projection, in- teractive analytics., layer:User Interaction, out-of-sample extension, projection, prolines, sampling, scalable visualization, supercluster:Data Visualization, type:Proposal of Solution},
}

@article{Deutch2016,
	title = {Learning {Queries} from {Examples} and {Their} {Explanations}},
	abstract = {To assist non-specialists in formulating database queries, multiple frameworks that automatically infer queries from a set of examples have been proposed. While highly useful, a shortcoming of the approach is that if users can only provide a small set of examples, many inherently different queries may qualify, and only some of these actually match the user intentions. Our main observation is that if users further explain their examples, the set of qualifying queries may be significantly more focused. To capture explanations, we leverage previously developed models of data provenance, and in particular "embedding" in the model of provenance semirings. An important advantage is that the obtained problem definition is generic and allows plugging-in explanation models of different levels of detail and granularity. We highlight several modeling and computational challenges in the context of the problem, and address them to develop efficient algorithms that infer conjunctive queries from examples and their explanations. We show the computational efficiency of the algorithms and favorable properties of inferred queries through a theoretical analysis as well as an experimental study using the TPC-H benchmark. Our experiments indicate that even when shown only few examples and their explanations, our system succeeds in "reverse engineering" many highly complex queries.},
	journal = {arXiv preprint arXiv:1602.03819},
	author = {Deutch, Daniel and Gilad, Amir},
	year = {2016},
	note = {arXiv: 1602.03819},
	keywords = {UserStudy:yes, cluster:Automatic Exploration, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
}

@inproceedings{Wu2017a,
	title = {Combining {Design} and {Performance} in a {Data} {Visualization} {Management} {System}},
	abstract = {Interactive data visualizations have emerged as a prominent way to bring data exploration and analysis capabilities to both techni-cal and non-technical users. Despite their ubiquity and importance across applications, multiple design-and performance-related chal-lenges lurk beneath the visualization creation process. To meet these challenges, application designers either use visualization systems (e.g., Endeca, Tableau, and Splunk) that are tailored to domain-specific analyses, or manually design, implement, and optimize their own solutions. Unfortunately, both approaches typically slow down the creation process. In this paper, we describe the status of our progress towards an end-to-end relational approach in our data visualization management system (DVMS). We introduce DeVIL, a SQL-like language to express static as well as interactive visualizations as database views that combine user inputs modeled as event streams and database relations, and we show that DeVIL can express a range of interaction techniques across several taxonomies of interactions. We then describe how this relational lens enables a number of new functionalities and system design directions and highlight several of these directions. These include (a) the use of provenance queries to express and optimize interactions, (b) the application of concurrency control ideas to interactions, (c) a streaming framework to improve near-interactive visualizations, and (d) techniques to synthesize interactive interfaces tailored to end-users.},
	booktitle = {{CIDR}'17},
	author = {Wu, Eugene and Psallidas, Fotis and Miao, Zhengjie and Zhang, Haoci and Rettig, Laura and Wu, Yifan and Sellam, Thibault},
	year = {2017},
	keywords = {cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
}

@inproceedings{Chaudhuri2015,
	title = {Information at your {Fingertips}: {Only} a dream for enterprises?},
	volume = {2015-May},
	isbn = {978-1-4799-7963-9},
	url = {http://ieeexplore.ieee.org/document/7113266/},
	doi = {10.1109/ICDE.2015.7113266},
	abstract = {We review how the state of information technology has evolved for consumers vs. enterprises. We discuss some of the key challenges in enterprise search over structured data and suggest a few promising directions for the research community.},
	booktitle = {Proceedings - {International} {Conference} on {Data} {Engineering}},
	author = {Chaudhuri, Surajit},
	year = {2015},
	note = {ISSN: 10844627},
	pages = {1--4},
}

@inproceedings{Alabi2016,
	title = {{PFunk}-{H}: approximate query processing using perceptual models.},
	isbn = {978-1-4503-4207-0},
	url = {https://dl.acm.org/citation.cfm?id=2939512},
	doi = {10.1145/2939502.2939512},
	abstract = {Interactive visualization tools (e.g., crossfilter) are critical to many data analysts by making the discovery and verification of hypotheses quick and seamless. Increasing data sizes has made the scalability of these tools a necessity. To bridge the gap between data sizes and interactivity, many visualization systems have turned to sampling-based approximate query processing frameworks. However, these systems are currently oblivious to human perceptual visual accuracy. This could either lead to overly aggressive sampling when the approximation accuracy is higher than needed or an incorrect visual rendering when the accuracy is too lax. Thus, for both correctness and efficiency, we propose to use empirical knowledge of human perceptual limitations to automatically bound the error of approximate answers meant for visualization. This paper explores a preliminary model of sampling-based approximate query processing that uses perceptual models (encoded as functions) to construct approximate answers intended for visualization. We present initial results that show that the approximate and non-approximate answers for a given query differ by a perceptually indiscernible amount, as defined by perceptual functions.},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	author = {Alabi, Daniel and 0002, Eugene Wu},
	year = {2016},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {10},
}

@inproceedings{Siddiqui2017,
	title = {Fast-{Forwarding} to {Desired} {Visualizations} with zenvisage},
	isbn = {978-1-4503-2138-9},
	doi = {10.1145/1235},
	abstract = {Data exploration and analysis, especially for non-programmers, remains a tedious and frustrating process of trial-and-error—data scientists spend many hours poring through visualizations in the hope of finding those that match desired patterns. We demonstrate zen-visage, an interactive data exploration system tailored towards "fast-forwarding " to desired trends, patterns, or insights, without much effort from the user. zenvisage's interface supports simple drag-and-drop and sketch-based interactions as specification mechanisms for the exploration need, as well as an intuitive data exploration language called ZQL for more complex needs. zenvisage is being developed in collaboration with ad analysts, battery scientists, and genomic data analysts, and will be demonstrated on similar datasets.},
	booktitle = {{CIDR}'17},
	author = {Siddiqui, Tarique and Lee, John and Kim, Albert and Xue, Edward and Wang, Chaoran and Zou, Yuxuan and Guo, Lijin and Liu, Changfeng and Yu, Xiaofo and Karahalios, Karrie and Parameswaran, Aditya},
	year = {2017},
	keywords = {cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
}

@inproceedings{Ehsan2016,
	title = {{MuVE}: {Efficient} {Multi}-{Objective} {View} {Recommendation} for {Visual} {Data} {Exploration}},
	isbn = {978-1-5090-2019-5},
	url = {http://ieeexplore.ieee.org/document/7498285/},
	doi = {10.1109/ICDE.2016.7498285},
	abstract = {—To support effective data exploration, there is a well-recognized need for solutions that can automatically rec-ommend interesting visualizations, which reveal useful insights into the analyzed data. However, such visualizations come at the expense of high data processing costs, where a large number of views are generated to evaluate their usefulness. Those costs are further escalated in the presence of numerical dimensional attributes, due to the potentially large number of possible binning aggregations, which lead to a drastic increase in the number of possible visualizations. To address that challenge, in this paper we propose the MuVE scheme for Multi-Objective View Recommendation for Visual Data Exploration. MuVE introduces a hybrid multi-objective utility function, which captures the impact of binning on the utility of visualizations. Consequently, novel algorithms are proposed for the efficient recommendation of data visualizations that are based on numerical dimensions. The main idea underlying MuVE is to incrementally and progressively assess the different benefits provided by a visualization, which allows an early pruning of a large number of unnecessary operations. Our extensive experimental results show the significant gains provided by our proposed scheme.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Ehsan, Humaira and Sharaf, Mohamed A. and Chrysanthis, Panos K.},
	year = {2016},
	keywords = {cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {731--742},
}

@inproceedings{Panev2016,
	title = {Computing similar entity rankings via reverse engineering of top-k database queries},
	isbn = {978-1-5090-2108-6},
	url = {http://ieeexplore.ieee.org/document/7495643/},
	doi = {10.1109/ICDEW.2016.7495643},
	abstract = {Ranked lists are an essential methodology to succinctly summarize outstanding items, computed over database tables or crowdsourced in dedicated websites. Previous work has already addressed the problem of reverse engineering top-k queries over a database. However, existing systems fail to return any answer to the user when a precisely matching query has not been found. In this work, we tackle this problem of determining queries that compute lists similar to a user-specified input ranking. More precisely, for a ranked list of entities L and a similarity threshold θ, we want to find queries that return lists Lr with d(L,Lr) ≤ θ, where d(L,Lr) is the distance between the lists. Through a detailed experimental study we show that our system is able to achieve, in most of the cases, a Recall@10 higher than 80\%.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} {Workshops}, {ICDEW} 2016},
	author = {Panev, Kiril and Milchevski, Evica and Michel, Sebastian},
	year = {2016},
	pages = {181--188},
}

@misc{Liu2016,
	title = {Initial {Sampling} for {Automatic} {Interactive} {Data} {Exploration}},
	abstract = {In many real world applications, users might not know the queries to send to a database in order to retrieve data in the user-interested areas. Users can apply a trial and error method to discover the queries. However, as the data set is usually quite large, the discovery of queries will take a long time and the whole process is labor-intensive. We want to build a discovery-oriented, interactive data exploration system, that guides users to their interested data areas through interactive sample labeling process. In each iteration, the system will strategically select some sample points to present to users for feedback, as relevant or irrelevant, and finally converge to a query that is able to retrieve all the data in the user-interested area. In...},
	author = {Liu, Wenzhao and Diao, Yanlei and Liu, Anna},
	year = {2016},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
}

@inproceedings{eurp.20171155,
	title = {Towards an {Adaptive} {Framework} for {Real}-{Time} {Visualization} of {Streaming} {Big} {Data}},
	isbn = {978-3-03868-044-4},
	url = {https://diglib.eg.org/handle/10.2312/eurp20171155},
	doi = {10.2312/eurp.20171155},
	abstract = {Big data poses new challenges and the need for flexible, interactive, and dynamic visualization techniques. Existing approaches, especially in enterprise data visualization with static graphics or interactive dashboards, are limited at the scale of big data, given the volume and diversity of data to consider. Streaming data further compounds on the problem with the need for real-time analytics and visualizations. On the data acquisition and collection side of things, traditional business analytics platforms are being extended with support for technologies such as Apache Spark for improvement in performance. However, for real-time data visualization for streaming data, it is necessary to go beyond Apache Spark with in-memory processing and new data visualization idioms. We propose a framework for the dynamic visualization of real-time streaming big data, resilient to both its volume and rate of change. Some of the different directions we explore include: (a) the efficient processing and consumption of streaming data; (b) the automated detection of relevant changes in the data stream, highlighting entities that merit a detailed analysis; (c) the choice of the best idioms to visualize big data, possibly leading to the development of new visualization idioms; (d) real-time visualization changes.},
	booktitle = {{EuroVis} 2017 - {Posters}},
	publisher = {The Eurographics Association},
	author = {Khan, Amin M and Gonçalves, Daniel and Leão, Duarte C},
	editor = {Puig, Anna Puig and Isenberg, Tobias},
	year = {2017},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {13--15},
}

@article{Kwon2017,
	title = {Sampling for scalable visual analytics},
	volume = {37},
	issn = {02721716},
	url = {http://ieeexplore.ieee.org/abstract/document/7819391/},
	doi = {10.1109/MCG.2017.6},
	abstract = {The ability to scale interactive visual analysis to massive datasets is becoming increasingly important. We make a case for sampling as an essential tool for scalable interactive visual analysis. We first outline prior work by the database community on sampling for visualization of “aggregation queries” and then consider how these results might be improved and extended to a broader setting. In particular, we discuss issues important to sampling-based visual analytics and delineate three research directions: (1) understanding the interplay between sampling and perception, (2) assessing and visually representing sampling-induced uncertainty, and (3) giving the non-expert user interactive control over the sampling process. More generally, we need to better understand how users interact with sampling to enable wider adoption of sampling for scalable visual analytics.},
	number = {1},
	journal = {IEEE Computer Graphics and Applications},
	author = {Kwon, Bum Chul and Verma, Janu and Haas, Peter J. and Demiralp, Çaǧatay},
	year = {2017},
	keywords = {Interactive visual analytics, Online aggregation, Sampling, Scalable visualization, Visualization, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Evaluation Research},
	pages = {100--108},
}

@inproceedings{Park2016,
	title = {Visualization-aware sampling for very large databases},
	isbn = {978-1-5090-2019-5},
	url = {https://arxiv.org/abs/1510.03921},
	doi = {10.1109/ICDE.2016.7498287},
	abstract = {Interactive visualizations are crucial in ad hoc data exploration and analysis. However, with the growing number of massive datasets, generating visualizations in interactive timescales is increasingly challenging. One approach for improving the speed of the visualization tool is via data reduction in order to reduce the computational overhead, but at a potential cost in visualization accuracy. Common data reduction techniques, such as uniform and stratified sampling, do not exploit the fact that the sampled tuples will be transformed into a visualization for human consumption. We propose a visualization-aware sampling (VAS) that guarantees high quality visualizations with a small subset of the entire dataset. We validate our method when applied to scatter and map plots for three common visualization goals: regression, density estimation, and clustering. The key to our sampling method's success is in choosing tuples which minimize a visualization-inspired loss function. Our user study confirms that optimizing this loss function correlates strongly with user success in using the resulting visualizations. We also show the NP-hardness of our optimization problem and propose an efficient approximation algorithm. Our experiments show that, compared to previous methods, (i) using the same sample size, VAS improves user's success by up to 35\% in various visualization tasks, and (ii) VAS can achieve a required visualization quality up to 400 times faster.},
	booktitle = {2016 {IEEE} 32nd {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Park, Yongjoo and Cafarella, Michael and Mozafari, Barzan},
	year = {2016},
	note = {arXiv: 1510.03921},
	keywords = {RInteractive: Yes, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {755--766},
}

@inproceedings{Ding2016,
	title = {Sample + {Seek} : {Approximating} {Aggregates} with {Distribution} {Precision} {Guarantee}},
	isbn = {978-1-4503-3531-7},
	url = {https://dl.acm.org/citation.cfm?id=2915249},
	doi = {10.1145/2882903.2915249},
	abstract = {Data volumes are growing exponentially for our decision-support systems making it challenging to ensure interactive response time for ad-hoc queries without increasing cost of hardware. Aggregation queries with Group By that produce an aggregate value for every combination of values in the grouping columns are the most important class of ad-hoc queries. As small errors are usually tolerable for such queries, approximate query processing (AQP) has the potential to answer them over very large datasets much faster. In many cases analysts require the distribution of (group, aggvalue) pairs in the estimated answer to be guaranteed within a certain er- ror threshold of the exact distribution. Existing AQP techniques are inadequate for two main reasons. First, users cannot express such guarantees. Second, sampling techniques used in traditional AQP can produce arbitrarily large errors even for SUM queries. To address those limitations, we first introduce a new precision met- ric, called distribution precision, to express such error guarantees. We then study how to provide fast approximate answers to aggre- gation queries with distribution precision guaranteed within a user- specified error bound. The main challenges are to provide rigorous error guarantees and to handle arbitrary highly selective predicates without maintaining large-sized samples. We propose a novel sam- pling scheme called measure-biased sampling to address the former challenge. For the latter, we propose two new indexes to augment in-memory samples. Like other sampling-based AQP techniques, our solution supports any aggregate that can be estimated from ran- dom samples. In addition to deriving theoretical guarantees, we conduct experimental study to compare our system with state-of- the-art AQP techniques and a commercial column-store database system on both synthetic and real enterprise datasets. Our system provides a median speed-up of more than 100x with around 5\% distribution error compared with the commercial database.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data}},
	author = {Ding, Bolin and Huang, Silu and Chaudhuri, Surajit and Chakrabarti, Kaushik and Wang, Chi},
	year = {2016},
	note = {ISSN: 07308078},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {679--694},
}

@article{Li2016,
	title = {Wander {Join}: {Online} {Aggregation} via {Random} {Walks}},
	issn = {07308078},
	url = {http://doi.acm.org/10.1145/2882903.2915235},
	doi = {10.1145/2882903.2915235},
	abstract = {Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This paper proposes a new approach, the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori. Compared with ripple join, wander join is particularly efficient for equality joins involving multiple tables, but also supports θ-joins. Selection predicates and group-by clauses can be handled as well. Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join. In particular, we have integrated and tested wander join in the latest version of PostgreSQL, demonstrating its practicality in a full-edged database system. © 2016 ACM.},
	journal = {Proceedings of the 2016 International Conference on Management of Data},
	author = {Li, Feifei and Wu, Bin and Yi, Ke and Zhao, Zhuoyue},
	year = {2016},
	note = {ISBN: 978-1-4503-3531-7},
	keywords = {joins, online aggregation, random walks},
	pages = {615--629},
}

@article{Islam2013,
	title = {A framework for query refinement with user feedback},
	volume = {86},
	issn = {01641212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121213000265},
	doi = {10.1016/j.jss.2013.01.069},
	abstract = {SQL queries in the existing relational data model implement the binary satisfaction of tuples. That is, a data tuple is filtered out from the result set if it does not satisfy the constraints expressed in the predicates of the user submitted query. Posing appropriate queries for ordinary users is very difficult in the first place if they lack knowledge of the underlying dataset. Therefore, imprecise queries are commonplace for many users. In connection with this, this paper presents a framework for capturing user intent through feedback for refining the initial imprecise queries that can fulfill the users' information needs. The feedback in our framework consists of both unexpected tuples currently present in the query output and expected tuples that are missing from the query output. We show that our framework does not require users to provide the complete set of feedback tuples because only a subset of this feedback can suffice. We provide the point domination theory to complement the other members of feedback. We also provide algorithms to handle both soft and hard requirements for the refinement of initial imprecise queries. Experimental results suggest that our approach is promising compared to the decision tree based query refinement approach. © 2013 Elsevier Inc. All rights reserved.},
	number = {6},
	journal = {Journal of Systems and Software},
	author = {Islam, Md Saiful and Liu, Chengfei and Zhou, Rui},
	year = {2013},
	note = {ISBN: 0164-1212},
	keywords = {Imprecise query, Query refinement, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1580--1595},
}

@article{Bidoit2014,
	title = {Query-{Based} {Why}-{Not} {Provenance} with {NedExplain}},
	doi = {10.5441/002/edbt.2014.14},
	abstract = {With the increasing amount of available data and transformations manipulating the data, it has become essential to analyze and de- bug data transformations. A sub-problem of data transformation analysis is to understand why some data are not part of the result of a relational query. One possibility to explain the lack of data in a query result is to identify where in the query we lost data pertinent to the expected outcome. A first approach to this so called why-not provenance has been recently proposed, but we show that this first approach has some shortcomings.{\textbackslash}nTo overcome these shortcomings, we propose NedExplain, an algorithm to explain data missing from a query result. NedExplain computes the why-not provenance for monotone relational queries with aggregation. After providing necessary definitions, this paper contributes a detailed description of the algorithm. A comparative evaluation shows that it is both more efficient and effective than the state-of-the-art approach.},
	number = {c},
	journal = {Intl. Conf. on Extending Database Technology (EDBT)},
	author = {Bidoit, Nicole and Herschel, Melanie and Tzompanaki, Katerina},
	year = {2014},
	note = {ISBN: 9783893180653},
	pages = {145--156},
}

@article{Schulz2016,
	title = {An enhanced visualization process model for incremental visualization},
	volume = {22},
	issn = {10772626},
	url = {http://ieeexplore.ieee.org/document/7172541/},
	doi = {10.1109/TVCG.2015.2462356},
	abstract = {With today’s technical possibilities, a stable visualization scenario can no longer be assumed as a matter of course, as underlying data and targeted display setup are much more in flux than in traditional scenarios. Incremental visualization approaches are a means to address this challenge, as they permit the user to interact with, steer, and change the visualization at intermediate time points and not just after it has been completed. In this paper, we put forward a model for incremental visualizations that is based on the established Data State Reference Model, but extends it in ways to also represent partitioned data and visualization operators to facilitate intermediate visualization updates. In combination, partitioned data and operators can be used independently and in combination to strike tailored compromises between output quality, shown data quantity, and responsiveness—i.e., frame rates. We showcase the new expressive power of this model by discussing the opportunities and challenges of incremental visualization in general and its usage in a real world scenario in particular.},
	number = {7},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Schulz, Hans Jorg and Angelini, Marco and Santucci, Giuseppe and Schumann, Heidrun},
	year = {2016},
	pmid = {27244708},
	note = {ISBN: 1077-2626},
	keywords = {Visualization pipeline, cluster:Visual Optimizations, data state reference model, layer:User Interaction, proactive visualization, progressive visualization, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1830--1842},
}

@inproceedings{Yasir2012b,
	title = {Exploiting schema and documentation for summarizing relational databases},
	volume = {7678 LNCS},
	isbn = {978-3-642-35541-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-35542-4_7},
	doi = {10.1007/978-3-642-35542-4_7},
	abstract = {Schema summarization approaches are used for carrying out schema matching and developing user interfaces. Generating schema summary for any given database is a challenge which involves identifying semantically correlated elements in a database schema. Research efforts are being made to propose schema summarization approaches by exploiting database schema and data stored in the database. In this paper, we have made an effort to propose an efficient schema summarization approach by exploiting database schema and the database documentation. We propose a notion of table similarity by exploiting referential relationship between tables and the similarity of passages describing the corresponding tables in the database documentation. Using the notion of table similarity, we propose a clustering based approach for schema summary generation. Experimental results on a benchmark database show the effectiveness of the proposed approach. © Springer-Verlag 2012.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {Yasir, Ammar and Swamy, Mittapally Kumara and Reddy, Polepalli Krishna},
	year = {2012},
	note = {ISSN: 03029743},
	keywords = {Database usability, Schema Matching, Schema summarization},
	pages = {77--90},
}

@article{Herschel2013,
	title = {Wondering {Why} {Data} are {Missing} from {Query} {Results}? {Ask} {Conseil} {Why}-{Not}},
	url = {https://dl.acm.org/citation.cfm?id=2505725},
	doi = {10.1145/2505515.2505725},
	abstract = {In analyzing and debugging data transformations, or more specifically relational queries, a subproblem is to understand why some data are not part of the query result. This problem has recently been addressed from different perspectives for various fragments of relational queries. The different perspectives yield different, yet complementary explanations of such missing-answers. This paper first aims at unifying the different approaches by defining a new type of explanation, called hybrid explanation, that encompasses the variety of previously defined types of explanations. This solution goes beyond simply forming the union of explanations produced by different algorithms and is shown to be able to explain a larger set of missing-answers. Second, we present Conseil, an algorithm to generate hybrid explanations. Conseil is also the first algorithm to handle non-monotonic queries. Experiments on efficiency and explanation quality show that Conseil is comparable to and even outperforms previous algorithms. Copyright 2013 ACM.},
	journal = {International Conference on Information and Knowledge Management, Proceedings},
	author = {Herschel, Melanie},
	year = {2013},
	note = {ISBN: 9781450322638},
	keywords = {query-analysis},
	pages = {2213--2218},
}

@article{Herschel2015,
	title = {A {Hybrid} {Approach} to {Answering} {Why}-{Not} {Questions} on {Relational} {Query} {Results}},
	volume = {5},
	issn = {19361955},
	url = {http://dl.acm.org/citation.cfm?doid=2698232.2665070},
	doi = {10.1145/2665070},
	abstract = {In analyzing and debugging data transformations, or more specifically relational queries, a subproblem is to understand why some data are not part of the query result. This problem has recently been addressed from different perspectives for various fragments of relational queries. The different perspectives yield different yet complementary explanations of such missing answers. This article first aims at unifying the different approaches by defining a new type of explanation, called hybrid explanation, that encompasses the variety of previously defined types of explanations. This solution goes beyond simply forming the union of explanations produced by different algorithms and is shown to be able to explain a larger set of missing answers. Second, we present Conseil, an algorithm to generate hybrid explanations. Conseil is also the first algorithm to handle nonmonotonic queries. Experiments on efficiency and explanation quality show that Conseil is comparable and even outperforms previous algorithms. This article extends a previous short conference paper by providing proofs, additional theorems, and a detailed discussion of each step of the Conseil algorithm. It also significantly extends the experimental evaluation on efficiency and explanation quality. © 2015 ACM.},
	number = {3},
	journal = {Journal of Data and Information Quality},
	author = {Herschel, Melanie},
	year = {2015},
	pages = {1--29},
}

@article{Bonifati2015,
	title = {Learning {Path} {Queries} on {Graph} {Databases}},
	doi = {10.5441/002/edbt.2015.11},
	abstract = {We investigate the problem of learning graph queries by exploiting user examples. The input consists of a graph database in which the user has labeled a few nodes as posi-tive or negative examples, depending on whether or not she would like the nodes as part of the query result. Our goal is to handle such examples to find a query whose output is what the user expects. This kind of scenario is pivotal in several application settings where unfamiliar users need to be assisted to specify their queries. In this paper, we focus on path queries defined by regular expressions, we identify fundamental difficulties of our problem setting, we formal-ize what it means to be learnable, and we prove that the class of queries under study enjoys this property. We ad-ditionally investigate an interactive scenario where we start with an empty set of examples and we identify the informa-tive nodes i.e., those that contribute to the learning process. Then, we ask the user to label these nodes and iterate the learning process until she is satisfied with the learned query. Finally, we present an experimental study on both real and synthetic datasets devoted to gauging the effectiveness of our learning algorithm and the improvement of the interactive approach.},
	journal = {International Conference on Extending Database Technology (EDBT)},
	author = {Bonifati, Angela and Ciucanu, Radu and Lemay, Aurélien},
	year = {2015},
	note = {ISBN: 9783893180677},
	keywords = {cluster:Automatic Exploration, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {109--120},
}

@article{Elmore2015,
	title = {A demonstration of the {BigDAWG} polystore system},
	volume = {8},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2824032.2824098},
	doi = {10.14778/2824032.2824098},
	abstract = {This paper presents BigDAWG, a reference implementation of a new architecture for "Big Data" applications. Such applications not only call for large-scale analytics, but also for real-time streaming support, smaller analytics at interactive speeds, data visualization, and cross-storage-system queries. Guided by the principle that "one size does not fit all " , we build on top of a variety of storage engines, each designed for a specialized use case. To illustrate the promise of this approach, we demonstrate its effective-ness on a hospital application using data from an intensive care unit (ICU). This complex application serves the needs of doctors and researchers and provides real-time support for streams of patient data. It showcases novel approaches for querying across multiple storage engines, data visualization, and scalable real-time analytics.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Elmore, A. and Kraska, T. and Madden, S. and Maier, D. and Mattson, T. and Papadopoulos, S. and Parkhurst, J. and Tatbul, N. and Vartak, M. and Zdonik, S. and Duggan, J. and Stonebraker, M. and Balazinska, M. and Cetintemel, U. and Gadepally, V. and Heer, J. and Howe, B. and Kepner, J.},
	year = {2015},
	pages = {1908--1911},
}

@inproceedings{McCamish2016,
	title = {A {Signaling} {Game} {Approach} to {Databases} {Querying} and {Interaction}},
	volume = {1644},
	isbn = {978-1-4503-3833-2},
	url = {https://dl.acm.org/citation.cfm?id=2809487},
	doi = {10.1145/2808194.2809487},
	abstract = {As most database users cannot precisely express their information needs, it is challenging for database management systems to understand them. We propose a novel formal framework for representing and understanding information needs in database querying and exploration. Our framework considers querying as a collaboration between the user and the database management system to establish a it mutual language for representing information needs. We formalize this collaboration as a signaling game, where each mutual language is an equilibrium for the game. A query interface is more effective if it establishes a less ambiguous mutual language faster. We discuss some equilibria, strategies, and the convergence in this game. In particular, we propose a reinforcement learning mechanism and analyze it within our framework. We prove that this adaptation mechanism for the query interface improves the effectiveness of answering queries stochastically speaking, and converges almost surely. We extend out results for the cases that the user also modifies her strategy during the interaction.},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {The} {Theory} of {Information} {Retrieval}},
	author = {Termehchy, Arash and Touri, Behrouz},
	year = {2016},
	note = {arXiv: 1603.04068
ISSN: 16130073},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {361--364},
}

@article{Brereton2007,
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	issn = {01641212},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone. ?? 2006 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	year = {2007},
	note = {ISBN: 0164-1212},
	keywords = {Empirical software engineering, Systematic Reviews},
	pages = {571--583},
}

@inproceedings{Abouzied:2013:ILA:2452376.2452377,
	address = {New York, NY, USA},
	title = {Invisible loading},
	isbn = {978-1-4503-1597-5},
	url = {http://dl.acm.org/citation.cfm?id=2452376.2452377},
	doi = {10.1145/2452376.2452377},
	abstract = {Commercial analytical database systems suffer from a high "time-to-first-analysis": before data can be processed, it must be modeled and schematized (a human effort), transferred into the database's storage layer, and optionally clustered and indexed (a computational effort). For many types of structured data, this upfront effort is unjustifiable, so the data are processed directly over the file system using the Hadoop framework, despite the cumulative performance benefits of processing this data in an analytical database system. In this paper we describe a system that achieves the immediate gratification of running MapReduce jobs directly over a file system, while still making progress towards the long-term performance benefits of database systems. The basic idea is to piggyback on MapReduce jobs, leverage their parsing and tuple extraction operations to incrementally load and organize tuples into a database system, while simultaneously processing the file system data. We call this scheme Invisible Loading, as we load fractions of data at a time at almost no marginal cost in query latency, but still allow future queries to run much faster.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {ACM},
	author = {Abouzied, Azza and Abadi, Daniel J. and Silberschatz, Avi},
	year = {2013},
	note = {Series Title: EDBT '13},
	keywords = {cluster:Adaptive Indexing, cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, supercluster:Indexes, type:Proposal of Solution},
	pages = {1--10},
}

@article{Nandi2011,
	title = {Guided {Interaction}: {Rethinking} the {Query}-{Result} {Paradigm}},
	volume = {4},
	issn = {21508097},
	abstract = {Many decades of research, coupled with continuous increases in computing power, have enabled highly efficient execution of queries on large databases. In consequence, for many databases, far more time is spent by users formulating queries than by the system evaluating them. It stands to reason that, looking at the overall query experience we provide users, we should pay attention to how we can assist users in the holistic process of obtaining the information they desire from the database, and not just the constituent activity of efficiently generating a result given a complete precise query. In this paper, we examine the conventional query-result paradigm employed by databases and demonstrate challenges encountered when following this paradigm for an information seeking task. We recognize that the process of query specification itself is a major stumbling block. With current computational abilities, we are at a point where we can make use of the data in the database to aid in this process. To this end, we propose a new paradigm, guided interaction, to solve the noted challenges, by using interaction to guide the user through the query formulation, query execution and result examination processes. The user can be given advance information during query specification that can not only assist in query formulation, but may also lead to abandonment of an unproductive query direction or the satisfaction of information need even before the query specification is complete. There are significant engineering challenges to constructing the system we envision, and the technological building blocks to address these challenges exist today.},
	number = {12},
	journal = {Vldb},
	author = {Nandi, Arnab and Jagadish, H V},
	year = {2011},
	pages = {1466--1469},
}

@inproceedings{Zoumpatianos2014,
	title = {Indexing for interactive exploration of big data series},
	isbn = {07308078 (ISSN); 9781450323765 (ISBN)},
	url = {https://dl.acm.org/citation.cfm?id=2610498},
	doi = {10.1145/2588555.2610498},
	abstract = {Numerous applications continuously produce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available, which is not currently possible with the state-of-the-art indexing methods and for very large data series collections. In this paper, we present the first adaptive indexing mechanism, specifically tailored to solve the problem of indexing and querying very large data series collections. The main idea is that instead of building the complete index over the complete data set up-front and querying only later, we interactively and adaptively build parts of the index, only for the parts of the data on which the users pose queries. The net effect is that instead of waiting for extended periods of time for the index creation, users can immediately start exploring the data series. We present a detailed design and evaluation of adaptive data series indexing over both synthetic data and real-world workloads. The results show that our approach can gracefully handle large data series collections, while drastically reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), adaptive data series indexing has already answered 3 * 105 queries.},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Zoumpatianos, K and Idreos, S and Palpanas, T},
	year = {2014},
	keywords = {Big Data, Detailed design, Index creation, Indexing (of information), Indexing methods, Indexing techniques, Interactive exploration, RDistributed: Yes, Synthetic data, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1555--1566},
}

@misc{Yu2010,
	title = {Keyword {Search} in {Relational} {Databases}: {A} {Survey}},
	abstract = {The integration of DB and IR provides flexible ways for users to query information in the same platform [6, 2, 3, 7, 5, 28]. On one hand, the sophisticated DB facilities provided by RDBMSs assist users to query well-structured information using SQL. On the other hand, IR techniques allow users to search unstructured information using keywords based on scoring and ranking, and do not need users to understand any database schemas.},
	author = {Yu, Jeffrey Xu and Qin, Lu and Chang, Lijun},
	year = {2010},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Evaluation Research},
}

@article{Wu2012a,
	title = {The {Case} for {Data} {Visualization} {Management} {Systems} [ {Vision} {Paper} ]},
	issn = {21508097},
	doi = {10.14778/2732951.2732964},
	abstract = {Most visualizations today are produced by retrieving data from a database and using a specialized visualization tool to render it. This decoupled approach results in significant duplication of functionality, such as aggregation and filters, and misses tremendous opportunities for cross-layer optimizations. In this paper, we present the case for an in-tegrated Data Visualization Management System (DVMS) based on a declarative visualization language that fully com-piles the end-to-end visualization pipeline into a set of rela-tional algebra queries. Thus the DVMS can be both expressive via the visualization language, and performant by leveraging traditional and visualization-specific optimizations to scale interactive visualizations to massive datasets.},
	journal = {The VLDB Journal},
	author = {Wu, Eugene and Battle, Leilani and Madden, Samuel R},
	year = {2012},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {903--906},
}

@inproceedings{Vieira2011,
	title = {On query result diversification},
	isbn = {978-1-4244-8958-9},
	url = {http://ieeexplore.ieee.org/document/5767846/},
	doi = {10.1109/ICDE.2011.5767846},
	abstract = {In this paper we describe a general framework for evaluation and optimization of methods for diversifying query results. In these methods, an initial ranking candidate set produced by a query is used to construct a result set, where elements are ranked with respect to relevance and diversity features, i.e., the retrieved elements should be as relevant as possible to the query, and, at the same time, the result set should be as diverse as possible. While addressing relevance is relatively simple and has been heavily studied, diversity is a harder problem to solve. One major contribution of this paper is that, using the above framework, we adapt, implement and evaluate several existing methods for diversifying query results. We also propose two new approaches, namely the Greedy with Marginal Contribution (GMC) and the Greedy Randomized with Neighborhood Expansion (GNE) methods. Another major contribution of this paper is that we present the first thorough experimental evaluation of the various diversification techniques implemented in a common framework. We examine the methods' performance with respect to precision, running time and quality of the result. Our experimental results show that while the proposed methods have higher running times, they achieve precision very close to the optimal, while also providing the best result quality. While GMC is deterministic, the randomized approach (GNE) can achieve better result quality if the user is willing to tradeoff running time.},
	booktitle = {2011 {IEEE} 27th {International} {Conference} on {Data} {Engineering}},
	author = {Vieira, Marcos R. and Razente, Humberto L. and Barioni, Maria C.N. and Hadjieleftheriou, Marios and Srivastava, Divesh and Traina, Caetano and Tsotras, Vassilis J.},
	year = {2011},
	note = {ISSN: 10844627},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1163--1174},
}

@article{Tran2009,
	title = {Query by output},
	url = {https://dl.acm.org/citation.cfm?doid=1559845.1559902},
	doi = {10.1145/1559845.1559902},
	abstract = {It has recently been asserted that the usability of a database is as important as its capability. Understanding the database schema, the hidden relationships among attributes in the data all play an important role in this context. Subscribing to this viewpoint, in this paper, we present a novel data-driven approach, called Query By Output (QBO), which can enhance the usability of database systems. The central goal of QBO is as follows: given the output of some query Q on a database D, denoted by Q(D), we wish to construct an alternative query Q? such that Q(D) and Q? (D) are instance-equivalent. To generate instance-equivalent queries from Q(D), we devise a novel data classification-based technique that can handle the at-least-one semantics that is inherent in the query derivation. In addition to the basic framework, we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility. Our framework is evaluated comprehensively on three real data sets and the results show that the instance-equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities.},
	journal = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
	author = {Tran, Quoc Trung and Chan, Chee-Yong and Parthasarathy, Srinivasan},
	year = {2009},
	note = {ISBN: 9781605585512},
	keywords = {cluster:Assisted Query Formulation, instance-equivalent queries, layer:User Interaction, query by output, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {535--548},
}

@article{Sellam,
	title = {Meet {Charles}, big data query advisor},
	abstract = {In scientific data management and business analytics, the most informative queries are a holy grail. Data collection becomes increasingly simpler, yet data exploration gets significantly harder. Exploratory querying is likely to return an empty or an overwhelming result set. On the other hand, data mining algorithms require extensive preparation, ample time and do not scale well. In this paper, we address this challenge at its core, i.e., how to query the query space associated with a given database. The space considered is formed by conjunctive predicates. To express them, we introduce the Segmentation Description Language (SDL). The user provides a query. Charles, our query advisory system, breaks its extent into meaningful segments and returns the subsequent SDL descriptions. This provides insight into the set described and offers the user directions for further exploration. We introduce a novel algorithm to generate SDL answers. We evaluate them using four orthogonal criteria: homogeneity, simplicity, breadth, and entropy. A prototype implementation has been constructed and the landscape of follow-up research is sketched.},
	journal = {CIDR'13},
	author = {Sellam, Thibault and Kersten, Martin},
	year = {2013},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
}

@inproceedings{Sidirourgos2013,
	title = {Scientific discovery through weighted sampling},
	isbn = {978-1-4799-1292-6},
	url = {http://ieeexplore.ieee.org/document/6691587/},
	doi = {10.1109/BigData.2013.6691587},
	abstract = {Scientific discovery has shifted from being an exercise of theory and computation, to become the exploration of an ocean of observational data. Scientists explore data originated from modern scientific instruments in order to discover interesting aspects of it and formulate their hypothesis. Such workloads press for new database functionality. We aim at sampling scientific databases to create many different impressions of the data, on which the scientists can quickly evaluate exploratory queries. However, scientific databases introduce different challenges for sample construction compared to classical business analytical applications. We propose adaptive weighted sampling as an alternative to uniform sampling. With weighted sampling only the most informative data is being sampled, thus more relevant data to the scientific discovery is available to examine a hypothesis. Relevant data is considered to be the focal points of the scientific search, and can be defined either a priori with the use of functions, or by monitoring the query workload. We study such query workloads, and we detail different families of weight functions. Finally, we give a quantitative and qualitative evaluation of weighted sampling.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Big} {Data}},
	author = {Sidirourgos, Lefteris and Kersten, Martin and Boncz, Peter},
	year = {2013},
	keywords = {Astronomy, Sampling, Scientific Computing, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {300--306},
}

@inproceedings{Shen:2014:DQB:2588555.2593664,
	address = {New York, NY, USA},
	title = {Discovering {Queries} {Based} on {Example} {Tuples}},
	isbn = {978-1-4503-2376-5},
	url = {http://doi.acm.org/10.1145/2588555.2593664},
	doi = {10.1145/2588555.2593664},
	abstract = {An enterprise information worker is often aware of a few example tuples (but not the entire result) that should be present in the output of the query. We study the problem of discovering the minimal project join query that contains the given example tuples in its out- put. Efficient discovery of such queries is challenging. We propose novel algorithms to solve this problem. Our experiments on real- life datasets show that the proposed solution is significantly more efficient compared with naïve adaptations of known techniques.},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Shen, Yanyan and Chakrabarti, Kaushik and Chaudhuri, Surajit and Ding, Bolin and Novik, Lev},
	year = {2014},
	note = {Series Title: SIGMOD '14},
	keywords = {cluster:Assisted Query Formulation, example tuples, filter selection, layer:User Interaction, project join query, pruning, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {493--504},
}

@article{Sidirourgos2011,
	title = {{SciBORQ}: {Scientific} data management with {Bounds} {On} {Runtime} and {Quality}},
	abstract = {Data warehouses underlying virtual observatories stress the capa-bilities of database management systems in many ways. They are filled, on a daily basis, with large amounts of factual information derived from intensive data scrubbing and computational feature extraction pipelines. The predominant data processing techniques focus on parallel loads and map-reduce feature extraction algo-rithms. Querying these huge databases require a sizable computing cluster, while ideally the initial investigation should run interactively, using as few resources as possible. In this paper, we explore a different route, one based on the observation that at any given time only a fraction of the data is of primary value for a specific task. This fraction becomes the focus of scientific reflection through an iterative process of ad-hoc query refinement. Steering through data to facilitate scientific discovery demands guarantees for the query execution time. In addition, strict bounds on errors are required to satisfy the demands of scientific use, such that query results can be used to test hypotheses reliably. We propose SciBORQ, a framework for scientific data exploration that gives precise control over runtime and quality of query answering. We present novel techniques to derive multiple interesting data samples, called impressions. An impression is selected such that the statistical error of a query answer remains low, while the result can be computed within strict time bounds. Impressions differ from previous sampling approaches in their bias towards the focal point of the scientific data exploration, their multi-layer design, and their adaptiveness to shifting query workloads. The ultimate goal is a complete system for scientific data exploration and discovery, capable of producing quality answers with strict error bounds in pre-defined time frames},
	journal = {CIDR'11},
	author = {Sidirourgos, L and Kersten, M and Boncz, P},
	year = {2011},
	keywords = {Astronomy, RInteractive: Yes, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {296--301},
}

@article{Richter2014,
	title = {Towards zero-overhead static and adaptive indexing in {Hadoop}},
	volume = {23},
	issn = {0949877X},
	doi = {10.1007/s00778-013-0332-z},
	abstract = {Several research works have focused on supporting index access in MapReduce systems. These works have allowed users to significantly speed up selective MapReduce jobs by orders of magnitude. However, all these proposals require users to create indexes upfront, which might be a difficult task in certain applications (such as in scientific and social applications) where workloads are evolving or hard to predict. To overcome this problem, we propose LIAH (Lazy Indexing and Adaptivity in Hadoop), a parallel, adaptive approach for indexing at minimal costs for MapReduce systems. The main idea of LIAH is to automatically and incrementally adapt to users' workloads by creating clustered indexes on HDFS data blocks as a byproduct of executing MapReduce jobs. Besides distributing indexing efforts over multiple computing nodes, LIAH also parallelises indexing with both map tasks computation and disk I/O. All this without any additional data copy in main memory and with minimal synchronisation. The beauty of LIAH is that it piggybacks index creation on map tasks, which read relevant data from disk to main memory anyways. Hence, LIAH does not introduce any additional read I/O-costs and exploit free CPU cycles. As a result and in contrast to existing adaptive indexing works, LIAH has a very low (or invisible) indexing overhead, usually for the very first job. Still, LIAH can quickly converge to a complete index, i.e. all HDFS data blocks are indexed. Especially, LIAH can trade early job runtime improvements with fast complete index convergence. We compare LIAH with HAIL, a state-of-the-art indexing technique, as well as with standard Hadoop with respect to indexing overhead and workload performance.},
	number = {3},
	journal = {The VLDB Journal},
	author = {Richter, Stefan and Quiané-Ruiz, Jorge Arnulfo and Schuh, Stefan and Dittrich, Jens},
	year = {2014},
	note = {arXiv: 1212.3480v1},
	keywords = {Big Data, HDFS, Hadoop, Indexing, Map reduce, Physical design, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {469--494},
}

@inproceedings{Qarabaqi2014,
	title = {User-driven refinement of imprecise queries},
	isbn = {978-1-4799-2554-4},
	url = {http://ieeexplore.ieee.org/document/6818355/},
	doi = {10.1109/ICDE.2014.6816711},
	abstract = {With the advent of big data everywhere, the need for new techniques for exploratory search in large databases is magnified. The focus of this work is on less technical users who often query a database through a limited interface. We propose user-driven query refinement as an interactive process to aid users create and refine query conditions. This process is characterized by three key challenges: (1) dealing with incomplete and imprecise query conditions, (2) keeping user effort low, and (3) guaranteeing interactive system response time. We address the first two challenges with a probability-based framework that guides the user to the most important query conditions. To recover from input errors, we introduce the notion of sensitivity and propose efficient algorithms for identifying the most sensitive user-specified query conditions, i.e., those conditions that had the greatest influence on the query results. For the third challenge, we develop techniques to estimate the required probabilities within a given hard realtime limit.},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering} {Workshops}},
	author = {Qarabaqi, Bahar and Riedewald, Mirek},
	year = {2014},
	note = {ISSN: 10844627},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {916--927},
}

@inproceedings{Psallidas:2015:STS:2723372.2749452,
	address = {New York, NY, USA},
	title = {S4: {Top}-k {Spreadsheet}-{Style} {Search} for {Query} {Discovery}},
	isbn = {978-1-4503-2758-9},
	url = {http://doi.acm.org/10.1145/2723372.2749452},
	doi = {10.1145/2723372.2749452},
	abstract = {An enterprise information worker is often aware of a few example tuples that should be present in the output of the query. Query discovery systems have been developed to discover project-join queries that contain the given example tuples in their output. However, they require the output to exactly contain all the example tuples and do not perform any ranking. To address this limitation, we study the problem of efficiently discovering top-k project join queries which approximately contain the given example tuples in their output. We extend our algorithms to incrementally produce results as soon as the user finishes typing/modifying a cell. Our experiments on real-life and synthetic datasets show that our proposed solution is significantly more efficient compared with applying state-of-the-art algorithms.},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Psallidas, Fotis and Ding, Bolin and Chakrabarti, Kaushik and Chaudhuri, Surajit},
	year = {2015},
	note = {Series Title: SIGMOD '15},
	keywords = {UserStudy:yes, cluster:Assisted Query Formulation, example spreadsheet, layer:User Interaction, relevance ranking, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {2001--2016},
}

@inproceedings{Pirk:2014:DCF:2619228.2619232,
	address = {New York, NY, USA},
	title = {Database {Cracking}: {Fancy} {Scan}, {Not} {Poor} {Man}'s {Sort}!},
	isbn = {978-1-4503-2971-2},
	url = {http://doi.acm.org/10.1145/2619228.2619232},
	doi = {10.1145/2619228.2619232},
	abstract = {Database Cracking is an appealing approach to adaptive indexing: on every range-selection query, the data is partitioned using the sup- plied predicates as pivots. The core of database cracking is, thus, pivoted partitioning. While pivoted partitioning, like scanning, re- quires a single pass through the data it tends to have much higher costs due to lower CPU efficiency. In this paper, we conduct an in-depth study of the reasons for the low CPU efficiency of pivoted partitioning. Based on the findings, we develop an optimized version with significantly higher (single-threaded) CPU efficiency. We also develop a number of multi-threaded implementations that are effectively bound by memory bandwidth. Combining all of these optimizations we achieve an implementation that has costs close to or better than an ordinary scan on a variety of systems ranging from low-end (cheaper than \$300) desktop machines to high-end (above \$60,000) servers.},
	booktitle = {Proceedings of the {Tenth} {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	publisher = {ACM},
	author = {Pirk, Holger and Petraki, Eleni and Idreos, Stratos and Manegold, Stefan and Kersten, Martin},
	year = {2014},
	note = {Series Title: DaMoN '14},
	keywords = {cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {4:1--4:8},
}

@article{Neophytou2012,
	title = {{AstroShelf}: understanding the universe through scalable navigation of a galaxy of annotations},
	issn = {07308078},
	url = {http://dl.acm.org/citation.cfm?doid=2213836.2213940},
	doi = {10.1145/2213836.2213940},
	abstract = {This demo presents AstroShelf, our on-going effort to enable astrophysicists to collaboratively investigate celestial objects using data originating from multiple sky surveys, hosted at different sites. The AstroShelf platform combines database and data stream, workflow and visualization technologies to provide a means for querying and displaying telescope images (in a Google Sky manner), visualizations of spectrum data, and for managing annotations. In addition to the user interface, AstroShelf supports a programmatic interface (available as a web service), which allows astrophysicists to incorporate functionality from AstroShelf in their own programs. A key feature is Live Annotations which is the detection and delivery of events or annotations to users in real-time, based on their profiles. We demonstrate the capabilities of AstroShelf through real end-user exploration scenarios (with participation from stargazers in the audience), in the presence of simulated annotation workloads executed through web services. Â© 2012 ACM.},
	journal = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
	author = {Neophytou, Panayiotis and Gheorghiu, Roxana and Hachey, Rebecca and Luciani, Timothy and Bao, Di and Labrinidis, Alexandros and Marai, Elisabeta G. and Chrysanthis, Panos K},
	year = {2012},
	note = {ISBN: 9781450312479},
	keywords = {Astronomy, Big Data, Continuous Workflows, Scientific Computing, Visualization, cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {713--716},
}

@article{Koch2014,
	title = {Abstraction {Without} {Regret} in {Database} {Systems} {Building}: a {Manifesto}},
	volume = {37},
	url = {http://infoscience.epfl.ch/record/197359},
	abstract = {It has been said that all problems in computer science can be solved by adding another level of indirection, except for performance problems, which are solved by removing levels of indirection. Compilers are our tools for removing levels of indirection automatically. However, we do not trust them when it comes to systems building. Most performance-critical systems are built in low-level programming languages such as C. Some of the downsides of this compared to using modern high-level programming languages are very well known: bugs, poor programmer productivity, a talent bottleneck, and cruelty to program- ming language researchers. In the future we might even add suboptimal performance to this list. In this article, I argue that compilers can be competitive with and outperform human experts at low-level database systems programming. Performance-critical database systems are a limited-enough domain for us to encode systems programming skills as compiler optimizations. However, mainstream compil- ers cannot do this: We need to work on optimizing compilers specialized for the systems programming domain. Recent progress makes their creation eminently feasible.},
	number = {1},
	journal = {IEEE Data Engineering Bulletin},
	author = {Koch, Christoph},
	year = {2014},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Philosophical Paper},
	pages = {70--79},
}

@article{Nandi2013,
	title = {Querying {Without} {Keyboards}},
	abstract = {Computing devices that use non-traditional methods to in- teract with data are becoming more popular than those that use keyboard-based interaction. Applications and query in- terfaces for such devices pose a fundamentally different set of workloads on the underlying databases. We characterize these differences and show how today’s query representa- tion and evaluation techniques are not well-suited to these new non-keyboard interaction modalities. We propose a new database architecture to address these issues, and demon- strate that it can be built using already existing components.},
	journal = {Cidr},
	author = {Nandi, Arnab},
	year = {2013},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Philosophical Paper},
	pages = {0--3},
}

@inproceedings{Khan2014,
	address = {New York, NY, USA},
	title = {{DivIDE}: {Efficient} {Diversification} for {Interactive} {Data} {Exploration}},
	isbn = {978-1-4503-2722-0},
	url = {http://doi.acm.org/10.1145/2618243.2618253},
	doi = {10.1145/2618243.2618253},
	abstract = {Today, Interactive Data Exploration (IDE) has become a main constituent of many discovery-oriented applications, in which users repeatedly submit exploratory queries to identify interesting subspaces in large data sets. Returning relevant yet diverse results to such queries provides users with quick insights into a rather large data space. Meanwhile, search results diversification adds additional cost to an already computationally expensive exploration process. To address this challenge, in this paper, we propose a novel diversification scheme called DivIDE, which targets the problem of efficiently diversifying the results of queries posed during data exploration sessions. In particular, our scheme exploits the properties of data diversification functions while leveraging the natural overlap occurring between the results of different queries so that to provide significant reductions in processing costs. Our extensive experimental evaluation on both synthetic and real data sets shows the significant benefits provided by our scheme as compared to existing methods.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {ACM},
	author = {Khan, Hina A and Sharaf, Mohamed A and Albarrak, Abdullah},
	year = {2014},
	note = {Series Title: SSDBM '14},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {15:1--15:12},
}

@inproceedings{Key2012,
	title = {{VizDeck}: self-organizing dashboards for visual analytics},
	isbn = {978-1-4503-1247-9},
	url = {https://dl.acm.org/citation.cfm?id=2213931},
	doi = {10.1145/2213836.2213931},
	abstract = {We present VizDeck, a web-based tool for exploratory visual analytics of unorganized relational data. Motivated by collaborations with domain scientists who search for complex patterns in hundreds of data sources simultaneously, VizDeck automatically recommends appropriate visualizations based on the statistical properties of the data and adopts a card game metaphor to help organize the recommended visualizations into interactive visual dashboard applications in seconds with zero programming. The demonstration allows users to derive, share, and permanently store their own dashboard from hundreds of real science datasets using a production system deployed at the University of Washington.},
	booktitle = {Proceedings of the {2Nd} {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	author = {Key, Alicia and Howe, Bill and Perry, Daniel and Aragon, Cecilia R.},
	year = {2012},
	note = {ISSN: 07308078},
	keywords = {Astronomy, UserStudy:yes, cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {681--684},
}

@inproceedings{Kamat2014,
	title = {Distributed and interactive cube exploration},
	isbn = {978-1-4799-2554-4},
	url = {http://ieeexplore.ieee.org/document/6816674/},
	doi = {10.1109/ICDE.2014.6816674},
	abstract = {Interactive ad-hoc analytics over large datasets has become an increasingly popular use case. We detail the challenges encountered when building a distributed system that allows the interactive exploration of a data cube. We introduce DICE, a distributed system that uses a novel session-oriented model for data cube exploration, designed to provide the user with interactive sub-second latencies for specified accuracy levels. A novel framework is provided that combines three concepts: faceted exploration of data cubes, speculative execution of queries and query execution over subsets of data. We discuss design considerations, implementation details and optimizations of our system. Experiments demonstrate that DICE provides a sub-second interactive cube exploration experience at the billion-tuple scale that is at least 33\% faster than current approaches.},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering}},
	author = {Kamat, Niranjan and Jayachandran, Prasanth and Tunga, Karthik and Nandi, Arnab},
	year = {2014},
	note = {ISSN: 10844627},
	keywords = {RDistributed: Yes, RInteractive: Yes, UserStudy:yes, cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution, ★},
	pages = {472--483},
}

@article{Kalinin2014,
	title = {Interactive {Data} {Exploration} {Using} {Semantic} {Windows}},
	issn = {07308078},
	url = {http://doi.acm.org/10.1145/2588555.2593666},
	doi = {10.1145/2588555.2593666},
	abstract = {We present a new interactive data exploration approach, called Semantic Windows (SW), in which users query for multidimensional " windows " of interest via standard DBMS-style queries enhanced with exploration constructs. Users can specify SWs using (i) shape-based properties, e.g., " iden-tify all 3-by-3 windows " , as well as (ii) content-based proper-ties, e.g., " identify all windows in which the average bright-ness of stars exceeds 0.8 " . This SW approach enables the interactive processing of a host of useful exploratory queries that are difficult to express and optimize using standard DBMS techniques. SW uses a sampling-guided, data-driven search strategy to explore the underlying data set and quickly identify windows of interest. To facilitate human-in-the-loop style interactive processing, SW is optimized to produce online results dur-ing query execution. To control the tension between online performance and query completion time, it uses a tunable, adaptive prefetching technique. To enable exploration of big data, the framework supports distributed computation. We describe the semantics and implementation of SW as a distributed layer on top of PostgreSQL. The experimental results with real astronomical and artificial data reveal that SW can offer online results quickly and continuously with little or no degradation in query completion times.},
	journal = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
	author = {Kalinin, Alexander and Cetintemel, Ugur and Zdonik, Stan},
	year = {2014},
	note = {ISBN: 978-1-4503-2376-5},
	keywords = {cluster:Data Prefetching, data exploration, layer:Middleware, query processing, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {505--516},
}

@inproceedings{Jagadish2011,
	title = {Organic databases},
	volume = {7108 LNCS},
	isbn = {978-3-642-25730-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-25731-5_5},
	doi = {10.1007/978-3-642-25731-5_5},
	abstract = {Databases today are carefully engineered: there is an expensive and deliberate design process, after which a database schema is defined; during this design process, various possible instance examples and use cases are hypothesized and carefully analyzed; finally, the schema is ready and then can be populated with data. All of this effort is a major barrier to database adoption. In this paper, we explore the possibility of organic database creation instead of the traditional engineered approach. The idea is to let the user start storing data in a database with a schema that is just enough to cove the instances at hand. We then support efficient schema evolution as new data instances arrive. By designing the database to evolve, we can sidestep the expensive front-end cost of carefully engineering the design of the database. Indeed, the deliberate design model complicates not only database creation, but also database transformation (i.e., schema mapping). Because traditional schema mapping tasks are carefully engineered with declarative specification hidden beneath complex user interface. In this paper, we also study the issue of organic database transformation, which automatically induces schema mappings from sample target database instances.},
	booktitle = {Databases in {Networked} {Information} {Systems}},
	author = {Jagadish, H. V. and Nandi, Arnab and Qian, Li},
	year = {2011},
	note = {ISSN: 03029743},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Evaluation Research, ★},
	pages = {49--63},
}

@article{Idreos2009,
	title = {Self-organizing {Tuple} {Reconstruction} in {Column}-stores},
	url = {https://dl.acm.org/citation.cfm?doid=1559845.1559878},
	doi = {10.1145/1559845.1559878},
	abstract = {Column-stores gained popularity as a promising physical design alternative. Each attribute of a relation is physically stored as a separate column allowing queries to load only the required attributes. The overhead incurred is on-the-fly tuple reconstruction for multi-attribute queries. Each tuple reconstruction is a join of two columns based on tuple IDs, making it a significant cost component. The ultimate physical design is to have multiple presorted copies of each base table such that tuples are already appropriately orga- nized in multiple different orders across the various columns. This requires the ability to predict the workload, idle time to prepare, and infrequent updates. In this paper, we propose a novel design, partial side- ways cracking, that minimizes the tuple reconstruction cost in a self-organizing way. It achieves performance similar to using presorted data, but without requiring the heavy initial presorting step itself. Instead, it handles dynamic, unpredictable workloads with no idle time and frequent up- dates. Auxiliary dynamic data structures, called cracker maps, provide a direct mapping between pairs of attributes used together in queries for tuple reconstruction. A map is continuously physically reorganized as an integral part of query evaluation, providing faster and reduced data access for future queries. To enable flexible and self-organizing be- havior in storage-limited environments, maps are material- ized only partially as demanded by the workload. Each map is a collection of separate chunks that are individually reor- ganized, dropped or recreated as needed. We implemented partial sideways cracking in an open-source column-store. A detailed experimental analysis demonstrates that it brings significant performance benefits for multi-attribute queries.},
	journal = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
	author = {Idreos, Stratos and Kersten, Martin L. and Manegold, Stefan},
	year = {2009},
	note = {ISBN: 9781605585512},
	keywords = {cluster:Adaptive Indexing, database cracking, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {297--308},
}

@inproceedings{Idreos2007,
	title = {Updating a cracked database},
	isbn = {978-1-59593-686-8},
	url = {https://dl.acm.org/citation.cfm?doid=1247480.1247527},
	doi = {10.1145/1247480.1247527},
	abstract = {A cracked database is a datastore continuously reorganized based on operations being executed. For each query, the data of interest is physically reclustered to speed-up future access to the same, overlapping or even disjoint data. This way, a cracking DBMS self-organizes and adapts itself to the workload. So far, cracking has been considered for static databases only. In this paper, we introduce several novel algorithms for high-volume insertions, deletions and updates against a cracked database. We show that the nice performance properties of a cracked database can be maintained in a dynamic environment where updates interleave with queries. Our algorithms comply with the cracking philosophy, i.e., a table is informed on pending insertions and deletions, but only when the relevant data is needed for query processing just enough pending update actions are applied. We discuss details of our implementation in the context of an open-source DBMS and we show through a detailed ex- perimental evaluation that our algorithms always manage to keep the cost of querying a cracked datastore with pending updates},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Idreos, Stratos and Kersten, Martin L. and Manegold, Stefan},
	year = {2007},
	note = {ISSN: 07308078},
	keywords = {cluster:Adaptive Indexing, database cracking, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {413},
}

@article{Idreos2013,
	title = {{dbTouch}: {Analytics} at your {Fingertips}.},
	abstract = {As we enter the era of data deluge, turning data into knowl- edge has become the major challenge across most sciences and businesses that deal with data. In addition, as we in- crease our ability to create data, more and more people are confronted with data management problems on a daily basis for numerous aspects of every day life. A fundamental need is data exploration through interactive tools, i.e., being able to quickly and effortlessly determine data and patterns of interest. However, modern database systems have not been designed with data exploration and usability in mind; they require users with expert knowledge and skills, while they react in a strict and monolithic way to every user request, resulting in correct answers but slow response times. In this paper, we introduce the vision of a new generation of data management systems, called dbTouch; our vision is to enable interactive and intuitive data exploration via data- base kernels which are tailored for touch-based exploration. No expert knowledge is needed. Data is represented in a visual format, e.g., a column shape for an attribute or a fat rectangle shape for a table, while users can touch those shapes and interact/query with gestures as opposed to firing complex SQL queries. The system does not try to consume all data; instead it analyzes only parts of the data at a time, continuously refining the answers and continuously reacting to user input. Every single touch on a data object can be seen as a request to run an operator or a collection of opera- tors over part of the data. Users react to running results and continuously adjust the data exploration - they continuously determine the data to be processed next by adjusting the di- rection and speed of a gesture, i.e., a collection of touches; the database system does not have control on the data flow anymore. We discuss the various benefits that dbTouch sys- tems bring for data analytics as well as the new and unique challenges for database research in combination with touch interfaces. In addition, we provide an initial architecture, implementation and evaluation (and demo) of a dbTouch prototype over IOs for IPad.},
	journal = {CIDR'13},
	author = {Idreos, Stratos and Liarou, Erietta},
	year = {2013},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
}

@inproceedings{Fan2011,
	title = {Interactive {SQL} query suggestion: {Making} databases user-friendly},
	isbn = {978-1-4244-8958-9},
	url = {http://ieeexplore.ieee.org/document/5767843/},
	doi = {10.1109/ICDE.2011.5767843},
	abstract = {SQL is a classical and powerful tool for querying relational databases. However, it is rather hard for inexperienced users to pose SQL queries, as they are required to be proficient in SQL syntax and have a thorough understanding of the underlying schema. To give users gratification, we propose SQLSUGG, an effective and user-friendly keyword-based method to help various users formulate SQL queries. SQLSUGG suggests SQL queries as users type in keywords, and can save users' typing efforts and help users avoid tedious SQL debugging. To achieve high suggestion effectiveness, we propose queryable templates to model the structures of SQL queries. We propose a template ranking model to suggest templates relevant to query keywords. We generate SQL queries from each suggested template based on the degree of matchings between keywords and attributes. For efficiency, we propose a progressive algorithm to compute top-k templates, and devise an efficient method to generate SQL queries from templates. We have implemented our methods on two real data sets, and the experimental results show that our method achieves high effectiveness and efficiency.},
	booktitle = {2011 {IEEE} 27th {International} {Conference} on {Data} {Engineering}},
	author = {Fan, Ju and Li, Guoliang and Zhou, Lizhu},
	year = {2011},
	note = {ISSN: 10844627},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {351--362},
}

@inproceedings{Dimitriadou2014,
	title = {Explore-by-{Example}: {An} {Automatic} {Query} {Steering} {Framework} for {Interactive} {Data} {Exploration}},
	isbn = {978-1-4503-2376-5},
	url = {https://dl.acm.org/citation.cfm?doid=2588555.2610523},
	doi = {10.1145/2588555.2610523},
	abstract = {Interactive Data Exploration (IDE) is a key ingredient of a diverse set of discovery-oriented applications, including ones from scientific computing and evidence-based medicine. In these applications, data discovery is a highly ad hoc interactive process where users execute numerous exploration queries using varying predicates aiming to balance the trade-off between collecting all relevant information and reducing the size of returned data. Therefore, there is a strong need to support these human-in-the-loop applications by assisting their navigation in the data to find interesting objects. In this paper, we introduce AIDE, an Automatic Interactive Data Exploration framework, that iteratively steers the user towards interesting data areas and predicts a query that retrieves his objects of interest. Our approach leverages relevance feedback on database samples to model user interests and strategically collects more samples to refine the model while minimizing the user effort. AIDE integrates machine learning and data management techniques to provide effective data exploration results (matching the user's interests with high accuracy) as well as high interactive performance. It delivers highly accurate query predictions for very common conjunctive queries with very small user effort while, given a reasonable number of samples, it can predict with high accuracy complex conjunctive queries. Furthermore, it provides interactive performance by limiting the user wait time per iteration to less than a few seconds in average. Our user study indicates that AIDE is a practical exploration framework as it significantly reduces the user effort and the total exploration time compared with the current state-of-the-art approach of manual exploration.},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Dimitriadou, Kyriaki and Papaemmanouil, Olga and Diao, Yanlei},
	year = {2014},
	note = {ISSN: 07308078},
	keywords = {UserStudy:yes, cluster:Automatic Exploration, data exploration, database sampling, layer:User Interaction, or, or distributed, or hard copies of, permission to make digital, query formulation, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {517--528},
}

@article{Cudre-Mauroux2009,
	title = {The {Case} for {RodentStore}, an {Adaptive}, {Declarative} {Storage} {System}},
	abstract = {Recent excitement in the database community surrounding new applications—analytic, scientific, graph, geospatial, etc.—has led to an explosion in research on database storage systems. New storage systems are vital to the database community, as they are at the heart of making database systems perform well in new application domains. Unfortunately, each such system also represents a sub- stantial engineering effort including a great deal of duplication of mechanisms for features such as transactions and caching. In this paper, we make the case for RodentStore, an adaptive and declarative storage system providing a high-level interface for describing the physical representation of data. Specifically, RodentStore uses a declarative storage algebra whereby administrators (or database design tools) specify how a logical schema should be grouped into collections of rows, columns, and/or arrays, and the order in which those groups should be laid out on disk. We describe the key operators and types of our algebra, outline the general architecture of RodentStore, which interprets algebraic expressions to generate a physical representation of the data, and describe the interface between RodentStore and other parts of a database system, such as the query optimizer and executor. We provide a case study of the potential use of RodentStore in representing dense geospatial data collected from a mobile sensor network, showing the ease with which different storage layouts can be expressed using some of our algebraic constructs and the potential performance gains that a RodentStore-built storage system can offer.},
	journal = {CIDR'09},
	author = {Cudre-Mauroux, Philippe and Wu, Eugene and Madden, Samuel},
	year = {2009},
	note = {arXiv: 0909.1779},
	keywords = {cluster:Flexible Engines, decomposition storage model, layer:Database Layer, storage algebra, supercluster:Indexes, type:Proposal of Solution},
}

@article{Cormode2011,
	title = {Synopses for {Massive} {Data}: {Samples}, {Histograms}, {Wavelets}, {Sketches}},
	volume = {4},
	issn = {1931-7883},
	url = {http://www.nowpublishers.com/article/Details/DBS-004},
	doi = {10.1561/1900000004},
	abstract = {Methods for Approximate Query Processing (AQP) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in AQP. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time efficiency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance. We also discuss the trade-offs between the different synopsis types.},
	number = {1-3},
	journal = {Foundations and Trends in Databases},
	author = {Cormode, Graham},
	year = {2011},
	note = {arXiv: ISBN 0-89791-128-8
ISBN: 1601985169, 9781601985163},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1--294},
}

@article{DittrichJens2011,
	title = {Towards a {One} {Size} {Fits} {All} {Database} {Architecture}},
	issn = {{\textless}null{\textgreater}},
	abstract = {We propose a newtype of database system coined OctopusDB. Our approach suggests a unified, one size fits all data processing architecture for OLTP, OLAP, streaming systems, and scan-oriented database systems. OctopusDB radically departs from existing ar- chitectures in the following way: it uses a logical event log as its primary storage structure. To make this approach efficient we in- troduce the concept of Storage Views (SV), i.e. secondary, alter- native physical data representations covering all or subsets of the primary log. OctopusDB (1) allows us to use different types of SVs for different subsets of the data; and (2) eliminates the need to use different types of database systems for different applications. Thus, based on the workload, OctopusDB emulates different types of systems (rowstores, column stores, streaming systems, and more importantly, any hybrid combination of these). This is a feature impossible to achieve with traditional DBMSs.},
	journal = {CIDR '11},
	author = {Dittrich, Jens, Alekh Jindal},
	year = {2011},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {195--198},
}

@article{Drosou2013,
	title = {{YmalDB}: {Exploring} relational databases via result-driven recommendations},
	volume = {22},
	issn = {10668888},
	url = {https://link.springer.com/article/10.1007/s00778-013-0311-4},
	doi = {10.1007/s00778-013-0311-4},
	abstract = {The typical user interaction with a database system is through queries. However, many times users do not have a clear understanding of their information needs or the exact content of the database. In this paper, we propose assisting users in database exploration by recommending to them additional items, called Ymal (“You May Also Like”) results, that, although not part of the result of their original query, appear to be highly related to it. Such items are computed based on the most interesting sets of attribute values, called faSets, that appear in the result of the original query. The interestingness of a faSet is defined based on its frequency in the query result and in the database. Database frequency estimations rely on a novel approach of maintaining a set of representative rare faSets. We have implemented our approach and report results regarding both its performance and its usefulness.},
	number = {6},
	journal = {The VLDB Journal},
	author = {Drosou, Marina and Pitoura, Evaggelia},
	year = {2013},
	keywords = {Data exploration, Faceted search, RInteractive: Yes, Recommendations, UserStudy:yes, cluster:Automatic Exploration, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {849--874},
}

@inproceedings{Bonifati2014,
	title = {Interactive {Inference} of {Join} {Queries}},
	isbn = {978-3-89318-065-3},
	url = {https://hal.inria.fr/hal-00875680},
	doi = {10.5441/002/edbt.2014.41},
	abstract = {We investigate the problem of inferring join queries from user interactions. The user is presented with a set of candidate tuples and is asked to label them as positive or negative depending on whether or not she would like the tuples as part of the join result. The goal is to quickly infer an arbitrary n-ary join predicate across two relations by keeping the number of user interactions as minimal as possible. We assume no prior knowledge of the integrity constraints between the involved relations. This kind of scenario occurs in several application settings, such as data integration, reverse engineering of database queries, and constraint inference. In such scenarios, the database instances may be too big to be skimmed. We explore the search space by using a set of strategies that let us prune what we call "uninformative" tuples, and directly present to the user the informative ones i.e., those that allow to quickly find the goal query that the user has in mind. In this paper, we focus on the inference of joins with equality predicates and we show that for such joins deciding whether a tuple is uninformative can be done in polynomial time. Next, we propose several strategies for presenting tuples to the user in a given order that lets minimize the number of interactions. We show the efficiency and scalability of our approach through an experimental study on both benchmark and synthetic datasets. Finally, we prove that adding projection to our queries makes the problem intractable.},
	booktitle = {International {Conference} on {Extending} {Database} {Technology} ({EDBT})},
	author = {Bonifati, Angela and Ciucanu, Radu and Staworko, Slawek},
	year = {2014},
	note = {Issue: c},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Evaluation Research},
	pages = {451--462},
}

@inproceedings{Battle2013,
	title = {Dynamic reduction of query result sets for interactive visualizaton},
	isbn = {978-1-4799-1292-6},
	url = {http://ieeexplore.ieee.org/document/6691708/},
	doi = {10.1109/BigData.2013.6691708},
	abstract = {Modern database management systems (DBMS) have been designed to efficiently store, manage and perform computations on massive amounts of data. In contrast, many existing visualization systems do not scale seamlessly from small data sets to enormous ones. We have designed a three-tiered visualization system called ScalaR to deal with this issue. ScalaR dynamically performs resolution reduction when the expected result of a DBMS query is too large to be effectively rendered on existing screen real estate. Instead of running the original query, ScalaR inserts aggregation, sampling or filtering operations to reduce the size of the result. This paper presents the design and implementation of ScalaR, and shows results for an example application, displaying satellite imagery data stored in SciDB as the back-end DBMS.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Big} {Data}},
	author = {Battle, Leilani and Stonebraker, Michael and Chang, Remco},
	year = {2013},
	keywords = {Data analysis, RDistributed: Yes, RInteractive: Yes, Scientific Computing, cluster:Visual Optimizations, interactive visualization, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution, ★},
	pages = {1--8},
}

@inproceedings{Alvarez2014,
	title = {Main {Memory} {Adaptive} {Indexing} for {Multi}-core {Systems}},
	isbn = {978-1-4503-2971-2},
	url = {https://dl.acm.org/citation.cfm?id=2619231},
	doi = {10.1145/2619228.2619231},
	abstract = {Adaptive indexing is a concept that considers index creation in databases as a by-product of query processing; as opposed to traditional full index creation where the indexing effort is performed up front before answering any queries. Adaptive indexing has received a considerable amount of attention, and several algorithms have been proposed over the past few years; including a recent experimental study comparing a large number of existing methods. Until now, however, most adaptive indexing algorithms have been designed single-threaded, yet with multi-core systems already well established, the idea of designing parallel algorithms for adaptive indexing is very natural. In this regard, and to the best of our knowl- edge, only one parallel algorithm for adaptive indexing has recently appeared in the literature: The parallel version of standard cracking. In this paper we describe three alternative parallel algorithms for adaptive indexing, including a second variant of a parallel standard cracking algorithm. Additionally, we describe a hybrid parallel sorting algorithm, and a NUMA-aware method based on sorting. We then thoroughly compare all these algorithms experimentally. Parallel sorting algorithms serve as a realistic baseline for multi-threaded adaptive indexing techniques. In total we experimentally compare seven parallel algorithms. The initial set of experiments considered in this paper indicates that our parallel algorithms sig- nificantly improve over previously known ones. Our results also suggest that, although adaptive indexing algorithms are a good design choice in single-threaded environments, the rules change con- siderably in the parallel case. That is, in future highly-parallel environments, sorting algorithms could be serious alternatives to adaptive indexing.},
	booktitle = {Proceedings of the {Tenth} {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	author = {Alvarez, Victor and Richter, Stefan},
	year = {2014},
	note = {arXiv: 1404.2034},
	keywords = {Information Systems, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
}

@inproceedings{Agarwal2014,
	title = {Knowing {When} {You}'re {Wrong}: {Building} {Fast} and {Reliable} {Approximate} {Query} {Processing} {Systems}},
	isbn = {978-1-4503-2376-5},
	url = {http://dl.acm.org/citation.cfm?id=2593667},
	doi = {10.1145/2588555.2593667},
	abstract = {Modern data analytics applications typically process massive amounts of data on clusters of tens, hundreds, or thousands of machines to support near-real-time decisions.The quantity of data and limitations of disk and memory bandwidth often make it infeasible to deliver answers at interactive speeds. However, it has been widely observed that many applications can tolerate some degree of inaccuracy. This is especially true for exploratory queries on data, where users are satisfied with "close-enough" answers if they can come quickly. A popular technique for speeding up queries at the cost of accuracy is to execute each query on a sample of data, rather than the whole dataset. To ensure that the returned result is not too inaccurate, past work on approximate query processing has used statistical techniques to estimate "error bars" on returned results. However, existing work in the sampling-based approximate query processing (S-AQP) community has not validated whether these techniques actually generate accurate error bars for real query workloads. In fact, we find that error bar estimation often fails on real world production workloads. Fortunately, it is possible to quickly and accurately diagnose the failure of error estimation for a query. In this paper, we show that it is possible to implement a query approximation pipeline that produces approximate answers and reliable error bars at interactive speeds.},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Agarwal, Sameer and Milner, Henry and Kleiner, Ariel and Talwalkar, Ameet and Jordan, Michael and Madden, Samuel and Mozafari, Barzan and Stoica, Ion},
	year = {2014},
	note = {ISSN: 07308078},
	keywords = {cluster:Query Approximation, diagnostics, error estimation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {481--492},
}

@article{Alagiannis2014,
	title = {{H2O}: {A} {Hands}-free {Adaptive} {Store}},
	issn = {07308078},
	url = {https://dl.acm.org/citation.cfm?id=2588555.2610502},
	doi = {10.1145/2588555.2610502},
	abstract = {Modern state-of-the-art database systems are designed around a single data storage layout. This is a fixed decision that drives the whole architectural design of a database system, i.e., row-stores, column-stores. However, none of those choices is a universally good solution; different workloads require different storage layouts and data access methods in order to achieve good performance. In this paper, we present the H2O system which introduces two novel concepts. First, it is flexible to support multiple storage layouts and data access patterns in a single engine. Second, and most importantly, it decides on-the-fly, i.e., during query processing, which design is best for classes of queries and the respective data parts. At any given point in time, parts of the data might be materialized in various patterns purely depending on the query workload; as the workload changes and with every single query, the storage and access patterns continuously adapt. In this way, H2O makes no a priori and fixed decisions on how data should be stored, allowing each single query to enjoy a storage and access pattern which is tailored to its specific properties. We present a detailed analysis of H2O using both synthetic benchmarks and realistic scientific workloads. We demonstrate that while existing systems cannot achieve maximum performance across all workloads, H2O can always match the best case performance without requiring any tuning or workload knowledge.},
	journal = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
	author = {Alagiannis, Ioannis and Idreos, Stratos and Ailamaki, Anastasia},
	year = {2014},
	note = {ISBN: 9781450323765},
	keywords = {cluster:Adaptive Storage, dynamic operators, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {1103--1114},
}

@article{Abouzied2012,
	title = {{DataPlay}: interactive tweaking and example-driven correction of graphical database queries},
	url = {http://dl.acm.org/citation.cfm?doid=2380116.2380144},
	doi = {10.1145/2380116.2380144},
	abstract = {Writing complex queries in SQL is a challenge for users. Prior work has developed several techniques to ease query specification but none of these techniques are applicable to a particularly difficult class of queries: quantified queries. Our hypothesis is that users prefer to specify quantified queries interactively by trial-and-error. We identify two impediments to this form of interactive trial-and-error query specification in SQL: (i) changing quantifiers often requires global syntactical query restructuring, and (ii) the absence of non-answers from SQL's results makes verifying query correctness difficult. We remedy these issues with DataPlay, a query tool with an underlying graphical query language, a unique data model and a graphical interface. DataPlay provides two interaction features that support trial-and-error query specification. First, DataPlay allows users to directly manipulate a graphical query by changing quantifiers and modifying dependencies between constraints. Users receive real-time feedback in the form of updated answers and non-answers. Second, DataPlay can auto-correct a user's query, based on user feedback about which tuples to keep or drop from the answers and non-answers. We evaluated the effectiveness of each interaction feature with a user study and we found that direct query manipulation is more effective than auto-correction for simple queries but auto-correction is more effective than direct query manipulation for more complex queries.},
	journal = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
	author = {Abouzied, Azza and Hellerstein, Joseph and Silberschatz, Avi},
	year = {2012},
	note = {ISBN: 9781450315807},
	keywords = {UserStudy:yes, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {207},
}

@inproceedings{Abouzied2013,
	title = {Learning and verifying quantified boolean queries by example},
	isbn = {978-1-4503-2066-5},
	url = {http://dl.acm.org/citation.cfm?doid=2463664.2465220},
	doi = {10.1145/2463664.2465220},
	abstract = {To help a user specify and verify quantified queries --- a class of database queries known to be very challenging for all but the most expert users --- one can question the user on whether certain data objects are answers or non-answers to her intended query. In this paper, we analyze the number of questions needed to learn or verify qhorn queries, a special class of Boolean quantified queries whose underlying form is conjunctions of quantified Horn expressions. We provide optimal polynomial-question and polynomial-time learning and verification algorithms for two subclasses of the class qhorn with upper constant limits on a query's causal density.},
	booktitle = {Proceedings of the {32Nd} {ACM} {SIGMOD}-{SIGACT}-{SIGAI} {Symposium} on {Principles} of {Database} {Systems}},
	author = {Abouzied, Azza and Angluin, Dana and Papadimitriou, Christos and Hellerstein, Joseph M. and Silberschatz, Avi},
	year = {2013},
	note = {arXiv: 1304.4303v1},
	keywords = {cluster:Assisted Query Formulation, example-driven synthesis, ification, layer:User Interaction, qhorn, quantified boolean queries, query learning, query ver-, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {49},
}

@article{Dickersin1994,
	title = {Identifying relevant studies for systematic reviews.},
	volume = {309},
	issn = {0959-8138},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2541778&tool=pmcentrez&rendertype=abstract},
	doi = {10.1136/bmj.310.6981.741a},
	abstract = {OBJECTIVE: To examine the sensitivity and precision of Medline searching for randomised clinical trials.{\textbackslash}n{\textbackslash}nDESIGN: Comparison of results of Medline searches to a "gold standard" of known randomised clinical trials in ophthalmology published in 1988; systematic review (meta-analysis) of results of similar, but separate, studies from many fields of medicine.{\textbackslash}n{\textbackslash}nPOPULATIONS: Randomised clinical trials published in 1988 in journals indexed in Medline, and those not indexed in Medline and identified by hand search, comprised the gold standard. Gold standards for the other studies combined in the meta-analysis were based on: randomised clinical trials published in any journal, whether indexed in Medline or not; those published in any journal indexed in Medline; or those published in a selected group of journals indexed in Medline.{\textbackslash}n{\textbackslash}nMAIN OUTCOME MEASURE: Sensitivity (proportion of the total number of known randomised clinical trials identified by the search) and precision (proportion of publications retrieved by Medline that were actually randomised clinical trials) were calculated for each study and combined to obtain weighted means. Searches producing the "best" sensitivity were used for sensitivity and precision estimates when multiple searches were performed.{\textbackslash}n{\textbackslash}nRESULTS: The sensitivity of searching for ophthalmology randomised clinical trials published in 1988 was 82\%, when the gold standard was for any journal, 87\% for any journal indexed in Medline, and 88\% for selected journals indexed in Medline. Weighted means for sensitivity across all studies were 51\%, 77\%, and 63\%, respectively. The weighted mean for precision was 8\% (median 32.5\%). Most searchers seemed not to use freetext subject terms and truncation of those terms.{\textbackslash}n{\textbackslash}nCONCLUSION: Although the indexing terms available for searching Medline for randomised clinical trials have improved, sensitivity still remains unsatisfactory. A mechanism is needed to "'register" known trials, preferably by retrospective tagging of Medline entries, and incorporating trials published before 1966 and in journals not indexed by Medline into the system.},
	number = {6964},
	journal = {BMJ (Clinical research ed.)},
	author = {Dickersin, K and Scherer, R and Lefebvre, C},
	year = {1994},
	pmid = {7718048},
	note = {ISBN: 0959-535X},
	keywords = {Evaluation Studies as Topic, MEDLINE, MEDLINE: standards, Randomized Controlled Trials as Topic, Sensitivity and Specificity, Subject Headings},
	pages = {1286--91},
}

@inproceedings{Zhang:2010:SRS:2227057.2227071,
	address = {Swindon, UK},
	title = {On searching relevant studies in software engineering},
	abstract = {peer-reviewed},
	booktitle = {14th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} ({EASE})},
	publisher = {BCS Learning \& Development Ltd.},
	author = {Zhang, He and Babar, Ma},
	year = {2010},
	note = {Series Title: EASE'10
Issue: 1994},
	keywords = {Systematic Reviews, evidence-based software engineering, quasi-gold standard},
	pages = {1--10},
}

@inproceedings{Bailey2007,
	title = {Search engine overlaps: {Do} they agree or disagree?},
	isbn = {0-7695-2962-3},
	doi = {10.1109/REBSE.2007.4},
	abstract = {Secondary studies, such as systematic literature reviews and mapping studies, are an essential element of the evidence-based paradigm. A critical part of the review process is the identification of all relevant research. As such, any researcher intending to conduct a secondary review should be aware of the strengths and weakness of the search engines available. Analyse the overlap between search engine results for software engineering studies. Three independent studies were conducted to evaluate the overlap between multiple search engines for different search areas. The findings indicate that very little overlap was found between the search engines. To complete a systematic review, researchers must use multiple search terms and search engines. The lack of overlap might also be caused by inconsistent keyword selection amongst authors.},
	booktitle = {Proceedings - {ICSE} 2007 {Workshops}: {Second} {International} {Workshop} on {Realising} {Evidence}-{Based} {Software} {Engineering}, {REBSE}'07},
	author = {Bailey, John and Zhang, Cheng and Budgen, David and Turner, Mark and Charters, Stuart},
	year = {2007},
}

@incollection{Bartolucci2010,
	title = {Overview, {Strengths}, and {Limitations} of {Systematic} {Reviews} and {Meta}-{Analyses}},
	isbn = {978-3-540-22239-2},
	abstract = {Evidence-based practice in nursing, although supported in principle, its translation has been difficult and challenging. However, it has been improving in its popularity over time; indeed, a promising future for evidence-based nursing practice.},
	booktitle = {Evidence-{Based} {Practice}: {Toward} {Optimizing} {Clinical} {Outcomes}},
	author = {Bartolucci, Alfred A and Hillegass, William B},
	year = {2010},
	doi = {10.1007/978-3-642-05025-1},
	pages = {17--33},
}

@article{Petticrew756,
	title = {Why certain systematic reviews reach uncertain conclusions},
	volume = {326},
	issn = {09598138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.326.7392.756},
	doi = {10.1136/bmj.326.7392.756},
	abstract = {The “stainless steel” law of evaluation states that the better designed the outcome evaluation, the less effective the intervention seems. This article explores how this law may be operating in relation to systematic reviews.},
	number = {7392},
	journal = {Bmj},
	author = {Petticrew, M.},
	year = {2003},
	pmid = {12676848},
	note = {Publisher: BMJ Publishing Group Ltd
ISBN: 0959-535X},
	pages = {756--758},
}

@article{Cetintemel2013,
	title = {Query {Steering} for {Interactive} {Data} {Exploration}.},
	abstract = {Traditional DBSMs are suited for applications in which the structure, meaning and contents of the database, as well as the questions to be asked are already well understood. There is, however, a class of applications that we will collectively refer to as Interactive Data Exploration (IDE) applications, in which this is not the case. IDE is a key ingredient of a diverse set of discovery-oriented applications we are dealing with, including ones from scientific computing, financial analysis, evidence-based medicine, and genomics. The need for effective IDE will only increase as data are being collected at an unprecedented rate. IDE is fundamentally a multi-step, non-linear process with imprecise end-goals. For example, data-driven scientific discovery through IDE often requires non-expert users to iteratively interact with the system to make sense of and to identify interesting patterns and relationships in large, amorphous data sets. To make the most of the increasingly available complex and big data sets, users would need an “expert assistant” who would be able to effectively and efficiently guide them through the data space. Having a human assistant is not only expensive but also unrealistic. Thus, it is essential that we automate this task. We propose database systems be augmented with an automated “database navigator” (DBNav) service that assists as a “tour guide” to facilitate IDE. Just like a car navigation system that offers advice on the routes to be taken and display points of interest, DBNav would similarly steer the user towards interesting “trajectories” through the data, while highlighting relevant features. Like any good tour guide, DBNav should consider many kinds of information; in particular, it should be sensitive to a user’s goals and interests, as well as common navigation patterns that applications exhibit. We sketch a general data navigation framework and discuss some specific components and approaches that we believe belong to any such system. 1.},
	journal = {Cidr},
	author = {Çetintemel, U and Cherniack, Mitch and DeBrabant, J},
	year = {2013},
	keywords = {cluster:Exploration Interfaces, layer:User Interaction, supercluster:Exploration Interfaces},
	pages = {1--4},
}

@article{Riahi2016,
	title = {Integration of end-user {Cloud} storage for {CMS} analysis},
	issn = {0167739X},
	doi = {10.1016/j.future.2017.04.021},
	abstract = {© 2017.End-user Cloud storage is increasing rapidly in popularity in research communities thanks to the collaboration capabilities it offers, namely synchronisation and sharing. CERN IT has implemented a model of such storage named, CERNBox, integrated with the CERN AuthN and AuthZ services. To exploit the use of the end-user Cloud storage for the distributed data analysis activity, the CMS experiment has started the integration of CERNBox as a Grid resource. This will allow CMS users to make use of their own storage in the Cloud for their analysis activities as well as to benefit from synchronisation and sharing capabilities to achieve results faster and more effectively. It will provide an integration model of Cloud storages in the Grid, which is implemented and commissioned over the world's largest computing Grid infrastructure, Worldwide LHC Computing Grid (WLCG).In this paper, we present the integration strategy and infrastructure changes needed in order to transparently integrate end-user Cloud storage with the CMS distributed computing model. We describe the new challenges faced in data management between Grid and Cloud and how they were addressed, along with details of the support for Cloud storage recently introduced into the WLCG data movement middleware, FTS3. The commissioning experience of CERNBox for the distributed data analysis activity is also presented.},
	journal = {Future Generation Computer Systems},
	author = {Riahi, H. and Aimar, A. and Ayllón, A.Á. and Balcas, J. and Ciangottini, D. and Hernández, J.M. and Keeble, O. and Magini, N. and Manzi, A. and Mascetti, L. and Mascheroni, M. and Tanasijczuk, A.J. and Vaandering, E.W.},
	year = {2016},
	keywords = {Data transfer protocols, Distributed data analysis, Distributed data management},
}

@inproceedings{Idreos2015,
	title = {Overview of {Data} {Exploration} {Techniques}},
	isbn = {978-1-4503-2758-9},
	url = {http://dl.acm.org/citation.cfm?doid=2723372.2731084},
	doi = {10.1145/2723372.2731084},
	abstract = {Data exploration is about efficiently extracting knowledge from data even if we do not know exactly what we are look- ing for. In this tutorial, we survey recent developments in the emerging area of database systems tailored for data ex- ploration. We discuss new ideas on how to store and access data as well as new ideas on how to interact with a data sys- tem to enable users and applications to quickly figure out which data parts are of interest. In addition, we discuss how to exploit lessons-learned from past research, the new chal- lenges data exploration crafts, emerging applications and future research directions.},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data} - {SIGMOD} '15},
	author = {Idreos, Stratos and Papaemmanouil, Olga and Chaudhuri, Surajit},
	year = {2015},
	note = {ISSN: 07308078},
	keywords = {★},
	pages = {277--281},
}

@article{Kitchenham2013,
	title = {A systematic review of systematic review process research in software engineering},
	volume = {55},
	issn = {09505849},
	doi = {10.1016/j.infsof.2013.07.010},
	abstract = {Context: Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research. Objective: To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process. Method: We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools. Results: We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult. Conclusion: We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem. ?? 2013 Elsevier B.V. All rights reserved.},
	number = {12},
	journal = {Information and Software Technology},
	author = {Kitchenham, Barbara and Brereton, Pearl},
	year = {2013},
	note = {ISBN: 09505849},
	keywords = {Mapping study, Systematic Reviews},
	pages = {2049--2075},
}

@inproceedings{Zhi2015,
	title = {{AQUAdex}: {A} highly efficient indexing and retrieving method for astronomical big data of time series images},
	isbn = {978-3-319-27121-7},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-27122-4_7},
	doi = {10.1007/978-3-319-27122-4_7},
	abstract = {In the era of Big Data, scientific research is challenged with handling massive data sets. To actually take advantage of Big Data, the key problem is to retrieve the desired cup of data from the ocean, as most applications only need a fraction of the entire data set. As the indexing and retrieving method is intrinsically connected with specific features of the data set and the goal of research, a universal solution is hardly possible. Designed for efficiently querying Big Data in astronomy time domain research, AQUAdex, a new spatial indexing and retrieving method is proposed to extract Time Series Images form Astronomical Big Data. By mapping images to tiles (pixels) on the celestial sphere, AQUAdex can complete queries 9 times faster, which is proven by theoretical analysis and experimental results. AQUAdex is especially suitable for Big Data applications because of its excellent scalability. The query time only increases 59 \% while the data size grows 14 times larger. © Springer International Publishing Switzerland 2015.},
	booktitle = {Algorithms and {Architectures} for {Parallel} {Processing}},
	author = {Hong, Zhi and Yu, Ce and Xia, Ruolei and Xiao, Jian and Wang, Jie and Sun, Jizhou and Cui, Chenzhou},
	year = {2015},
	note = {ISSN: 16113349},
	keywords = {FITS file, Pseudo-sphere index, Spatial index, Time series images, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {92--105},
}

@article{Micha2007,
	title = {Approximate range searching in external memory},
	volume = {59},
	issn = {01784617},
	url = {http://link.springer.com/article/10.1007/s00453-009-9297-0},
	doi = {10.1007/s00453-009-9297-0},
	abstract = {In this paper, we present two linear-size external memory data structures for approximate range searching. Our first structure, the BAR-B-tree, stores a set of N points in ℝd and can report all points inside a query range Q by accessing O(log BN+εγ+kε/B) disk blocks, where B is the disk block size, γ=1−d for convex queries and γ=−d otherwise, and kε is the number of points lying within a distance of ε⋅diam (Q) to the query range Q. Our second structure, the object-BAR-B-tree, is able to store objects of arbitrary shapes of constant complexity and provides similar query guarantees. In addition, both structures can support other types of range searching queries such as range aggregation and nearest-neighbor. Finally, we present I/O-efficient algorithms to build these structures.},
	number = {2},
	journal = {Algorithmica},
	author = {Streppel, Micha and Yi, Ke},
	year = {2011},
	note = {ISBN: 0045300992},
	keywords = {External memory algorithms, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {115--128},
}

@article{Kostas2016,
	title = {{ADS}: the adaptive data series index},
	volume = {25},
	issn = {0949877X},
	url = {http://link.springer.com/article/10.1007/s00778-016-0442-5},
	doi = {10.1007/s00778-016-0442-5},
	abstract = {Numerous applications continuously pro-duce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available. This, however, is not currently possible with the state-of-the-art indexing methods and for very large data series collections. In this paper, we present the first adaptive indexing mechanism, specifically tailored to solve the problem of indexing and querying very large data series collections. We present a detailed design and evaluation of our method using approximate and exact query algo-rithms with both synthetic and real datasets. Adaptive indexing significantly outperforms previous solutions, gracefully handling large data series collections, reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), our method has already answered 3 * 10 5 queries.},
	number = {6},
	journal = {The VLDB Journal},
	author = {Zoumpatianos, Kostas and Idreos, Stratos and Palpanas, Themis},
	year = {2016},
	keywords = {RInteractive: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {843--866},
}

@inproceedings{HuiHongyuan2015,
	title = {Enhancing {Parallel} {Data} {Loading} for {Large} {Scale} {Scientific} {Database}},
	url = {http://link.springer.com/10.1007/978-3-319-27122-4_11},
	doi = {10.1007/978-3-319-27122-4_11},
	abstract = {The rapidly increased data size make large scale scientific database often have a huge time delay between loading data into the system and ready for receiving query request. To solve this problem, we proposed an efficient parallel data loading approach named FASTLoad. It is designed to maximize the given resource (e.g., network bandwidth, main memory) utilization for optimizing the data loading in large scale array model based scientific database system. To verify the efficiency of FASTLoad, we implemented it in our Adaptable Data Loading System and evaluate its performance over various sizes of large scientific data sets. Our experimental results show that the performance of FASTLoad can be 4 to 6 times fast than the built-in loading techniques of states-of-the-arts array model based scientific database system.},
	booktitle = {Big {Data} {Analytics}},
	author = {Li, Hui and Li, Hongyuan and Chen, Mei and Dai, Zhenyu and Zhu, Ming and Huang, Menglin},
	year = {2015},
	keywords = {cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {149--162},
}

@article{Syed2015,
	title = {{SOCR} data dashboard: an integrated big data archive mashing medicare, labor, census and econometric information},
	volume = {2},
	issn = {2196-1115},
	url = {http://www.journalofbigdata.com/content/2/1/13},
	doi = {10.1186/s40537-015-0018-z},
	abstract = {INTRODUCTION: Intuitive formulation of informative and computationally-efficient queries on big and complex datasets present a number of challenges. As data collection is increasingly streamlined and ubiquitous, data exploration, discovery and analytics get considerably harder. Exploratory querying of heterogeneous and multi-source information is both difficult and necessary to advance our knowledge about the world around us.{\textbackslash}n{\textbackslash}nRESEARCH DESIGN: We developed a mechanism to integrate dispersed multi-source data and service the mashed information via human and machine interfaces in a secure, scalable manner. This process facilitates the exploration of subtle associations between variables, population strata, or clusters of data elements, which may be opaque to standard independent inspection of the individual sources. This a new platform includes a device agnostic tool (Dashboard webapp, http://socr.umich.edu/HTML5/Dashboard/) for graphical querying, navigating and exploring the multivariate associations in complex heterogeneous datasets.{\textbackslash}n{\textbackslash}nRESULTS: The paper illustrates this core functionality and serviceoriented infrastructure using healthcare data (e.g., US data from the 2010 Census, Demographic and Economic surveys, Bureau of Labor Statistics, and Center for Medicare Services) as well as Parkinson's Disease neuroimaging data. Both the back-end data archive and the front-end dashboard interfaces are continuously expanded to include additional data elements and new ways to customize the human and machine interactions.{\textbackslash}n{\textbackslash}nCONCLUSIONS: A client-side data import utility allows for easy and intuitive integration of user-supplied datasets. This completely open-science framework may be used for exploratory analytics, confirmatory analyses, meta-analyses, and education and training purposes in a wide variety of fields.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Husain, Syed S and Kalinin, Alexandr and Truong, Anh and Dinov, Ivo D},
	year = {2015},
	pmid = {26236573},
	note = {ISBN: 2196-1115 (Print)},
	keywords = {cluster:Visualization Tools, layer:User Interaction, supercluster:Exploration Interfaces, type:Validation Research},
	pages = {13},
}

@inproceedings{Aqeel2015,
	title = {User's interpretations of features in visualization},
	volume = {550},
	isbn = {978-3-319-25116-5},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-25117-2_7},
	doi = {10.1007/978-3-319-25117-2_7},
	abstract = {Visualization is often used to identify features of interest in a dataset. The identification of features cannot be fully automated and the subjective interpretation of the user is involved in the identification of the feature. There can be many such interpretations, both from a single user as s/he explores the data, and also in collaborations. Managing all these interpretations is problematic. We propose a novel visualization architecture that addresses this problem. We illustrate our method by examining how geoseismic data is interpreted, since this application presents all of the issues above.},
	booktitle = {Computer {Vision}, {Imaging} and {Computer} {Graphics} - {Theory} and {Applications}},
	author = {Al-Naser, Aqeel and Rasheed, Masroor and Irving, Duncan and Brooke, John},
	year = {2015},
	note = {ISSN: 18650929},
	keywords = {Data acquisition and management, Data exploration, Geospatial visualization, Provenance, Query-driven visualization, UserStudy:yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {97--114},
}

@inproceedings{Le:2014:FFD:2666356.2594333,
	address = {New York, NY, USA},
	title = {{FlashExtract}: a framework for data extraction by examples},
	volume = {49},
	isbn = {978-1-4503-2784-8},
	url = {http://dl.acm.org/citation.cfm?doid=2594291.2594333},
	doi = {10.1145/2594291.2594333},
	abstract = {Various document types that combine model and view (e.g., text files, webpages, spreadsheets) make it easy to organize (possibly hierarchical) data, but make it difficult to extract raw data for any further manipulation or querying. We present a general framework FlashExtract to extract relevant data from semi-structured documents using examples. It includes: (a) an interaction model that allows end-users to give examples to extract various fields and to relate them in a hierarchical organization using structure and sequence constructs. (b) an inductive synthesis algorithm to synthesize the intended program from few examples in any underlying domainspecific language for data extraction that has been built using our specified algebra of few core operators (map, filter, merge, and pair). We describe instantiation of our framework to three different domains: text files, webpages, and spreadsheets. On our benchmark comprising 75 documents, FlashExtract is able to extract intended data using an average of 2.36 examples in 0.84 seconds per field. Copyright © 2014 ACM.},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Le, Vu and Gulwani, Sumit},
	month = jun,
	year = {2013},
	note = {Series Title: PLDI '14
Issue: 6
ISSN: 0362-1340},
	keywords = {cluster:Assisted Query Formulation, end-user programming, layer:User Interaction, program synthesis, programming by examples, supercluster:Exploration Interfaces, type:Validation Research},
	pages = {542--553},
}

@inproceedings{Adam2017,
	title = {{DBMS} {Data} {Loading}: {An} {Analysis} on {Modern} {Hardware}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-56111-0_6},
	doi = {10.1007/978-3-319-56111-0_6},
	abstract = {Data loading has traditionally been considered a " one-time deal " – an offline process out of the critical path of query execution. The architecture of DBMS is aligned with this assumption. Nevertheless, the rate in which data is produced and gathered nowadays has nullified the " one-off " assumption, and has turned data loading into a major bottleneck of the data analysis pipeline. This paper analyzes the behavior of modern DBMS in order to quantify their ability to fully exploit multicore processors and modern storage hardware during data loading. We examine multiple state-of-the-art DBMS, a variety of hardware configurations, and a combination of synthetic and real-world datasets to iden-tify bottlenecks in the data loading process and to provide guidelines on how to accelerate data loading. Our findings show that modern DBMS are unable to saturate the available hardware resources. We therefore identify opportunities to accelerate data loading.},
	booktitle = {Data {Management} on {New} {Hardware}},
	author = {Dziedzic, Adam and Karpathiotakis, Manos and Alagiannis, Ioannis and Appuswamy, Raja and Ailamaki, Anastasia},
	year = {2017},
	keywords = {cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, type:Evaluation Research},
}

@misc{Hamidur2016,
	title = {Ins and {Outs} of {Big} {Data}: {A} {Review}},
	url = {http://link.springer.com/10.1007/978-3-319-51234-1_7},
	abstract = {Today with the fast development of digital technologies and advance communications a gigantic amount of data sets with massive and complex structures called ‘Big data’ is being produced everyday enormously and exponentially. Again, the arrival of social media, advent of smart homes, offices and hospitals are connected as Internet of Things (IoT), this influence also a lot to Big data. According to the study, Big data presents data sets with large magnitude including structured, semi-structured or unstructured data. The study also presents the new technologies for data analyzing, collecting, fast searching, proper sharing, exact storing, speedy transferring, hidden pattern visualization and violations of privacy etc. This paper presents an overview of ins and outs of Big Data where the content, scope, samples, methods, advantages, challenges and privacy of Big data have been discussed. The goal of this article is to provide big data knowledge to the research community for the sake of its many real life applications such as traffic management, driver monitoring, health care in hospitals, meteorology and so on.},
	author = {Rahman, Hamidur and Begum, Shahina and Ahmed, Mobyen Uddin},
	year = {2016},
	doi = {10.1007/978-3-319-51234-1_7},
	note = {Pages: 44-51
Publication Title: Internet of Things Technologies for HealthCare},
	keywords = {★},
}

@misc{PeterThanisch2013,
	title = {Cell-at-a-{Time} {Approach} to {Lazy} {Evaluation} of {Dimensional} {Aggregations}},
	url = {http://link.springer.com/10.1007/978-3-642-40131-2_31},
	abstract = {We present a lazy evaluation technique for computing summarized information from dimensional databases. Our technique works well with a very large number of dimensions. While the traditional approach has been to preprocess analysis models from which the user selects the data of interest, in our approach only the cells required by the user are calculated using a cell-by-cell computation strategy.},
	author = {Thanisch, Peter and Nummenmaa, Jyrki and Niemi, Tapio and Niinimäki, Marko},
	year = {2013},
	doi = {10.1007/978-3-642-40131-2_31},
	note = {Pages: 349-358
Publication Title: Data Warehousing and Knowledge Discovery},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@inproceedings{Muhammad2015,
	title = {Optimized {Multi}-{Resolution} {Indexing} and {Retrieval} {Scheme} of {Time} {Series}},
	url = {http://link.springer.com/10.1007/978-3-319-23485-4_61},
	doi = {10.1007/978-3-319-23485-4_61},
	abstract = {Multi-resolution representation has been successfully used for indexing and retrieval of time series. In a previous work we presented Tight-MIR, a multi-resolution representation method which speeds up the similarity search by using distances pre-computed at indexing time. At query time Tight-MIR applies two pruning conditions to filter out non-qualifying time series. Tight-MIR has the disadvantage of storing all the distances corresponding to all resolution levels, even those whose pruning power is low. At query time Tight-MIR also processes all stored resolution levels. In this paper we optimize the Tight-MIR algorithm by enabling it to store and process only the resolution levels with the maximum pruning power. The experiments we conducted on the new optimized version show that it does not only require less storage space, but it is also faster than the original algorithm.},
	booktitle = {Progress in {Artificial} {Intelligence}},
	author = {Muhammad Fuad, Muhammad Marwan},
	year = {2015},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Validation Research},
	pages = {603--608},
}

@inproceedings{Yves2010,
	title = {Merging file systems and data bases to fit the grid},
	volume = {6265 LNCS},
	isbn = {3-642-15107-8},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-15108-8_2},
	doi = {10.1007/978-3-642-15108-8_2},
	abstract = {Grids are widely used by CPU intensive applications requiring to access data with high level queries as well as in a file based manner. Their requirements include accessing data through metadata of different kinds, system or application ones. In addition, grids provide large storage capabilities and support cooperation between sites. However, these solutions are relevant only if they supply good performance. This paper presents Gedeon, a middleware that proposes a hybrid approach for scientific data management for grid infrastructures. This hybrid approach consists in merging distributed files systems and distributed databases functionalities offering thus semantically enriched data management and preserving easiness of use and deployment. Taking advantage of this hybrid approach, advanced cache strategies are deployed at different levels to provide efficiency. Gedeon has been implemented, tested and used in the bioinformatic field.},
	booktitle = {Data {Management} in {Grid} and {Peer}-to-{Peer} {Systems}},
	author = {Denneulin, Yves and Labbé, Cyril and D'Orazio, Laurent and Roncancio, Claudia},
	year = {2010},
	note = {ISSN: 03029743},
	keywords = {RDistributed: Yes, cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {13--25},
}

@article{Debin2015,
	title = {{LuBase}: {A} search-efficient hybrid storage system for massive text data},
	volume = {9529},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-27122-4_10},
	doi = {10.1007/978-3-319-27122-4_10},
	abstract = {Recent years have witnessed a great deal of enthusiasm devoting to big data analytics systems, some of them, with the property of high scalability and fault tolerance, are extensively used in real productions. However, such systems are mostly designed for processing immutable data stored in HDFS, not suitable for real-time text data in NoSQL database like HBase. In this paper, we propose a search-efficient hybrid storage system termed LuBase for large-scale text data analytics scenarios. Not just a novel hybrid storage system with fine-grained index, LuBase also presents a new query process flow which can fully employ pre-built full-text index to accelerate the execution of interactive queries and achieve more efficient I/O performance at the same time. We implemented LuBase in a data analytics system based on Impala. Experimental results demonstrate that LuBase can reap huge fruits from Lucene index technique and bring significant performance improvement for Impala when querying HBase.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Jia, Debin and Liu, Zhengwei and Gu, Xiaoyan and Li, Bo and Gu, Jingzi and Wang, Weiping and Meng, Dan},
	year = {2015},
	pmid = {618839},
	note = {ISBN: 9783319271217},
	keywords = {HBase, Hybrid storage system, Impala, Lucene, Massive text data},
	pages = {134--148},
}

@misc{Yijun2011,
	title = {A {TV} {Commercial} {Detection} {System}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-23982-3_5},
	author = {Luo, Yijun LiSuhuai},
	year = {2011},
	doi = {10.1007/978-3-642-23982-3_5},
	note = {Publication Title: Web Information Systems and Mining},
}

@misc{B.2009,
	title = {Single {Vector} {Large} {Data} {Cardinality} {Structure} to {Handle} {Compressed} {Database} in a {Distributed} {Environment}},
	url = {http://link.springer.com/10.1007/978-3-642-05201-9_12},
	abstract = {Loss-less data compression is attractive in database systems as it may facilitate query performance improvement and storage reduction. Although there are many compression techniques that handle the whole database in main memory, problems arise when the amount of data increases gradually over time, and also when the data has high cardinality. Management of a rapidly evolving large volume of data in a scalable way is very challenging. This paper describes a disk based single vector large data cardinality approach, incorporating data compression in a distributed environment. The approach provides substantial storage performance improvement compared to other high performance database systems. The presented compressed database structure provides direct addressability in a distributed environment, thereby reducing retrieval latency when handling large volumes of data.},
	author = {Alom, B. M. Monjurul and Henskens, Frans and Hannaford, Michael},
	year = {2009},
	doi = {10.1007/978-3-642-05201-9_12},
	note = {Pages: 147-160
Publication Title: Software and Data Technologies},
}

@misc{Salman2017,
	title = {Approximate {OLAP} on {Sustained} {Data} {Streams}},
	url = {http://link.springer.com/10.1007/978-3-319-55699-4_7},
	abstract = {Many organizations require detailed and real time analysis of their business data for effective decision making. OLAP is one of the commonly used methods for the analysis of static data and has been studied by many researchers. OLAP is also applicable to data streams, however the requirement to produce real time analysis on fast and evolving data streams is not possible unless the data to be analysed reside on memory. Keeping in view the limited size and the volatile nature of the memory, we propose a novel architecture AOLAP which in addition to storing raw data streams to the secondary storage, maintains data stream’s summaries in a compact memory-based data structure. This work proposes the use of piece-wise linear approximation (PLA) for storing such data summaries corresponding to each materialized node in the OLAP cube. Since the PLA is a compact data structure, it can store the long data streams’ summaries in comparatively smaller space and can give approximate answers to OLAP queries. OLAP analysts query different nodes in the OLAP cube interactively. To support such analysis by the PLA-based data cube without the unnecessary amplification of querying errors, inherent in the PLA structure, many nodes should be materialized. However, even though each PLA structure is compact, it is impossible to materialize all the nodes in the OLAP cube. Thus, we need to select the best set of materialized nodes which can give query results with the minimum approximation errors within the given memory bound. This problem is NP-hard. Hence this work also proposes an optimization scheme to support this selection. Detailed experimental evaluation is performed to prove the effectiveness of the use of PLA structure and the optimization scheme.},
	author = {Shaikh, Salman Ahmed and Kitagawa, Hiroyuki},
	year = {2017},
	doi = {10.1007/978-3-319-55699-4_7},
	note = {Pages: 102-118
Publication Title: Database Systems for Advanced Applications},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@misc{Kongfa2007,
	title = {{PHC}: {A} {Rapid} {Parallel} {Hierarchical} {Cubing} {Algorithm} on {High} {Dimensional} {OLAP}},
	url = {http://link.springer.com/10.1007/978-3-540-72905-1_7},
	abstract = {Data cube has been playing an essential role in OLAP (online analytical processing). The pre-computation of data cubes is critical for improving the response time of OLAP systems. However, as the size of data cube grows, the time it takes to perform this pre-computation becomes a significant performance bottleneck. In a high dimensional OLAP, it might not be practical to build all these cuboids and their indices. In this paper, we propose a parallel hierarchical cubing algorithm, based on an extension of the previous minimal cubing approach. The algorithm has two components: decomposition of the cube space based on multiple dimension attributes, and an efficient OLAP query engine based on a prefix bitmap encoding of the indices. This method partitions the high dimensional data cube into low dimensional cube segments. Such an approach permits a significant reduction of CPU and I/O overhead for many queries by restricting the number of cube segments to be processed for both the fact table and bitmap indices. The proposed data allocation and processing model support parallel I/O and parallel processing, as well as load balancing for disks and processors. Experimental results show that the proposed parallel hierarchical cubing method is significantly more efficient than other existing cubing methods.},
	author = {Hu, Kongfa and Chen, Ling and Chen, Yixin},
	year = {2007},
	doi = {10.1007/978-3-540-72905-1_7},
	note = {Pages: 72-82
Publication Title: Algorithms and Architectures for Parallel Processing},
}

@article{Vadim2015,
	title = {Big data, big systems, big challenges: {A} personal experience},
	volume = {8974},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-46823-4_3},
	doi = {10.1007/978-3-662-46823-4_3},
	abstract = {I have been fortunate to work in two remarkable research communities, Akademgorodok in Siberia and Silicon Valley in California, and that my professional career stretched over two very different, yet both exciting epochs of the computer systems and science evolution: steady accumulation of knowledge and technology in 1960s–1980s and, then, “Internet Big Bang” and Information Revolution in 1990s–2010s. In this talk, I track the trends in the development of large computer systems which I witnessed working first at Hewlett-Packard Laboratories and then at Carnegie-Mellon University, Silicon Valley Campus. This is not a general survey, I exemplify those trends by the systems in the analysis or/and design of which I and my colleagues participated.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Kotov, Vadim E.},
	year = {2015},
	note = {ISBN: 9783662468227},
	pages = {41--44},
}

@misc{Ying-Lian2015,
	title = {A {Two}-{Stage} {Sparse} {Selection} {Method} for {Extracting} {Characteristic} {Genes}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-22186-1_58},
	author = {Lei, Ying-Lian GaoJin-Xing LiuChun-Hou ZhengSheng-Jun LiYu-Xia},
	year = {2015},
	doi = {10.1007/978-3-319-22186-1_58},
	note = {Publication Title: Intelligent Computing Theories and Methodologies},
}

@inproceedings{Yueguo2014,
	title = {A study of {SQL}-on-{Hadoop} systems},
	volume = {8807},
	isbn = {978-3-319-13020-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-13021-7_12},
	doi = {10.1007/978-3-319-13021-7_12},
	abstract = {Hadoop is now the de facto standard for storing and processing big data, not only for unstructured data but also for some structured data. As a result, providing SQL analysis functionality to the big data resided in HDFS becomes more and more important. Hive is a pioneer system that support SQL-like analysis to the data in HDFS. However, the performance of Hive is not satisfactory for many applications. This leads to the quick emergence of dozens of SQL-on-Hadoop systems that try to support interactive SQL query processing to the data stored in HDFS. This paper firstly gives a brief technical review on recent efforts of SQLon- Hadoop systems. Then we test and compare the performance of five representative SQL-on-Hadoop systems, based on some queries selected or derived from the TPC-DS benchmark. According to the results, we show that such systems can benefit more from the applications of many parallel query processing techniques that have been widely studied in the traditional MPP analytical databases. \&copy; Springer International Publishing Switzerland 2014.},
	booktitle = {Big {Data} {Benchmarks}, {Performance} {Optimization}, and {Emerging} {Hardware}},
	author = {Chen, Yueguo and Qin, Xiongpai and Bian, Haoqiong and Chen, Jun and Dong, Zhaoan and Du, Xiaoyong and Gao, Yanjie and Liu, Dehai and Lu, Jiaheng and Zhang, Huijie},
	year = {2014},
	note = {ISSN: 16113349},
	keywords = {Benchmark, Big Data, Interactive query, SQL-on-Hadoop, cluster:Exploration Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Validation Research},
	pages = {154--166},
}

@misc{Gerald2000,
	title = {A {Typed} {Access} {Control} {Model} for {CORBA}},
	url = {http://link.springer.com/chapter/10.1007/10722599_6},
	author = {Brose, Gerald},
	year = {2000},
	doi = {10.1007/10722599_6},
	note = {Publication Title: Computer Security - ESORICS 2000},
}

@article{Hong-Cheu2007,
	title = {A {Logic}-{Based} {Approach} to {Mining} {Inductive} {Databases}},
	volume = {4487},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-72584-8_35},
	doi = {10.1007/978-3-540-72584-8_35},
	abstract = {In this paper, we discuss the main problems of inductive query languages and optimisation issues. We present a logic-based inductive query language and illustrate the use of aggregates and exploit a new join operator to model specific data mining tasks. We show how a fixpoint operator works for association rule mining and a clustering method. A preliminary experimental result shows that fixpoint operator outperforms SQL and Apriori methods. The results of our framework could be useful for inductive query language design in the development of inductive database systems.},
	journal = {Computational Science -- ICCS 2007},
	author = {Liu, H. and Yu, J. X. and Zeleznikow, J. and Guan, Y.},
	year = {2007},
	note = {ISBN: 9783540725831},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {270},
}

@article{Meen2016,
	title = {How are they different? {A} quantitative domain comparison of information visualization and data visualization (2000???2014)},
	volume = {107},
	issn = {15882861},
	url = {http://link.springer.com/article/10.1007/s11192-015-1830-0},
	doi = {10.1007/s11192-015-1830-0},
	abstract = {Information visualization and data visualization are often viewed as similar, but distinct domains, and they have drawn an increasingly broad range of interest from diverse sectors of academia and industry. This study systematically analyzes and compares the intellectual landscapes of the two domains between 2000 and 2014. The present study is based on bibliographic records retrieved from the Web of Science. Using a topic search and a citation expansion, we collected two sets of data in each domain. Then, we identified emerging trends and recent developments in information visualization and data visualization, captivated in intellectual landscapes, landmark articles, bursting keywords, and citation trends of the domains. We found out that both domains have computer engineering and applications as their shared grounds. Our study reveals that information visualization and data visualization have scrutinized algorithmic concepts underlying the domains in their early years. Successive literature citing the datasets focuses on applying information and data visualization techniques to biomedical research. Recent thematic trends in the fields reflect that they are also diverging from each other. In data visualization, emerging topics and new developments cover dimensionality reduction and applications of visual techniques to genomics. Information visualization research is scrutinizing cognitive and theoretical aspects. In conclusion, information visualization and data visualization have co-evolved. At the same time, both fields are distinctively developing with their own scientific interests.},
	number = {1},
	journal = {Scientometrics},
	author = {Kim, Meen Chul and Zhu, Yongjun and Chen, Chaomei},
	year = {2016},
	keywords = {Data science, Data visualization, Domain analysis, Information visualization, Visualization},
	pages = {123--165},
}

@article{Max2015,
	title = {Implementation of multidimensional databases in column-oriented {NoSQL} systems},
	volume = {9282},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-23135-8_6},
	doi = {10.1007/978-3-319-23135-8_6},
	abstract = {NoSQL (Not Only SQL) systems are becoming popular due to known advantages such as horizontal scalability and elasticity. In this paper, we study the implementation of multidimensional data warehouses with column- oriented NoSQL systems. We define mapping rules that transform the concep- tual multidimensional data model to logical column-oriented models. We con- sider three different logical models and we use them to instantiate data ware- houses. We focus on data loading, model-to-model conversion and OLAP cu- boid computation.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Chevalier, Max and El Malki, Mohammed and Kopliku, Arlind and Teste, Olivier and Tournier, Ronan},
	year = {2015},
	note = {ISBN: 9783319231341},
	keywords = {Data warehouse design, Model transformation rules, Multidimensional modelling, NoSQL databases},
	pages = {79--91},
}

@misc{Cong2017,
	title = {{IEVQ}: {An} {Iterative} {Example}-{Based} {Visual} {Query} for {Pathology} {Database}},
	url = {http://link.springer.com/10.1007/978-3-319-57741-8_3},
	abstract = {Microscopic image analysis of nuclei in pathology images generates tremendous amount of spatially derived data to support biomedical research and potential diagnosis. Such spatial data can be managed by traditional SQL based spatial databases and queried by SQL for spatial relationships. However, traditional spatial databases are designed for structured data with limited expressibility, which is difficult to support queries for complex visual patterns. Moreover, SQL based queries are not intuitive for biomedical researchers or pathologists. In this paper, we investigate the expressive power of visual query for spatial databases and propose an effective yet general Iterative Example-based Visual Query (IEVQuery) framework to query shapes and distributions. More specifically, we extract features from nuclei in pathology databases, such as shape polygon nuclei density distribution, and nuclei growth directions to build search indexes. The user employs visual interactions such as sketching to input queries for interesting patterns. Meanwhile, the user is allowed to iteratively create queries, which are based on previous search results, to finely tune the features more accurately to find preferred results. We build a system to enable users to specify sketch based queries interactively for (1) nuclei shapes, (2) nuclei densities, and (3) nuclei growth directions. To validate our methods, we take a pathology database [11] consisting of hundreds of millions of nuclei, and enable the user to search in the database to find most matching results through our system.},
	author = {Xie, Cong and Zhong, Wen and Kong, Jun and Xu, Wei and Mueller, Klaus and Wang, Fusheng},
	year = {2017},
	doi = {10.1007/978-3-319-57741-8_3},
	note = {Pages: 29-42
Publication Title: Data Management and Analytics for Medicine and Healthcare},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
}

@misc{Pablo2016,
	title = {A {Unified} {Internal} {Representation} of the {Outer} {World} for {Social} {Robotics}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-27149-1_57},
	author = {Bandera, Pablo BustosLuis J MansoJuan P BanderaAdrián Romero-GarcésLuis V CalderitaRebeca MarfilAntonio},
	year = {2016},
	doi = {10.1007/978-3-319-27149-1_57},
	note = {Publication Title: Robot 2015: Second Iberian Robotics Conference},
}

@misc{Alexandros2017,
	title = {Does {Online} {Evaluation} {Correspond} to {Offline} {Evaluation} in {Query} {Auto} {Completion}?},
	url = {http://link.springer.com/10.1007/978-3-319-56608-5_70},
	abstract = {Query Auto Completion is the task of suggesting queries to the users of a search engine while they are typing a query in the search box. Over the recent years there has been a renewed interest in research on improving the quality of this task. The published improvements were assessed by using offline evaluation techniques and metrics. In this paper, we provide a comparison of online and offline assessments for Query Auto Completion. We show that there is a large potential for significant bias if the raw data used in an online experiment is re-used for offline experiments afterwards to evaluate new methods.},
	author = {Bampoulidis, Alexandros and Palotti, João and Lupu, Mihai and Brassey, Jon and Hanbury, Allan},
	year = {2017},
	doi = {10.1007/978-3-319-56608-5_70},
	note = {Pages: 713-719
Publication Title: Advances in Information Retrieval},
	keywords = {UserStudy:yes, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Evaluation Research},
}

@article{Alfredo2009,
	title = {{CAMS}: {OLAPing} multidimensional data streams efficiently},
	volume = {5691 LNCS},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-03730-6_5},
	doi = {10.1007/978-3-642-03730-6_5},
	abstract = {In the context of data stream research, taming the multidimensionality of real-life data streams in order to efficiently support OLAP analysis/mining tasks is a critical challenge. Inspired by this fundamental motivation, in this paper we introduce CAMS (C ube-based A cquisition model for M ultidimensional S treams), a model for efficiently OLAPing multidimensional data streams. CAMS combines a set of data stream processing methodologies, namely (i) the OLAP dimension flattening process, which allows us to obtain dimensionality reduction of multidimensional data streams, and (ii) the OLAP stream aggregation scheme, which aggregates data stream readings according to an OLAP-hierarchy-based membership approach. We complete our analytical contribution by means of experimental assessment and analysis of both the efficiency and the scalability of OLAPing capabilities of CAMS on synthetic multidimensional data streams. Both analytical and experimental results clearly connote CAMS as an enabling component for next-generation Data Stream Management Systems. © 2009 Springer Berlin Heidelberg.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Cuzzocrea, Alfredo},
	year = {2009},
	note = {ISBN: 3642037291},
	pages = {48--62},
}

@inproceedings{Marco2016,
	title = {{IVIS4BigData}: {A} reference model for advanced visual interfaces supporting big data analysis in virtual research environments},
	volume = {10084 LNCS},
	isbn = {978-3-319-50069-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-50070-6_1},
	doi = {10.1007/978-3-319-50070-6_1},
	abstract = {This paper introduces an approach to develop an up-to-date reference model that can support advanced visual user interfaces for distributed Big Data Analysis in virtual labs to be used in e-Science, industrial research, and Data Science education. The paper introduces and motivates the current situation in this application area as a basis for a corresponding problem statement that is utilized to derive goals and objectives of the approach. Furthermore, the relevant state-of-the-art is revisited and remaining challenges are identified. An exemplar set of use cases, corresponding user stereotypes as well as a conceptual design model to address these challenges are introduced. A corresponding architectural system model is suggested as a conceptual reference architecture to support proof-of-concept implementations as well as to support interoperability in distributed infrastructures. Conclusions and an outlook on future work complete the paper. © Springer International Publishing AG 2016.},
	booktitle = {Advanced {Visual} {Interfaces}. {Supporting} {Big} {Data} {Applications}},
	author = {Bornschlegl, Marco X. and Berwind, Kevin and Kaufmann, Michael and Engel, Felix C. and Walsh, Paul and Hemmje, Matthias L. and Riestra, Ruben},
	year = {2016},
	note = {ISSN: 16113349},
	keywords = {Distributed Big Data Analysis, Information visualization, Visualization, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Philosophical Paper},
	pages = {1--18},
}

@inproceedings{Jonas2013,
	title = {Visual {Data} {Exploration} {Using} {Webbles}},
	volume = {372},
	isbn = {978-3-642-38835-4},
	url = {http://dx.doi.org/10.1007/978-3-642-38836-1_10},
	doi = {10.1007/978-3-642-38836-1_10},
	abstract = {We describe a system for visual exploration of data built using pluggable software components called Webbles. The system specifies a small common interface, a set of slots the plugins are expected to have, and any Webble following this interface can be plugged in at runtime. The system contains several types of visualization components, some built from scratch, some built by writing Webble wrappers for existing software, and some built by writing small interface wrappers for existing Webbles. The visualization components allow for interactive exploration of data, and selections or grouping of data in one visualization component are propagated to other components automatically. Interaction is done through direct manipulation of the visualization results.},
	booktitle = {Webble {Technology}},
	author = {Sjöbergh, Jonas and Tanaka, Yuzuru},
	year = {2013},
	note = {ISSN: 18650929},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {119--128},
}

@article{Patrice2014,
	title = {A trace-based approach to identifying users’ engagement and qualifying their engaged-behaviours in interactive systems: application to a social game},
	volume = {24},
	url = {http://link.springer.com/article/10.1007/s11257-014-9150-2},
	doi = {10.1007/s11257-014-9150-2},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Lavoué, Patrice BouvierKarim SehabaÉlise},
	year = {2014},
}

@misc{Assitan2013,
	title = {A {Trace} {Analysis} {Based} {Approach} for {Modeling} {Context} {Components}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-40972-1_31},
	author = {Mille, Assitan TraoréHélène TattegrainAlain},
	year = {2013},
	doi = {10.1007/978-3-642-40972-1_31},
	note = {Publication Title: Modeling and Using Context},
}

@misc{Ahmed2002,
	title = {Distributed {Video} {Documents} {Indexing} and {Content}-{Based} {Retrieving}},
	url = {http://link.springer.com/chapter/10.1007/3-540-36166-9_17},
	author = {Favory, Ahmed MostefaouiLoic},
	year = {2002},
	doi = {10.1007/3-540-36166-9_17},
	note = {Publication Title: Protocols and Systems for Interactive Distributed Multimedia},
}

@misc{Rong2012,
	title = {Deterministic {View} {Selection} for {Data}-{Analysis} {Queries}: {Properties} and {Algorithms}},
	url = {http://link.springer.com/10.1007/978-3-642-33074-2_15},
	abstract = {The view-selection problem is a combinatorial optimization problem that arises in the context of on-line analytical processing (OLAP) in database management systems. We pose the problem as an integer programming (IP) model, study its structural properties, and propose effective techniques for reducing the search space of views and thus the size of the corresponding IP model. We then use these results to design both exact methods and heuristic algorithms that are effective for solving relatively large realistic-size instances of the problem.},
	author = {Huang, Rong and Chirkova, Rada and Fathi, Yahya},
	year = {2012},
	doi = {10.1007/978-3-642-33074-2_15},
	note = {Pages: 195-208
Publication Title: Advances in Databases and Information Systems},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Evaluation Research},
}

@article{Frank2015,
	title = {A time-series compression technique and its application to the smart grid},
	volume = {24},
	issn = {1066-8888},
	url = {http://link.springer.com/10.1007/s00778-014-0368-8},
	doi = {10.1007/s00778-014-0368-8},
	abstract = {Time-series data is increasingly collected in many domains. One example is the smart electricity infrastructure, which generates huge volumes of such data from sources such as smart electricity meters. Although today these data are used for visualization and billing in mostly 15-min resolution, its original temporal resolution frequently is more fine-grained, e.g., seconds. This is useful for various analytical applications such as short-term forecasting, disaggregation and visualization. However, transmitting and storing huge amounts of such fine-grained data are prohibitively expensive in terms of storage space in many cases. In this article, we present a compression technique based on piecewise regression and two methods which describe the performance of the compression. Although our technique is a general approach for time-series compression, smart grids serve as our running example and as our evaluation scenario. Depending on the data and the use-case scenario, the technique compresses data by ratios of up to factor 5,000 while maintaining its usefulness for analytics. The proposed technique has outperformed related work and has been applied to three real-world energy datasets in different scenarios. Finally, we show that the proposed compression technique can be implemented in a state-of-the-art database management system.},
	number = {2},
	journal = {The VLDB Journal},
	author = {Eichinger, Frank and Efros, Pavel and Karnouskos, Stamatis and Böhm, Klemens},
	year = {2015},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {193--218},
}

@article{Guoliang2008,
	title = {Retune: {Retrieving} and materializing tuple units for effective keyword search over relational databases},
	volume = {5231 LNCS},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-87877-3_34},
	doi = {10.1007/978-3-540-87877-3-34},
	abstract = {The existing approaches of keyword search over relational databases always identify the relationships between tuples on the fly, which are rather inefficient as such relational relationships are very rich in the underlying databases. Alternatively, this paper proposes an alternative way by retrieving and materializing tuple units for facilitating the online processing of keyword search. We first propose a novel concept of tuple units, which are composed of the relevant tuples connected by the primary-foreign-key relationships. We then demonstrate how to generate and materialize the tuple units, and the technique for generating the tuple units can be done by issuing SQL statements and thus can be performed directly on the underlying RDBMS without modification to the database engine. Finally, we examine the techniques of indexing and ranking to improve the search efficiency and search quality. We have implemented our method and the experimental results show that our approach achieves much better search performance, and outperforms the alternative literatures significantly.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Li, Guoliang and Feng, Jianhua and Zhou, Lizhu},
	year = {2008},
	note = {ISBN: 3540878769},
	pages = {469--483},
}

@article{Nieves2015,
	title = {A compact {RDF} store using suffix arrays},
	volume = {9309},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-23826-5_11},
	doi = {10.1007/978-3-319-23826-5_11},
	abstract = {RDF has become a standard format to describe resources in the Semantic Web and other scenarios. RDF data is composed of triples (subject, predicate, object), referring respectively to a resource, a property of that resource, and the value of such property. Compact storage schemes allow fitting larger datasets in main memory for faster processing. On the other hand, supporting efficient SPARQL queries on RDF datasets requires index data structures to accompany the data, which hampers compactness. As done for text collections, we introduce a selfindex for RDF data, which combines the data and its index in a single representation that takes less space than the raw triples and efficiently supports basic SPARQL queries. Our storage format, RDFCSA, builds on compressed suffix arrays. Although there exist more compact representations of RDF data, RDFCSA uses about half of the space of the raw data (and replaces it) and displays much more robust and predictable query times around 1–2 microseconds per retrieved triple. RDFCSA is 3 orders of magnitude faster than representations like MonetDB or RDF- 3X, while using the same space as the former and 6 times less space than the latter. It is also faster than the more compact representations on most queries, in some cases by 2 orders of magnitude.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Brisaboa, Nieves R. and Cerdeira-Pena, Ana and Fariña, Antonio and Navarro, Gonzalo},
	year = {2015},
	note = {ISBN: 9783319238258},
	pages = {103--115},
}

@article{Efthymia2016,
	title = {Incorporating change detection in the monitoring phase of adaptive query processing},
	volume = {7},
	issn = {1867-4828},
	url = {http://jisajournal.springeropen.com/articles/10.1186/s13174-016-0049-5},
	doi = {10.1186/s13174-016-0049-5},
	abstract = {Recent Big Data research typically emphasizes on the need to address the challenges stemming from the volume, velocity, variety and veracity aspects. However, another cross-cutting property of Big Data is volatility. In database technology, volatility is addressed with the help of adaptive query processing (AQP), which has become the dominant paradigm for executing queries in dynamic and/or streaming environments. As the characteristics of the runtime environment may vary significantly along time, AQP techniques employ a three-phase adaptivity loop to process the input queries, comprising feedback collection, analysis and re-optimization. In the monitoring phase, the standard approach is to collect feedback in a fixed-size sliding window. However, several problems arise when the techniques adopt a fixed-size sliding window for maintaining runtime collected feedback. In this work, we tackle this limitation and we propose a novel monitoring phase, which assesses the collected feedback rendering an AQP technique capable of taking more informed decisions during the subsequent phases. The proposed approach is non-intrusive to the state-of-the-art adaptivity loop and can adopt any state-of-the-art online change detection algorithm through its plug-and-play abstraction. Another contribution of this work is a novel algorithm for detecting changes in a filter’s drop probability, called β-CUSUM. The potential of the novel monitoring phase and of β-CUSUM is experimentally evaluated using both real-world and synthetic dataset},
	number = {1},
	journal = {Journal of Internet Services and Applications},
	author = {Tsamoura, Efthymia and Gounaris, Anastasios and Manolopoulos, Yannis},
	year = {2016},
	pages = {7},
}

@article{Kamesh2011,
	title = {Massive-scale {RDF} processing using compressed bitmap indexes},
	volume = {6809 LNCS},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-22351-8_30},
	doi = {10.1007/978-3-642-22351-8_30},
	abstract = {The Resource Description Framework (RDF) is a popular data model for representing linked data sets arising from the web, as well as large scientific data repositories such as UniProt. RDF data intrinsically represents a labeled and directed multi-graph. SPARQL is a query language for RDF that expresses subgraph pattern-finding queries on this implicit multigraph in a SQL-like syntax. SPARQL queries generate complex intermediate join queries; to compute these joins efficiently, this paper presents a new strategy based on bitmap indexes. We store the RDF data in column-oriented compressed bitmap structures, along with two dictionaries. We find that our bitmap index-based query evaluation approach is up to an order of magnitude faster the state-of-the-art system RDF-3X, for a variety of SPARQL queries on gigascale RDF data sets. © 2011 Springer-Verlag Berlin Heidelberg.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Madduri, Kamesh and Wu, Kesheng},
	year = {2011},
	note = {ISBN: 9783642223501},
	keywords = {RDF, SPARQL query optimization, large-scale data analysis},
	pages = {470--479},
}

@misc{Chung-Wen2008,
	title = {A {Tree}-{Based} {Approach} for {Event} {Prediction} {Using} {Episode} {Rules} over {Event} {Streams}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-85654-2_24},
	author = {Chen, Chung-Wen ChoYing ZhengYi-Hung WuArbee L P},
	year = {2008},
	doi = {10.1007/978-3-540-85654-2_24},
	note = {Publication Title: Database and Expert Systems Applications},
}

@article{Yueting2016,
	title = {D-{Ocean}: an unstructured data management system for data ocean environment},
	volume = {10},
	issn = {20952236},
	url = {http://link.springer.com/article/10.1007/s11704-015-5045-6},
	doi = {10.1007/s11704-015-5045-6},
	abstract = {Together with the big datamovement,many organizations collect their own big data and build distinctive applications. In order to provide smart services upon big data, massive variable data should be well linked and organized to form Data Ocean, which specially emphasizes the deep exploration of the relationships among unstructured data to support smart services. Currently, almost all of these applications have to deal with unstructured data by integrating various analysis and search techniques upon massive storage and processing infrastructure at the application level, which greatly increase the difficulty and cost of application development. This paper presents D-Ocean, an unstructured data management system for data ocean environment. D-Ocean has an open and scalable architecture, which consists of a core platform, pluggable components and auxiliary tools. It exploits a unified storage framework to store data in different kinds of data stores, integrates batch and incremental processing mechanisms to process unstructured data, and provides a combined search engine to conduct compound queries. Furthermore, a so-called RAISE process modeling is proposed to support the whole process of Repository, Analysis, Index, Search and Environment modeling, which can greatly simplify application development. The experiments and use cases in production demonstrate the efficiency and usability of D-Ocean.},
	number = {2},
	journal = {Frontiers of Computer Science},
	author = {Zhuang, Yueting and Wang, Yaoguang and Shao, Jian and Chen, Ling and Lu, Weiming and Sun, Jianling and Wei, Baogang and Wu, Jiangqin},
	year = {2016},
	keywords = {RAISE process modeling, cluster:Flexible Engines, index, layer:Database Layer, storage, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {353--369},
}

@article{Cristian2010,
	title = {Predicate-based indexing for desktop search},
	volume = {19},
	issn = {10668888},
	url = {http://link.springer.com/article/10.1007/s00778-010-0187-5},
	doi = {10.1007/s00778-010-0187-5},
	abstract = {Google and other products have revolutionized the way we search for information. There are, however, still a number of research challenges. One challenge that arises specifically in desktop search is to exploit the structure and semantics of documents, as defined by the application program that generated the data (e.g., Word, Excel, or Outlook). The current generation of search products does not understand these structures and therefore often returns wrong results. This paper shows how today’s search technology can be extended in order to take the specific semantics of certain structures into account. The key idea is to extend inverted file index structures with predicates which encode the circumstances under which certain keywords of a document become visible to a user. This paper provides a framework that allows to express the semantics of structures in documents and algorithms to construct enhanced, predicate-based indexes. Furthermore, this paper shows how keyword and phrase queries can be processed efficiently on such enhanced indexes. It is shown that the proposed approach has superior retrieval performance with regard to both recall and precision and has tolerable space and query running time overheads.},
	number = {5},
	journal = {VLDB Journal},
	author = {Duda, Cristian and Kossmann, Donald and Zhou, Chong},
	year = {2010},
	note = {ISBN: 1066-8888},
	keywords = {Databases, Desktop search, Information retrieval},
	pages = {735--758},
}

@article{Russell2001,
	title = {The {Architecture} of {ArcIMS}, a {Distributed} {Internet} {Map} {Server}},
	issn = {16113349},
	url = {http://159.226.100.157/sess_11293/http182dx.doi.org/10.1007/3-540-47724-1_20},
	doi = {10.1007/3-540-47724-1_20},
	abstract = {ESRI® ArcIMS® is an Internet Map Server software that facilitates authoring of maps, designing of Web sites using them, and their publication on the Internet. Its distributed architecture offers customization of server, plugability of components, scalability, load balancing, and fail over/recovery and facilitates 24x7 operations. This paper describes the architecture of ArcIMS 3, its components, and some of the live Web sites that use it. The paper also discusses some considerations for the next version of ArcIMS.},
	journal = {Advances in Spatial and Temporal Databases},
	author = {East, Russell and Goyal, Roop and Haddad, Art and Konovalov, Alexander and Rosso, Andrea and Tait, Mike and Theodore, Jay},
	year = {2001},
	note = {ISBN: 354042301X},
	pages = {387--403},
}

@article{Ronald2012,
	title = {Blink: {Not} your father's database!},
	volume = {126 LNBIP},
	issn = {18651348},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33500-6_1},
	doi = {10.1007/978-3-642-33500-6_1},
	abstract = {The Blink project's ambitious goals are to answer all Business Intelligence (BI) queries in mere seconds, regardless of the database size, with an extremely low total cost of ownership. It takes a very innovative and counter-intuitive approach to processing BI queries, one that exploits several disruptive hardware and software technology trends. Specifically, it is a new, workload-optimized DBMS aimed primarily at BI query processing, and exploits scale-out of commodity multi-core processors and cheap DRAM to retain a (copy of a) data mart completely in main memory. Additionally, it exploits proprietary compression technology and cache-conscious algorithms that reduce memory bandwidth consumption and allow most SQL query processing to be performed on the compressed data. Ignoring the general wisdom of the last three decades that the only way to scalably search large databases is with indexes, Blink always performs simple, "brute force" scans of the entire data mart in parallel on all nodes, without using any indexes or materialized views, and without any query optimizer to choose among them. The Blink technology has thus far been incorporated into two products: (1) an accelerator appliance product for DB2 for z/OS (on the "mainframe"), called the IBM Smart Analytics Optimizer for DB2 for z/OS, V1.1, which was generally available in November 2010; and (2) the Informix Warehouse Accelerator (IWA), a software-only version that was generally available in March 2011. We are now working on the next generation of Blink, called BLink Ultra, or BLU, which will significantly expand the "sweet spot" of Blink technology to much larger, disk-based warehouses and allow BLU to "own" the data, rather than copies of it. © 2012 Springer-Verlag.},
	journal = {Lecture Notes in Business Information Processing},
	author = {Barber, Ronald and Bendel, Peter and Czech, Marco and Draese, Oliver and Ho, Frederick and Hrle, Namik and Idreos, Stratos and Kim, Min Soo and Koeth, Oliver and Lee, Jae Gil and Li, Tianchao Tim and Lohman, Guy and Morfonios, Konstantinos and Mueller, Rene and Murthy, Keshava and Pandis, Ippokratis and Qiao, Lin and Raman, Vijayshankar and Szabo, Sandor and Sidle, Richard and Stolze, Knut},
	year = {2012},
	note = {ISBN: 9783642334993},
	keywords = {Business Intelligence, OLAP, cluster:Query Approximation, data dictionary, data mart, database management system, encoded data, main-memory, memory bandwidth, multi-core, query processing},
	pages = {1--22},
}

@article{Leonidas2016,
	title = {Incremental stream processing of nested-relational queries},
	volume = {9827},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-44403-1_19},
	doi = {10.1007/978-3-319-44403-1_19},
	abstract = {Current work on stream processing is focused on approximation techniques that calculate approximate answers to simple queries by focusing on a fixed or sliding window that contains the most recent tuples from an input stream and by using condensed synopses to summarize the state. It is widely believed that without using approximation techniques, most interesting queries would be blocking (i.e., they would have to wait for the end of stream to release their results) or unbounded (i.e., their memory requirements would grow proportionally to the stream size, which may be infinite). The goal of this paper is to convert nested-relational queries to incremental stream processing programs automatically. In contrast to most current stream processing systems that calculate approximate answers, our system derives incremental programs that return accurate results. This is accomplished by retaining a state during the query evaluation lifetime and by using incremental evaluation techniques to return an accurate snapshot answer at each time interval that depends on the current state and the data in the current fixed window. Our methods can handle most forms of declarative queries on nested data collections, including arbitrarily nested queries, group-by with aggregation, and equi-joins. We report on a prototype system implementation and we show some preliminary results on evaluating queries on a small computer cluster running Spark.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Fegaras, Leonidas},
	year = {2016},
	pmid = {4520227},
	note = {arXiv: 0909.3169v1
ISBN: 9783319444024},
	pages = {305--320},
}

@article{Peter2015,
	title = {Big data in manufacturing: a systematic mapping study},
	volume = {2},
	issn = {2196-1115},
	url = {http://www.journalofbigdata.com/content/2/1/20},
	doi = {10.1186/s40537-015-0028-x},
	abstract = {The manufacturing industry is currently in the midst of a data-driven revolution, which promises to transform traditional manufacturing facilities in to highly optimised smart manufacturing facilities. These smart facilities are focused on creating manufacturing intelligence from real-time data to support accurate and timely decision-making that can have a positive impact across the entire organisation. To realise these efficiencies emerging technologies such as Internet of Things (IoT) and Cyber Physical Systems (CPS) will be embedded in physical processes to measure and monitor real-time data from across the factory, which will ultimately give rise to unprecedented levels of data production. Therefore, manufacturing facilities must be able to manage the demands of exponential increase in data production, as well as possessing the analytical techniques needed to extract meaning from these large datasets. More specifically, organisations must be able to work with big data technologies to meet the demands of smart manufacturing. However, as big data is a relatively new phenomenon and potential applications to manufacturing activities are wide-reaching and diverse, there has been an obvious lack of secondary research undertaken in the area. Without secondary research, it is difficult for researchers to identify gaps in the field, as well as aligning their work with other researchers to develop strong research themes. In this study, we use the formal research methodology of systematic mapping to provide a breadth-first review of big data technologies in manufacturing.},
	number = {1},
	journal = {Journal of Big Data},
	author = {O’Donovan, Peter and Leahy, Kevin and Bruton, Ken and O’Sullivan, Dominic T. J.},
	year = {2015},
	pages = {20},
}

@misc{Céline2007,
	title = {A {Unified} {View} of {Objective} {Interestingness} {Measures}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-73499-4_40},
	author = {Crémilleux, Céline HébertBruno},
	year = {2007},
	doi = {10.1007/978-3-540-73499-4_40},
	note = {Publication Title: Machine Learning and Data Mining in Pattern Recognition},
}

@misc{Lei2016,
	title = {Spica: {A} {Path} {Bundling} {Model} for {Rational} {Route} {Recommendation}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-45817-5_7},
	author = {Yu, Lei LvYang LiuXiaohui},
	year = {2016},
	doi = {10.1007/978-3-319-45817-5_7},
	note = {Publication Title: Web Technologies and Applications},
}

@book{Kasun2015,
	title = {Modeling {Large} {Time} {Series} for {Efficient} {Approximate} {Query} {Processing}},
	volume = {9052},
	isbn = {978-3-319-22324-7 978-3-319-22323-0},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-22324-7_16},
	abstract = {Evolving customer requirements and increasing competition force business organizations to store increasing amounts of data and query them for information at any given time. Due to the current growth of data volumes, timely extraction of relevant information becomes more and more difficult with traditional methods. In addition, contemporary Decision Support Systems (DSS) favor faster approximations over slower exact results. Generally speaking, processes that require exchange of data become inefficient when connection bandwidth does not increase as fast as the volume of data. In order to tackle these issues, compression techniques have been introduced in many areas of data processing. In this paper, we outline a new system that does not query complete datasets but instead utilizes models to extract the requested information. For time series data we use Fourier and Cosine transformations and piece-wise aggregation to derive the models. These models are initially created from the original data and are kept in the database along with it. Subsequent queries are answered using the stored models rather than scanning and processing the original datasets. In order to support model query processing, we maintain query statistics derived from experiments and when running the system. Our approach can also reduce communication load by exchanging models instead of data. To allow seamless integration of model-based querying into traditional data warehouses, we introduce a SQL compatible query terminology. Our experiments show that querying models is up to 80 \% faster than querying over the raw data while retaining a high accuracy.},
	author = {Perera, Kasun S and Hahmann, Martin and Lehner, Wolfgang and Pedersen, Torben Bach and Thomsen, Christian},
	year = {2015},
	doi = {10.1007/978-3-319-22324-7_16},
	note = {Publication Title: Database Systems for Advanced Applications},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
}

@misc{Yuan2015,
	title = {A {Two} {Tiered} {Finite} {Mixture} {Modelling} {Framework} to {Cluster} {Customers} on {EFTPOS} {Network}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-26350-2_24},
	author = {Rumantir, Yuan JinGrace},
	year = {2015},
	doi = {10.1007/978-3-319-26350-2_24},
	note = {Publication Title: AI 2015: Advances in Artificial Intelligence},
}

@inproceedings{Martin2013,
	title = {Visual {Analytics} in {Environmental} {Research}: {A} {Survey} on {Challenges}, {Methods} and {Available} {Tools}},
	volume = {413},
	isbn = {978-3-642-41150-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-41151-9_58},
	doi = {10.1007/978-3-642-41151-9_58},
	abstract = {Visual analytics approaches bring an innovative and effective way how to{\textbackslash}ndeliver the knowledge from a particular domain to an individual user.{\textbackslash}nWith the use of visual analytics methods we can easily discover the{\textbackslash}nunexpected relations and interesting patterns, which are hidden in the{\textbackslash}nhuge data warehouses. It builds on the human mind's ability to{\textbackslash}nunderstand the complex visualization of information. In this paper we{\textbackslash}nintroduce the potential usefulness of visual analytics for researchers{\textbackslash}nworking in the field of environmental informatics. Current challenges{\textbackslash}nbeyond the survey are described here, including the summary of{\textbackslash}nparticular well-proven tools and scenarios, which can be applied in many{\textbackslash}nvarious fields of environmental research.},
	booktitle = {Environmental {Software} {Systems}. {Fostering} {Information} {Sharing}},
	author = {Komenda, Martin and Schwarz, Daniel},
	year = {2013},
	note = {ISSN: 18684238},
	keywords = {Visualization, cluster:Visual Optimizations, environmental data visualization, human cognition, layer:User Interaction, learning analytics, supercluster:Data Visualization, type:Philosophical Paper},
	pages = {618--629},
}

@misc{B.2017,
	title = {A {Unified} {Approach} for {Learning} {Expertise} and {Authority} in {Digital} {Libraries}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-55699-4_22},
	author = {de La RobertieL. ErmakovaY. PitarchA. TakasuO. Teste, B},
	year = {2017},
	doi = {10.1007/978-3-319-55699-4_22},
	note = {Publication Title: Database Systems for Advanced Applications},
}

@inproceedings{PeterBaumann2013,
	title = {The array database that is not a database: {File} based array query answering in rasdaman},
	volume = {8098 LNCS},
	isbn = {978-3-642-40234-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-40235-7_32},
	doi = {10.1007/978-3-642-40235-7_32},
	abstract = {Array DBMSs extend the set of supported data structures in databases with (potentially large) multi-dimensional arrays. This information category actually comprises a core data structure in many scientific applications. When it comes to Petabyte archives, storage costs prohibit importing (i.e., copying) such data into a database. Therefore, in-situ processing of database queries is required, that is: evaluating queries on the original files, without previous insertion into the database. We have implemented such an in-situ feature for the rasdaman Array DBMS. In this demonstration, we show with rasdaman how query processing in array databases can simultaneously rely on arrays stored in the database — as usual — and in operating system files, like preexisting archives.},
	booktitle = {Advances in {Spatial} and {Temporal} {Databases}},
	author = {Baumann, Peter and Dumitru, Alex Mircea and Merticariu, Vlad},
	year = {2013},
	note = {ISSN: 03029743},
	keywords = {RRaw: Yes, cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
	pages = {478--483},
}

@article{Ekaterina2015,
	title = {Visualizing {Big} {Data} with augmented and virtual reality: challenges and research agenda},
	volume = {2},
	issn = {2196-1115},
	url = {http://www.journalofbigdata.com/content/2/1/22},
	doi = {10.1186/s40537-015-0031-2},
	abstract = {This paper provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification{\textbackslash}n of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disadvantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applications in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented information in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Olshannikova, Ekaterina and Ometov, Aleksandr and Koucheryavy, Yevgeni and Olsson, Thomas},
	year = {2015},
	note = {ISBN: 21961115 (Linking)},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Evaluation Research},
	pages = {22},
}

@article{Tapio2011,
	title = {Special issue on data analysis methodologies for intelligent systems: guest editor’s introduction},
	volume = {37},
	url = {http://link.springer.com/article/10.1007/s10844-011-0177-0},
	doi = {10.1007/s10844-011-0177-0},
	journal = {Journal of Intelligent Information Systems},
	author = {Rauch, Tapio ElomaaPetr BerkaJan},
	year = {2011},
}

@inproceedings{Baoyuan2011,
	title = {{ZoomTree}: {Unrestricted} zoom paths in multiscale visual analysis of relational databases},
	volume = {229 CCIS},
	isbn = {978-3-642-25381-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-25382-9_21},
	doi = {10.1007/978-3-642-25382-9_21},
	abstract = {Unrestricted zoom paths are much desired to gain deep understandings during visual analysis of relational databases. We present a multiscale visualization system supporting unrestricted zoom paths. Our system has a flexible visual interface on the client side, called “ZoomTree”, and a powerful and efficient back end with GPU-based parallel online data cubing and CPU-based data clustering. Zoom-trees are seamlessly integrated with a table-based overview using “hyperlinks” embedded in the table, and are designed to represent the entire history of a zooming process that reveals multiscale data characteristics. Arbitrary branching and backtracking in a zoom-tree are made possible by our fast parallel online cubing algorithm for partially materialized data cubes. Partial materialization provides a good tradeoff among preprocessing time, storage and online query time. Experiments and a user study have confirmed the effectiveness of our design.},
	booktitle = {Computer {Vision}, {Imaging} and {Computer} {Graphics}. {Theory} and {Applications}},
	author = {Wang, Baoyuan and Chen, Gang and Bu, Jiajun and Yu, Yizhou},
	year = {2011},
	note = {ISSN: 18650929},
	keywords = {UserStudy:yes, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {299--317},
}

@article{Sylvain2014,
	title = {A transformation-based approach to context-aware modelling},
	volume = {13},
	url = {http://link.springer.com/article/10.1007/s10270-012-0239-y},
	doi = {10.1007/s10270-012-0239-y},
	journal = {Software \& Systems Modeling},
	author = {den BerghTom Mens, Sylvain DegrandsartSerge DemeyerJan Van},
	year = {2014},
}

@article{Sriram2014,
	title = {{DIRAQ}: scalable in situ data- and resource-aware indexing for optimized query performance},
	volume = {17},
	issn = {15737543},
	url = {http://link.springer.com/article/10.1007/s10586-014-0358-z},
	doi = {10.1007/s10586-014-0358-z},
	abstract = {Scientific data analytics in high-performance computing environments has been evolving along with the advancement of computing capabilities. With the onset of exascale computing, the increasing gap between compute performance and I/O bandwidth has rendered the traditional post-simulation processing a tedious process. Despite the challenges due to increased data production, there exists an opportunity to benefit from “cheap” computing power to perform query-driven exploration and visualization during simulation time. To accelerate such analyses, applications traditionally augment, post-simulation, raw data with large indexes, which are then repeatedly utilized for data exploration. However, the generation of current state-of-the-art indexes involves a compute- and memory-intensive processing, thus rendering them inapplicable in an in situ context. In this paper we propose DIRAQ, a parallel in situ, in network data encoding and reorganization technique that enables the transformation of simulation output into a query-efficient form, with negligible runtime overhead to the simulation run. DIRAQ’s effective core-local, precision-based encoding approach incorporates an embedded compressed index that is 3–6× smaller than current state-of-the-art indexing schemes. Its data-aware index adjustmentation improves performance of group-level index layout creation by up to 35 \% and reduces the size of the generated index by up to 27 \%. Moreover, DIRAQ’s in network index merging strategy enables the creation of aggregated indexes that speed up spatial-context query responses by up to 10× versus alternative techniques. DIRAQ’s topology-, data-, and memory-aware aggregation strategy results in efficient I/O and yields overall end-to-end encoding and I/O time that is less than that required to write the raw data with MPI collective I/O.},
	number = {4},
	journal = {Cluster Computing},
	author = {Lakshminarasimhan, Sriram and Zou, Xiaocheng and Boyuka, David A. and Pendse, Saurabh V. and Jenkins, John and Vishwanath, Venkatram and Papka, Michael E. and Klasky, Scott and Samatova, Nagiza F.},
	year = {2014},
	keywords = {Exascale computing, Indexing, Query processing, RDistributed: Yes, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {1101--1119},
}

@article{Feifei2014,
	title = {Scalable data summarization on big data},
	volume = {32},
	issn = {15737578},
	url = {http://link.springer.com/article/10.1007/s10619-014-7145-y},
	doi = {10.1007/s10619-014-7145-y},
	abstract = {Across different scientific domains, engineering disciplines, and application scenarios, increasingly, users have to deal with large-scale, diverse, feature-rich, and high-resolution data sets that allow for data-intensive decision-making. The so-called big data challenge is making a profound transformation in computing. Big data not only refers to data sets that are large in size, but also covers data sets that are complex in structures, high dimensional, distributed, and heterogeneous. An effective framework when working with big data is through data summaries, such as different sampling methods, histograms, sketches and synopses, low-rank subspace approximation, dimensionality reduction techniques, etc. Instead of operating on complex and large raw data directly, these tools enable the execution of various data analytics tasks through appropriate and carefully constructed summaries, which improve their efficiency and scalability. Though some of these topics have been well studied in the past, the big data phenomena opens doors for interesting new research. These challenges include, but are not limited to, how to quantify the accuracy and efficiency trade-off when summarizing big data in massively parallel and distributed environments, how to summarize features in complex heterogeneous data, how to address IO and system issues in a summarization process, how to reduce communication cost when building a summary for a large data set stored in a cluster of commodity machines (such as a key-value store), how to dynamically maintain a summary in an incremental fashion under arbitrary arrivals of new data. As a result, answering the big data challenge through scalable data summarization is becoming of paramount importance. This special section of the Distributed and Parallel Databases (DAPD) features a strong collection of five papers, selected from 12 submissions, representing recent advances in summarizing big data. These works present novel techniques for IO-sampling, building various kinds of summaries in parallel environments, and summary-based parallel query processing and online aggregation. The first paper presents a unified framework for building an ℓ0ℓ0-sampler, which is is to sample near-uniformly from the support set of a dynamic multiset. This problem has a variety of applications within data analysis, computational geometry and graph algorithms. This paper has provided rigorous analyses for initiating and building ℓ0ℓ0-samplers in the proposed framework, and empirically studied different ℓ0ℓ0-sampling algorithms under the proposed framework. The second paper presents the PF-OLA framework for efficient implementations of parallel online aggregations. Online aggregation provides estimates to the final result of a computation during the actual processing. The user can stop the computation as soon as the estimate is accurate enough, typically early in the execution. This allows for the interactive data exploration of the largest datasets. This paper introduces the PF-OLA framework for parallel online aggregation in which the estimation virtually does not incur any overhead on top of the actual execution. The third paper studies how to conduct principal component analysis (PCA) in parallel. PCA is a well known technique for dimensionality reduction and feature extraction for large high dimensional data. This work extends a previous sequential method to a highly parallel algorithm that can compute PCA in one pass on a large data set based on summarization matrices. They also study how to integrate the algorithm with a DBMS; their solution is based on a combination of parallel data set summarization via user-defined aggregations and calling the MKL parallel variant of the LAPACK library to solve singular value decomposition in RAM. The fourth paper investigates how to build entity-based summarization for web search results in efficient and scalable fashion using MapReduce. An useful technique to achieve advanced exploration to exploit the availability of structured (and semantic) data in Web search is to enrich it with entity mining over the full contents of the search results. This paper considers a general scenario of providing such services as meta-services (that is, layered over systems that support keywords search) without a-priori indexing of the underlying document collection(s). They show how to make such service feasible for large data using MapReduce. The fifth paper presents efficient algorithms for executing set similarity join on large probabilistic data in MapReduce. Set similarity is a useful summary operator. However, set similarity join over two large data sets is expensive, especially in working with large probabilistic data. This paper examines various parallel methods in executing set similarity joins over such data in MapReduce and shows interesting results in improving the scalability and performance over the baseline method. We would like to thank all of the authors who submitted papers to this special section for their high-quality contributions. We also thank the referees for their generous help and valuable suggestions. We are grateful to Professor Divyakant Agrawal and Amit P. Sheth, the Editor-in-Chiefs of DAPD, for their strong support for this special section.},
	number = {3},
	journal = {Distributed and Parallel Databases},
	author = {Li, Feifei and Nath, Suman},
	year = {2014},
	pages = {313--314},
}

@inproceedings{Tuan2013,
	title = {{TimeExplorer}: {Similarity} search time series by their signatures},
	volume = {8033 LNCS},
	isbn = {978-3-642-41913-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-41914-0_28},
	doi = {10.1007/978-3-642-41914-0_28},
	abstract = {The analysis of different time series is an important activity in many areas of science and engineering. In this paper, we introduce a new method (feature extraction for time series) and an application (TimeExplorer) for similarity-based time series querying. The method is based on eleven characterizations of line graphs presenting time series. These characterizations include measures, such as, means, standard deviations, differences, and periodicities. A similarity metric is then computed on these measures. Finally, we use the similarity metric to search for similar time series in the database.},
	booktitle = {Advances in {Visual} {Computing}},
	author = {Dang, Tuan Nhon and Wilkinson, Leland},
	year = {2013},
	note = {Issue: PART 1
ISSN: 03029743},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {280--289},
}

@inproceedings{Milena2012,
	title = {Data vaults: {A} symbiosis between database technology and scientific file repositories},
	volume = {7338 LNCS},
	isbn = {978-3-642-31234-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-31235-9_32},
	doi = {10.1007/978-3-642-31235-9_32},
	abstract = {In this short paper we outline the data vault, a database-attached external file repository. It provides a true symbiosis between a DBMS and existing file-based repositories. Data is kept in its original format while scalable processing functionality is provided through the DBMS facilities. In particular, it provides transparent access to all data kept in the repository through an (array-based) query language using the file-type specific scientific libraries. The design space for data vaults is characterized by requirements coming from various fields. We present a reference architecture for their realization in (commercial) DBMSs and a concrete implementation in MonetDB for remote sensing data geared at content-based image retrieval.},
	booktitle = {Scientific and {Statistical} {Database} {Management}},
	author = {Ivanova, Milena and Kersten, Martin and Manegold, Stefan},
	year = {2012},
	note = {ISSN: 03029743},
	keywords = {RRaw: Yes, cluster:Adaptive Loading, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution, ★},
	pages = {485--494},
}

@inproceedings{Jaqueline2016,
	title = {A graphical system for interactive creation and exploration of dynamic information visualization},
	volume = {9734},
	isbn = {978-3-319-40348-9},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-40349-6_21},
	doi = {10.1007/978-3-319-40349-6_21},
	abstract = {Because of the growing amount of data available for analysis today, it is common to deal with large data sets, often too complex to be interpreted in their brute form. That is why Information Visualization techniques exist, to facilitate the analysis and interaction with data by humans through graphical abstractions. Motivated by the need to allow end users the autonomy to generate and edit visualizations according to their need, this work aims to underscore the importance of end user participation in the creation and support of these graphical abstractions of data. For this purpose, it is developed InterVis – a system for interactive creation of Information Visualizations based on dynamic data. The system is tested to verify whether this interactive creation of Infor-mation Visualizations, without programming, allied to the user knowledge of each application's domain, will be more efficient from the perspective of usability without significant loss of flexibility, as expected.},
	booktitle = {International {Conference} on {Human} {Interface} and the {Management} of {Information}},
	author = {Zaia, Jaqueline and Bernardes, João Luiz},
	year = {2016},
	note = {ISSN: 16113349},
	keywords = {Data abstraction, Graphical user interface (GUI), Information visualization, RInteractive: Yes, UserStudy:yes, cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {214--225},
}

@article{Radim2013,
	title = {Optimal and efficient generalized twig pattern processing: {A} combination of preorder and postorder filterings},
	volume = {22},
	issn = {10668888},
	url = {http://link.springer.com/article/10.1007/s00778-012-0295-5},
	doi = {10.1007/s00778-012-0295-5},
	abstract = {Searching for occurrences of a twig pattern query (TPQ) in an XML document is a core task of all XML database query languages. The generalized twig pattern (GTP) extends the TPQ model to include semantics related to output nodes, optional nodes, and boolean expressions which are part of the XQuery language. Preorder filtering holistic algorithms such as TwigStack represent a significant class of TPQ processing approaches with a linear worst-case I/O complexity with respect to the sum of the input and output sizes for some query classes. Another important class of holistic approaches is represented by postorder filtering holistic algorithms such as Twig2 Twig2Stack which introduced a linear output enumeration time with respect to the result size. In this article, we introduce a holistic algorithm called GTPStack which is the first approach capable of processing a GTP with a linear worst-case I/O complexity with respect to the GTP result size. This is achieved by using a combination of the preorder and postorder filterings before storing nodes in an intermediate storage. Additionally, another contribution of this article is an introduction of a new perspective of holistic algorithm optimality. We show that the optimality depends not only on a query class but also on XML document characteristics. This new view on the optimality extends the general knowledge about the type of queries for which the holistic algorithms are optimal. Moreover, it allows us to determine that GTPStack is optimal for any GTP when a specific XML document is considered. We present a comprehensive experimental study of the state-of-the-art holistic algorithms showing under which conditions GTPStack outperforms the other holistic approaches.},
	number = {3},
	journal = {VLDB Journal},
	author = {Bača, Radim and Krátký, Michal and Ling, Tok Wang and Lu, Jiaheng},
	year = {2013},
	note = {ISBN: 0077801202},
	keywords = {Generalized twig pattern, Holistic algorithms, Query processing},
	pages = {369--393},
}

@misc{Altaf2006,
	title = {The {Anatomy} of a {Stream} {Processing} {System}},
	url = {http://link.springer.com/chapter/10.1007/11788911_20},
	author = {Chakravarthy, Altaf GilaniSatyajeet SonuneBalakumar KendaiSharma},
	year = {2006},
	doi = {10.1007/11788911_20},
	note = {Publication Title: Flexible and Efficient Information Handling},
}

@inproceedings{Dimitrios2014,
	title = {The case for multi-engine data analytics},
	volume = {8374 LNCS},
	isbn = {978-3-642-54419-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-54420-0_40},
	doi = {10.1007/978-3-642-54420-0_40},
	abstract = {As big data analytics have become an important driver for ICT development, a large variety of approaches that apply these advanced technologies on a wide spectrum of applications has been introduced. In this paper we argue on the need of a multi-engine environment that will exploit the largely different models, cost and quality of the existing analytics engines. Such an environment further requires an intelligent management system for orchestrating and coordinating complex analytics tasks over the different available engines. After summarizing some of the current approaches in data analytics, we outline the structure of our envisioned Multi-Engine Management System and present some of the corresponding research directions in its design and development. \&copy; 2014 Springer-Verlag Berlin Heidelberg.},
	booktitle = {Euro-{Par} 2013: {Parallel} {Processing} {Workshops}},
	author = {Tsoumakos, Dimitrios and Mantas, Christos},
	year = {2014},
	note = {ISSN: 16113349},
	keywords = {Big Data, Datastores, Execution Engines, Modeling, Resource Scheduling, Visualization, cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Philosophical Paper},
	pages = {406--415},
}

@article{Dhrubajyoti2010,
	title = {A topology and context aware query distribution system in middleware for wireless sensor networks},
	volume = {90 CCIS},
	issn = {18650929},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-14493-6_30},
	doi = {10.1007/978-3-642-14493-6_30},
	journal = {Communications in Computer and Information Science},
	author = {Saha, Dhrubajyoti and Mallik, Dibyendu},
	year = {2010},
	note = {ISBN: 3642144926},
	pages = {284--295},
}

@inproceedings{Alberto2013,
	title = {Visual query specification and interaction with industrial engineering data},
	volume = {8034 LNCS},
	isbn = {978-3-642-41938-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-41939-3_6},
	doi = {10.1007/978-3-642-41939-3_6},
	abstract = {Nowadays, industrial engineering environments are typically characterized by sensors which stream massive amounts of different types of data. It is often difficult for industrial engineers to query, interact with, and interpret the data. In order to process many different kinds of distributed data stream sources originating from different kinds of data sources, a distributed federated data stream management system (FDSMS) is necessary. Although there exist some research efforts aimed at providing visual interfaces for querying temporal and real time data, there is virtually no existing work that provides a visual query specification and interaction interface that directly corresponds to a distributed federated data stream management system. This paper describes a visual environment that supports users in visually specifying queries and interacting with industrial engineering data. The visual environment comprises: a visual query specification and interaction environment, and a corresponding visual query language that runs on top of a distributed FDSMS. © 2013 Springer-Verlag.},
	booktitle = {Advances in {Visual} {Computing}},
	author = {Malagoli, Alberto and Leva, Mariano and Kimani, Stephen and Russo, Alessandro and Mecella, Massimo and Bergamaschi, Sonia and Catarci, Tiziana},
	year = {2013},
	note = {Issue: PART 2
ISSN: 03029743},
	keywords = {RDistributed: Yes, UserStudy:yes, Visualization, cluster:Novel Query Interfaces, data streams, industrial engineering, interaction, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {58--67},
}

@inproceedings{Luca2012,
	title = {Tsdb: {A} compressed database for time series},
	volume = {7189 LNCS},
	isbn = {978-3-642-28533-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-28534-9_16},
	doi = {10.1007/978-3-642-28534-9_16},
	abstract = {Large-scale network monitoring systems require efficient storage and consolidation of measurement data. Relational databases and popular tools such as the Round-Robin Database show their limitations when handling a large number of time series. This is because data access time greatly increases with the cardinality of data and number of measurements. The result is that monitoring systems are forced to store very few metrics at low frequency in order to grant data access within acceptable time boundaries. This paper describes a novel compressed time series database named tsdb whose goal is to allow large time series to be stored and consolidated in realtime with limited disk space usage. The validation has demonstrated the advantage of tsdb over traditional approaches, and has shown that tsdb is suitable for handling a large number of time series.},
	booktitle = {Traffic {Monitoring} and {Analysis}},
	author = {Deri, Luca and Mainardi, Simone and Fusco, Francesco},
	year = {2012},
	note = {ISSN: 03029743},
	keywords = {Large-scale datasets, cluster:Time Series, layer:Database Layer, supercluster:Indexes, time series, type:Proposal of Solution},
	pages = {143--156},
}

@article{Muhammad2016,
	title = {Big {Data} {Reduction} {Methods}: {A} {Survey}},
	volume = {1},
	issn = {2364-1185},
	url = {http://link.springer.com/10.1007/s41019-016-0022-0},
	doi = {10.1007/s41019-016-0022-0},
	abstract = {Research on big data analytics is entering in the new phase called fast data where multiple gigabytes of data arrive in the big data systems every second. Modern big data systems collect inherently complex data streams due to the volume, velocity, value, variety, variability, and veracity in the acquired data and consequently give rise to the 6Vs of big data. The reduced and relevant data streams are perceived to be more useful than collecting raw, redundant, inconsistent, and noisy data. Another perspective for big data reduction is that the million variables big datasets cause the curse of dimensionality which requires unbounded computational resources to uncover actionable knowledge patterns. This article presents a review of methods that are used for big data reduction. It also presents a detailed taxonomic discussion of big data reduction methods including the network theory, big data compression, dimension reduction, redundancy elimination, data mining, and machine learning methods. In addition, the open research issues pertinent to the big data reduction are also highlighted.},
	number = {4},
	journal = {Data Science and Engineering},
	author = {ur Rehman, Muhammad Habib and Liew, Chee Sun and Abbas, Assad and Jayaraman, Prem Prakash and Wah, Teh Ying and Khan, Samee U.},
	year = {2016},
	note = {ISBN: 4101901600},
	pages = {265--284},
}

@misc{Min-Huang2003,
	title = {A {Unified}, {Adjustable}, and {Extractable} {Biological} {Data} {Mining}-{Broker}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-45080-1_104},
	author = {Yuan, Min-Huang HoYue-Shan ChangMing-Chun ChengKuang-Lee LiShyan-Ming},
	year = {2003},
	doi = {10.1007/978-3-540-45080-1_104},
	note = {Publication Title: Intelligent Data Engineering and Automated Learning},
}

@inproceedings{Kostas2013,
	title = {End-{User} {Development} of {Information} {Visualization}},
	url = {http://link.springer.com/10.1007/978-3-642-38706-7_9},
	doi = {10.1007/978-3-642-38706-7_9},
	abstract = {This paper investigates End-User Development of Information Visualization. More specifically, we investigated how existing visualization tools allow end-user developers to construct visualizations. End-user developers have some developing or scripting skills to perform relatively advanced tasks such as data manipulation, but no formal training in programming. 18 visualization tools were surveyed from an end-user developer perspective. The results of this survey study show that end-user developers need better tools to create and modify custom visualizations. A closer collaboration between End-User Development and Information Visualization researchers could contribute towards the development of better tools to support custom visualizations. In addition, as empirical evaluations of these tools are lacking both research communities should focus more on this aspect. The study serves as a starting point towards the engagement of end-user developers in visualization development.},
	booktitle = {End-{User} {Development}},
	author = {Pantazos, Kostas and Lauesen, Soren and Vatrapu, Ravi},
	year = {2013},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Evaluation Research},
	pages = {104--119},
}

@article{Supriya2016,
	title = {A uniform representation of multi-variant data in intensive-query databases},
	volume = {12},
	issn = {16145054},
	url = {http://link.springer.com/10.1007/s11334-016-0275-9},
	doi = {10.1007/s11334-016-0275-9},
	abstract = {In this paper a new approach for the representation of multi-variant data is introduced. Current approaches consist on either hard-core coding techniques or conceptual / logical models to integrate structured and semi-structured data in customized, application-specific ways. The representation introduced here relies instead on unfolding technique to represent multi-variant data uniformly. This leads to a framework with core functionalities for organizing structured and semi-structured data. The paper presents also an efficient methodology towards retrieval of data from the proposed storage along with comparative performance analysis against existing practices. Accuracy, precision, and recall of the proposed technique are quantitatively evaluated and carefully reported.},
	number = {3},
	journal = {Innovations in Systems and Software Engineering},
	author = {Chakraborty, Supriya and Cortesi, Agostino and Chaki, Nabedu},
	year = {2016},
	keywords = {Domain-Index, Matching parameters},
	pages = {163--176},
}

@article{Lei2014,
	title = {{gStore}: a graph-based {SPARQL} query engine},
	volume = {23},
	url = {http://link.springer.com/article/10.1007/s00778-013-0337-7},
	doi = {10.1007/s00778-013-0337-7},
	abstract = {We address efficient processing of SPARQL queries over RDF datasets. The proposed techniques, incorporated into the gStore system, handle, in a uniform and scalable manner, SPARQL queries with wildcards and aggregate operators over dynamic RDF datasets. Our approach is graph based. We store RDF data as a large graph and also represent a SPARQL query as a query graph. Thus, the query answering problem is converted into a subgraph matching problem. To achieve efficient and scalable query processing, we develop an index, together with effective pruning rules and efficient search algorithms. We propose techniques that use this infrastructure to answer aggregation queries. We also propose an effective maintenance algorithm to handle online updates over RDF repositories. Extensive experiments confirm the efficiency and effectiveness of our solutions.},
	journal = {The VLDB Journal},
	author = {Zhao, Lei ZouM. Tamer ÖzsuLei ChenXuchuan ShenRuizhe HuangDongyan},
	year = {2014},
}

@article{Mohammed2000,
	title = {Parallel and {Distributed} {Data} {Mining}: {An} {Introduction}},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/3-540-46502-2_1},
	doi = {10.1007/3-540-46502-2_1},
	abstract = {The explosive growth in data collection in business and scienti c elds has literally forced upon us the need to analyze and mine useful knowledge from it. Data mining refers to the entire process of extracting useful and novel patterns/models from large datasets. Due to the huge size of data and amount of computation involved in data mining, high-performance computing is an essential component for any successful large-scale data mining application. This chapter presents a survey on large-scale parallel and distributed data mining algorithms and systems, serving as an introduction to the rest of this volume. It also discusses the issues and challenges that must be overcome for designing and implementing successful tools for large-scale data mining.},
	journal = {Genome},
	author = {Zaki, Mohammed J},
	year = {2000},
	note = {ISBN: 0302-9743},
	pages = {1--23},
}

@inproceedings{Julia2011,
	title = {A {Discussion} on {Visual} {Interactive} {Data} {Exploration} {Using} {Self}-{Organizing} {Maps}},
	isbn = {978-3-642-21566-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-21566-7_18},
	doi = {10.1007/978-3-642-21566-7_18},
	abstract = {In recent years, a variety of visualization techniques for visual data exploration based on self-organizing maps (SOMs) have been developed. To support users in data exploration tasks, a series of software tools emerged which integrate various visualizations. However, the focus of most research was the development of visualizations which improve the support in cluster identification. In order to provide real insight into the data set it is crucial that users have the possibility of interactively investigating the data set. This work provides an overview of state-of-the-art software tools for SOM-based visual data exploration. We discuss the functionality of software for specialized data sets, as well as for arbitrary data sets with a focus on interactive data exploration.},
	booktitle = {Advances in {Self}-{Organizing} {Maps}},
	author = {Moehrmann, Julia and Burkovski, Andre and Baranovskiy, Evgeny and Heinze, G.A. and Rapoport, Andrej and Heidemann, Gunther},
	year = {2011},
	keywords = {cluster:Visualization Tools, layer:User Interaction, supercluster:Data Visualization, type:Validation Research},
	pages = {178--187},
}

@article{Vit2010,
	title = {Exact indexing for massive time series databases under time warping distance},
	volume = {21},
	issn = {13845810},
	url = {http://link.springer.com/article/10.1007/s10618-010-0165-y},
	doi = {10.1007/s10618-010-0165-y},
	abstract = {Among many existing distance measures for time series data, Dynamic Time Warping (DTW) distance has been recognized as one of the most accurate and suitable distance measures due to its flexibility in sequence alignment. However, DTW distance calculation is computationally intensive. Especially in very large time series databases, sequential scan through the entire database is definitely impractical, even with random access that exploits some index structures since high dimensionality of time series data incurs extremely high I/O cost. More specifically, a sequential structure consumes high CPU but low I/O costs, while an index structure requires low CPU but high I/O costs. In this work, we therefore propose a novel indexed sequential structure called TWIST (Time Warping in Indexed Sequential sTructure) which benefits from both sequential access and index structure. When a query sequence is issued, TWIST calculates lower bounding distances between a group of candidate sequences and the query sequence, and then identifies the data access order in advance, hence reducing a great number of both sequential and random accesses. Impressively, our indexed sequential structure achieves significant speedup in a querying process by a few orders of magnitude. In addition, our method shows superiority over existing rival methods in terms of query processing time, number of page accesses, and storage requirement with no false dismissal guaranteed.},
	number = {3},
	journal = {Data Mining and Knowledge Discovery},
	author = {Niennattrakul, Vit and Ruengronghirunya, Pongsakorn and Ratanamahatana, Chotirat Ann},
	year = {2010},
	note = {arXiv: 0906.2459},
	keywords = {Dynamic time warping, Indexing, Time series, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {509--541},
}

@article{Wenyu2012,
	title = {A comparison of top-k temporal keyword querying over versioned text collections},
	volume = {7447 LNCS},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-32597-7_31},
	doi = {10.1007/978-3-642-32597-7_31},
	abstract = {As the web evolves over time, the amount of versioned text collections increases rapidly. Most web search engines will answer a query by ranking all known documents at the (current) time the query is posed. There are applications however (for example customer behavior analysis, crime investigation, etc.) that would need to efficiently query these sources as of some past time, that is, retrieve the results as if the user was posing the query in a past time instant, thus accessing data known as of that time. Ranking and searching over versioned documents considers not only keyword constraints but also the time dimension, most commonly, a time point or time range of interest. In this paper, we deal with top-k query evaluations with both keyword and temporal constraints over versioned textual documents. In addition to considering previous solutions, we propose novel data organization and indexing solutions: the first one partitions data along ranking positions, while the other maintains the full ranking order through the use of a multiversion ordered list. We present an experimental comparison for both time point and time interval constraints. For time-interval constraints, different querying definitions, such as aggregation functions and consistent top-k queries are evaluated. Experimental evaluations on large real world datasets demonstrate the advantages of the newly proposed data organization and indexing approaches.},
	number = {PART 2},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Huo, Wenyu and Tsotras, Vassilis J.},
	year = {2012},
	note = {ISBN: 9783642325960},
	pages = {360--374},
}

@inproceedings{Nigel2017,
	title = {Answering {Temporal} {Analytic} {Queries} over {Big} {Data} {Based} on {Precomputing} {Architecture}},
	url = {http://link.springer.com/10.1007/978-3-319-54472-4_27},
	doi = {10.1007/978-3-319-54472-4_27},
	abstract = {Big data explosion brings revolutionary changes to many aspects of our lives. Huge volume of data, along with its complexity poses big challenges to data analytic applications. Techniques proposed in data warehousing and online analytical processing (OLAP), such as precomputed multidimensional cubes, dramatically improve the response time of analytic queries based on relational databases. There are some recent works extending similar concepts into NoSQL such as constructing cubes from NoSQL stores and converting existing cubes into NoSQL stores. However, only few works are studying the precomputing structure deliberately within NoSQL databases. In this paper, we present an architecture for answering temporal analytic queries over big data by precomputing the results of granulated chunks of collections which are decomposed from the original large collection. By using the precomputing structure, we are able to answer the drill-down and roll-up temporal queries over large amount of data within reasonable response time.},
	booktitle = {Intelligent {Information} and {Database} {Systems}},
	author = {Franciscus, Nigel and Ren, Xuguang and Stantic, Bela},
	year = {2017},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {281--290},
}

@inproceedings{Philipp2015,
	title = {Querying {Time} {Interval} {Data}},
	url = {http://link.springer.com/10.1007/978-3-319-29133-8_3},
	doi = {10.1007/978-3-319-29133-8_3},
	abstract = {Analyzing huge amounts of time interval data is a task arising more and more frequently in different domains like resource utilization and scheduling, real time disposition, as well as health care. Analyzing this type of data using established, reliable, and proven technologies is desirable and required. However, utilizing commonly used tools and multidimensional models is not sufficient, because of modeling, querying, and processing limitations. In this paper, we address the problem of querying large data sets of time interval data, by introducing a query language capable to retrieve aggregated and analytical results from such a database. The introduced query language enables analysis of time interval data in an on-line analytical manner. It is based on requirements stated by business analysts from different domains. In addition, we introduce our query processing, established using a bitmap-based implementation. Finally, we present and critically discuss a performance analysis.},
	booktitle = {Enterprise {Information} {Systems}},
	author = {Meisen, Philipp and Keng, Diane and Meisen, Tobias and Recchioni, Marco and Jeschke, Sabina},
	year = {2015},
	keywords = {cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {45--68},
}

@article{Dushyanth2008,
	title = {Delay aware querying with {Seaweed}},
	volume = {17},
	issn = {10668888},
	url = {http://link.springer.com/article/10.1007/s00778-007-0060-3},
	doi = {10.1007/s00778-007-0060-3},
	abstract = {Large highly distributed data sets are poorly supported by current query technologies. Applications such as endsystem-based network management are characterized by data stored on large numbers of endsystems, with frequent local updates and relatively infrequent global one-shot queries. The challenges are scale (103 to 109 endsystems) and endsystem unavailability. In such large systems, a significant fraction of endsystems and their data will be unavailable at any given time. Existing methods to provide high data availability despite endsystem unavailability involve centralizing, redistributing or replicating the data. At large scale these methods are not scalable. We advocate a design that trades query delay for completeness, incrementally returning results as endsystems become available. We also introduce the idea of completeness prediction, which provides the user with explicit feedback about this delay/completeness trade-off. Completeness prediction is based on replication of compact data summaries and availability models. This metadata is orders of magnitude smaller than the data. Seaweed is a scalable query infrastructure supporting incremental results, online in-network aggregation and completeness prediction. It is built on a distributed hash table (DHT) but unlike previous DHT based approaches it does not redistribute data across the network. It exploits the DHT infrastructure for failure-resilient metadata replication, query dissemination, and result aggregation. We analytically compare Seaweed’s scalability against other approaches and also evaluate the Seaweed prototype running on a large-scale network simulator driven by real-world traces.},
	number = {2},
	journal = {The VLDB Journal},
	author = {Narayanan, Dushyanth and Donnelly, Austin and Mortier, Richard and Rowstron, Antony},
	year = {2008},
	note = {ISBN: 1595933859},
	keywords = {RDistributed: Yes, cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution, ★},
	pages = {315--331},
}

@article{Alessandro2014,
	title = {Beyond one billion time series: {Indexing} and mining very large time series collections with {iSAX2}+},
	volume = {39},
	issn = {02193116},
	url = {http://link.springer.com/article/10.1007/s10115-012-0606-6},
	doi = {10.1007/s10115-012-0606-6},
	abstract = {There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of time series. Exam-ples of such applications come from astronomy, biology, the web, and other domains. It is not unusual for these applications to involve numbers of time series in the order of hundreds of millions to billions. However, all relevant techniques that have been proposed in the literature so far have not considered any data collections much larger than one-million time series. In this paper, we describe iSAX 2.0 and its improvements, iSAX 2.0 Clustered and iSAX2+, three methods designed for indexing and mining truly massive collections of time series. We show that the main bottleneck in mining such massive datasets is the time taken to build the index, and we thus introduce a novel bulk loading mechanism, the first of this kind specifi-cally tailored to a time series index. We show how our methods allows mining on datasets that would otherwise be completely untenable, including the first published experiments to index one billion time series, and experiments in mining massive data from domains as diverse as entomology, DNA and web-scale image collections.},
	number = {1},
	journal = {Knowledge and Information Systems},
	author = {Camerra, Alessandro and Shieh, Jin and Palpanas, Themis and Rakthanmanon, Thanawin and Keogh, Eamonn},
	year = {2014},
	keywords = {Data mining, Indexing, Representations, Time series, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Evaluation Research},
	pages = {123--151},
}

@article{Cormac2011,
	title = {Facilitating casual users in interacting with linked data through domain expertise},
	volume = {6861 LNCS},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-23091-2_28},
	doi = {10.1007/978-3-642-23091-2_28},
	abstract = {Linked Data use has expanded rapidly in recent years; however there is still a lack of support for casual users to create complex queries over this Web of Data. Until this occurs, the real benefits of having such rich metadata available will not be realised by the general public. This paper introduces an approach to supporting casual users discover relevant information across multiple Linked Data repositories, by enabling them to leverage and tailor semantic attributes. Semantic attributes are semantically meaningful terms that encapsulate expert rules encoded in multiple formats, including SPARQL. Semantic attributes are created in SABer (Semantic Attribute Builder), which is usable by non-technical domain experts. This opens the approach to almost any domain. A detailed evaluation of SABer is described within this paper, as is a case study that shows how casual users can use semantic attributes to explore multiple structured data sources in the music domain. © 2011 Springer-Verlag Berlin Heidelberg.},
	number = {PART 2},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Hampson, Cormac and Conlan, Owen},
	year = {2011},
	note = {ISBN: 9783642230905},
	keywords = {Data Exploration, Domain Experts, Linked Data},
	pages = {319--333},
}

@misc{Peng2014,
	title = {A {Unified} {Semi}-supervised {Framework} for {Author} {Disambiguation} in {Academic} {Social} {Network}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-10085-2_1},
	author = {Xu, Peng WangJianyu ZhaoKai HuangBaowen},
	year = {2014},
	doi = {10.1007/978-3-319-10085-2_1},
	note = {Publication Title: Database and Expert Systems Applications},
}

@inproceedings{Yaying2011,
	title = {Research and application of query rewriting based on materialized views},
	volume = {86 CCIS},
	isbn = {978-3-642-19852-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-19853-3_12},
	doi = {10.1007/978-3-642-19853-3_12},
	abstract = {It is very important to answer queries quickly in database environment. Queries are transparently rewritten using materialized views and the time consumed by queries is reduced by avoiding accessing huge raw records and performing time-consuming operations in order to improve query speed. This paper discussed the extended query rewriting algorithm based on join relation with foreign key, studied Query System Based on Materialized Views for small databases, and proved its validity.},
	booktitle = {Information and {Automation}},
	author = {Hu, Yaying and Zhai, Weifang and Tian, Yulong and Gao, Tao},
	year = {2011},
	note = {ISSN: 18650929},
	keywords = {Foreign Key, Materialized View, Query Rewriting, cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Evaluation Research},
	pages = {85--91},
}

@article{Wei2010,
	title = {A {Tetrahedral} {Data} {Model} for {Unstructured} {Data} {Management}},
	volume = {53},
	issn = {1674-733X},
	url = {http://link.springer.com/10.1007/s11432-010-4030-9},
	doi = {10.1007/s11432-010-4030-9},
	abstract = {This paper proposes a tetrahedral datamodel for unstructured data management. The model defines the four components of unstructured data including: basic attributes, semantic characteristics, low-level features and raw data on its four facets, and the relations between these components. The internal implementation structure of the model and the data query language are designed and briefly introduced. This model provides a unified, integrated and associated description for different kinds of unstructured data, and supports intelligent data services such as associated retrieval and data mining. An example is given to demonstrate how to use the model for describing and manipulating data from a sample video base.},
	number = {8},
	journal = {Science China Information Sciences},
	author = {Li, Wei and Lang, Bo},
	year = {2010},
	keywords = {data model, intelligent services, query language},
	pages = {1497--1510},
}

@article{Ada2008,
	title = {Scaling and time warping in time series querying},
	volume = {17},
	issn = {10668888},
	url = {http://link.springer.com/article/10.1007/s00778-006-0040-z},
	doi = {10.1007/s00778-006-0040-z},
	abstract = {The last few years have seen an increasing understanding that dynamic time warping (DTW), a technique that allows local flexibility in aligning time series, is superior to the ubiquitous Euclidean distance for time series classification, clustering, and indexing. More recently, it has been shown that for some problems, uniform scaling (US), a technique that allows global scaling of time series, may just be as important for some problems. In this work, we note that for many real world problems, it is necessary to combine both DTW and US to achieve meaningful results. This is particularly true in domains where we must account for the natural variability of human actions, including biometrics, query by humming, motion-capture/animation, and handwriting recognition. We introduce the first technique which can handle both DTW and US simultaneously, our techniques involve search pruning by means of a lower bounding technique and multi-dimensional indexing to speed up the search. We demonstrate the utility and effectiveness of our method on a wide range of problems in industry, medicine, and entertainment.},
	number = {4},
	journal = {The VLDB Journal},
	author = {Fu, Ada Wai Chee and Keogh, Eamonn and Lau, Leo Yung Hang and Ratanamahatana, Chotirat Ann and Wong, Raymond Chi Wing},
	year = {2008},
	note = {ISBN: 1-59593-154-6},
	keywords = {Dynamic time warping, Nearest neighbor search, Scaled and warped matching, Subsequence matching, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {899--921},
}

@misc{Federico2008,
	title = {{SPYWatch}, {Overcoming} {Linguistic} {Barriers} in {Information} {Management}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-89900-6_8},
	author = {Priamo, Federico NeriAngelo},
	year = {2008},
	doi = {10.1007/978-3-540-89900-6_8},
	note = {Publication Title: Intelligence and Security Informatics},
}

@misc{Shoji2012,
	title = {A {Unified} {Framework} for {Modeling} and {Predicting} {Going}-{Out} {Behavior}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-31205-2_5},
	author = {Sato, Shoji TominagaMasamichi ShimosakaRui FukuiTomomasa},
	year = {2012},
	doi = {10.1007/978-3-642-31205-2_5},
	note = {Publication Title: Pervasive Computing},
}

@misc{Joong2015,
	title = {{SpreadView}: {A} {Multi}-touch {Based} {Multiple} {Contents} {Visualization} {Method} {Composed} of {Aligned} {Layers}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-20804-6_28},
	author = {Park, Joong Ho LeeHyoyoung KimJi-Hyung},
	year = {2015},
	doi = {10.1007/978-3-319-20804-6_28},
	note = {Publication Title: Distributed, Ambient, and Pervasive Interactions},
}

@misc{Peter1998,
	title = {Constraint databases: {A} survey},
	url = {http://link.springer.com/10.1007/BFb0035010},
	abstract = {Constraint databases generalize relational databases by finitely representable infinite relations. This paper surveys the state of the art in constraint databases: known results, remaining open problems and current research directions. The paper also describes a new algebra for databases with integer order constraints and a complexity analysis of evaluating queries in this algebra.},
	author = {Revesz, Peter Z.},
	year = {1997},
	doi = {10.1007/BFb0035010},
	note = {Pages: 209-246
Publication Title: Semantics in Databases},
}

@misc{Dominik2009,
	title = {Intelligent {Data} {Granulation} on {Load}: {Improving} {Infobright}’s {Knowledge} {Grid}},
	url = {http://link.springer.com/10.1007/978-3-642-10509-8_3},
	abstract = {One of the major aspects of Infobright’s relational database technology is automatic decomposition of each of data tables onto Rough Rows, each consisting of 64K of original rows. Rough Rows are automatically annotated by Knowledge Nodes that represent compact information about the rows’ values. Query performance depends on the quality of Knowledge Nodes, i.e., their efficiency in minimizing the access to the compressed portions of data stored on disk, according to the specific query optimization procedures. We show how to implement the mechanism of organizing the incoming data into such Rough Rows that maximize the quality of the corresponding Knowledge Nodes. Given clear business-driven requirements, the implemented mechanism needs to be fully integrated with the data load process, causing no decrease in the data load speed. The performance gain resulting from better data organization is illustrated by some tests over our benchmark data. The differences between the proposed mechanism and some well-known procedures of database clustering or partitioning are discussed. The paper is a continuation of our patent application [22].},
	author = {Ślęzak, Dominik and Kowalski, Marcin},
	year = {2009},
	doi = {10.1007/978-3-642-10509-8_3},
	note = {Pages: 12-25
Publication Title: Future Generation Information Technology},
	keywords = {cluster:Adaptive Storage, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution},
}

@article{Nikolay2015,
	title = {Big data normalization for massively parallel processing databases},
	volume = {9382},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-25747-1_16},
	doi = {10.1007/978-3-319-25747-1_16},
	abstract = {High performance querying and ad-hoc querying are commonly viewed as mutually exclusive goals in massively parallel processing databases. Furthermore, there is a contradiction between ease of extending the data model and ease of analysis. The modern 'Data Lake' approach, promises extreme ease of adding new data to a data model, however it is prone to eventually becoming a Data Swamp - unstructured, ungoverned, and out of control Data Lake where due to a lack of process, standards and governance, data is hard to find, hard to use and is consumed out of context. This paper introduces a novel technique, highly normalized Big Data using Anchor modeling, that provides a very efficient way to store information and utilize resources, thereby providing ad-hoc querying with high performance for the first time in massively parallel processing databases. This technique is almost as convenient for expanding data model as a Data Lake, while it is internally protected from transforming to Data Swamp. A case study of how this approach is used for a Data Warehouse at Avito over a three-year period, with estimates for and results of real data experiments carried out in HP Vertica, an MPP RDBMS, is also presented. This paper is an extension of theses from The 34th International Conference on Conceptual Modeling (ER 2015) (Golov and R??nnb??ck 2015) [1], it is complemented with numerical results about key operating areas of highly normalized big data warehouse, collected over several (1-3) years of commercial operation. Also, the limitations, imposed by using a single MPP database cluster, are described, and cluster fragmentation approach is proposed.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Golov, Nikolay and Rönnbäck, Lars},
	year = {2015},
	note = {ISBN: 9783319257464},
	keywords = {Big Data, Database, MPP, Modeling, Normalization, Performance, Querying},
	pages = {154--163},
}

@misc{Wang2014,
	title = {The {Application} of {Image} {Retrieval} {Technology} in the {Prevention} of {Diseases} and {Pests} in {Fruit} {Trees}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-54344-9_20},
	author = {Shu-han, Wang Zhi-junLiu XinJiang MengCheng},
	year = {2014},
	doi = {10.1007/978-3-642-54344-9_20},
	note = {Publication Title: Computer and Computing Technologies in Agriculture VII},
}

@inproceedings{Alexis2016,
	title = {Symbolic representation of time series: {A} hierarchical coclustering formalization},
	volume = {9785},
	isbn = {978-3-319-44411-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-44412-3_1},
	doi = {10.1007/978-3-319-44412-3_1},
	abstract = {The choice of an appropriate representation remains crucial for mining time series, particularly to reach a good trade-off between the dimensionality reduction and the stored information. Symbolic representations constitute a simple way of reducing the dimensionality by turning time series into sequences of symbols. SAXO is a data-driven symbolic representation of time series which encodes typical distributions of data points. This approach was first introduced as a heuristic algorithm based on a regularized coclustering approach. The main contribution of this article is to formalize SAXO as a hierarchical coclustering approach. The search for the best symbolic representation given the data is turned into a model selection problem. Comparative experiments demonstrate the benefit of the new formalization, which results in representations that drastically improve the compression of data while keeping useful information for classification tasks.},
	booktitle = {Advanced {Analysis} and {Learning} on {Temporal} {Data}},
	publisher = {Springer International Publishing},
	author = {Bondu, Alexis and Boullé, Marc and Cornuéjols, Antoine},
	year = {2016},
	note = {ISSN: 16113349},
	keywords = {Time series, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {3--16},
}

@misc{Yi-Yin2012,
	title = {A {Two}-{Layer} {Approach} for {Energy} {Efficiency} in {Mobile} {Location} {Sensing} {Applications}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-30054-7_24},
	author = {Chen, Yi-Yin ChangCheng-Yu LinLing-Jyh},
	year = {2012},
	doi = {10.1007/978-3-642-30054-7_24},
	note = {Publication Title: NETWORKING 2012},
}

@misc{J.2008,
	title = {Lower bounds for succinct data structures},
	abstract = {Indexing text files with methods such as suffix trees and suffix arrays permits extremely fast search for substrings. Unfortunately the space cost of these can dominate that of the raw data. For example, the naive implementation of a suffix tree on genetic information could take 80 times as much space as the raw data. Succinct data structures offer a technique by which the extra space of the indexing can be kept, at least in principle, to a “little oh” with respect to the raw data. This begs the question of how much extra space is necessary to support fast substring searches of other queries such as the rank/select problem or representing a permutation so that both the forward permutation and its inverse can be determined quickly. We survey some lower bounds on this type of problem, most notably the work of Demaine and López-Ortiz and of Golynski.},
	author = {Munro, J. Ian},
	year = {2008},
	doi = {10.1007/978-3-540-69068-9_2},
	note = {Pages: 3–3
Publication Title: Combinatorial Pattern Matching},
}

@article{Jürgen2012,
	title = {Freshening up while staying fast: {Towards} hybrid {SPARQL} queries},
	volume = {7603 LNAI},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33876-2_16},
	doi = {10.1007/978-3-642-33876-2_16},
	abstract = {Querying over cached indexes of Linked Data often suffers from stale or missing results due to infrequent updates and partial coverage of sources. Conversely, live decentralised approaches offer fresh results directly from the Web, but exhibit slow response times due to accessing numerous remote sources at runtime. We thus propose a hybrid query approach that improves upon both paradigms, offering fresher results from a broader range of sources than Linked Data caches while offering faster results than live querying. Our hybrid query engine takes a cached and live query engine as black boxes, where a hybrid query plan- ner splits an input query and delegates the appropriate sub-queries to each interface. In this paper, we discuss query planning alternatives and their main strengths and weaknesses.We also present coherence measures to quantify the coverage and freshness for cached indexes of Linked Data, and show how these measures can be used for hybrid query planning to optimise the trade-off between fresh results and fast runtimes.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Umbrich, Jürgen and Karnstedt, Marcel and Hogan, Aidan and Parreira, Josiane Xavier},
	year = {2012},
	note = {ISBN: 9783642338755},
	pages = {164--174},
}

@inproceedings{Jimeng2013,
	title = {Modeling {Semantic} and {Behavioral} {Relations} for {Query} {Suggestion}},
	url = {http://link.springer.com/10.1007/978-3-642-38562-9_68},
	doi = {10.1007/978-3-642-38562-9_68},
	abstract = {Query suggestion helps users to precisely express their search intents. The state-of-the-art approaches make great progress on high-frequency queries via click graphs. However, due to query ambiguity and click-through data sparseness, these approaches are limited to reality applications. To address the issue, this paper models both semantic and behavioral relations on hybrid bipartite graphs from click logs. Firstly, to overcome the sparseness, a semantic relation graph is established by multiple morphemes (queries, keywords, phrases and entities), independently of clicks. And then semantic relations between queries and other morphemes on the graph are used to find similar queries for query description. Secondly, in order to find related queries for multiple user intents, a behavioral relation graph is constructed by three kinds of user behaviors. Global clicks between queries and URLs display multiple intents by all users; local clicks imply personal preference by a single user; and query formulations in a session represent relations between queries. Finally, two hybrid methods are proposed to combine both semantic and behavioral relations to suggest related queries. To illustrate our methods, we employ the AOL query log data for query suggestion tasks. Experimental results demonstrate that more than 46.5\% of queries get improved suggestions compared with the baseline click models.},
	booktitle = {Web-{Age} {Information} {Management}},
	author = {Chen, Jimeng and Wang, Yuan and Liu, Jie and Huang, Yalou},
	year = {2013},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {666--678},
}

@article{Raquel2008,
	title = {Efficient data distribution for {DWS}},
	volume = {5182 LNCS},
	issn = {03029743 16113349},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-85836-2_8},
	doi = {10.1007/978-3-540-85836-2_8},
	abstract = {The DWS (Data Warehouse Striping) technique is a data partitioning approach especially designed for distributed data warehous- ing environments. In DWS the fact tables are distributed by an arbitrary number of low-cost computers and the queries are executed in parallel by all the computers, guarantying a nearly optimal speed up and scale up. Data loading in data warehouses is typically a heavy process that gets even more complex when considering distributed environments. Data partitioning brings the need for new loading algorithms that conciliate a balanced distribution of data among nodes with an efficient data alloca- tion (vital to achieve low and uniform response times and, consequently, high performance during the execution of queries). This paper evaluates several alternative algorithms and proposes a generic approach for the evaluation of data distribution algorithms in the context of DWS. The experimental results show that the effective loading of the nodes in a DWS system must consider complementary effects, minimizing the num- ber of distinct keys of any large dimension in the fact tables in each node, as well as splitting correlated rows among the nodes.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Almeida, R and Vieira, J and Vieira, M and Madeira, H and Bernardino, J},
	year = {2008},
	note = {ISBN: 3540858350 {\textbar} 9783540858355},
	keywords = {Data distribution, Data striping, Data warehousing},
	pages = {75--86},
}

@misc{Supriya2011,
	title = {A {Survey} on the {Semi}-{Structured} {Data} {Models}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-27245-5_31},
	abstract = {Semi-structured Databases (SSD) are becoming extremely popular in versatile applications including interactive web application, protein structure analysis, 3D object representation, personal lifetime information management. The list is endless. In order to meet the challenges of today’s complex applications, a generic SSD model is in demand. Many works have been reported on this. In this paper, expectations from a generic SSD model are studied by a critical survey among existing models.},
	author = {Chakraborty, Supriya and Chaki, Nabendu},
	year = {2011},
	doi = {10.1007/978-3-642-27245-5_31},
	note = {Pages: 257-266
Publication Title: Computer Information Systems – Analysis and Technologies},
	keywords = {generic data model, oem, ora-ss},
}

@article{Bernd1999,
	title = {Parallel/high-performance object-oriented scientific computing},
	volume = {1743},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/3-540-46589-8_13},
	doi = {10.1007/3-540-46589-8_13},
	abstract = {This chapter contains a summary of the presentations given at the Workshop on Parallel/High-Performance Object-Oriented Scientific Computing (POOSC’99) at the European Conference on Object-Oriented Programming (ECOOP’99) which was held in Lisbon, Portugal on June 15, 1999. The workshop was organized jointly by the Special Interest Group on Object-Oriented Technologies of the Esprit Working Group EuroTools and Los Alamos National Laboratory.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Mohr, Bernd and Bassetti, Federico and Davis, Kei and H??ttemann, Stefan and Launay, Pascale and Marinescu, Dan C. and Miller, David J. and Vandewart, Ruthe L. and M??ller, Matthias and Prodan, Augustin},
	year = {1999},
	note = {ISBN: 354066954X},
	pages = {222--239},
}

@article{George2016,
	title = {Towards a general array database benchmark: {Measuring} storage access},
	volume = {10044},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-49748-8_3},
	doi = {10.1007/978-3-319-49748-8_3},
	abstract = {Array databases have set out to close an important gap in data management, as multi-dimensional arrays play a key role in science and engineering data and beyond. Even more, arrays regularly contribute to the “Big Data” deluge, such as satellite images, climate simulation output, medical image modalities, cosmological simulation data, and datacubes in statistics. Array databases have proven advantageous in flexible access to massive arrays, and an increasing number of research prototypes is emerging. With the advent of more implementations a systematic comparison becomes a worthwhile endeavor. In this paper, we present a systematic benchmark of the storage access component of an Array DBMS. It is designed in a way that comparable results are produced regardless of any specific architecture and tuning. We apply this benchmark, which is available in the public domain, to three main proponents: rasdaman, SciQL, and SciDB. We present the benchmark and its design rationales, show the benchmark results, and comment on them.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Merticariu, George and Misev, Dimitar and Baumann, Peter},
	year = {2016},
	pages = {40--67},
}

@misc{Dongzhe2014,
	title = {A {Generic} {Approach} for {Bulk} {Loading} {Trie}-{Based} {Index} {Structures} on {External} {Storage}},
	url = {http://link.springer.com/10.1007/978-3-319-08010-9_8},
	abstract = {A wide range of applications require efficient management of sorted data on external storage. Recently, trie-based data structures have attracted much attention from the academia as a competitive alternative for the ubiquitous B-tree. In this paper, we present a novel approach for bulk loading disk-based trie structures (a.k.a. B-trie). Our algorithm sorts raw data at first and then builds the B-trie directly from the sorted data. Data in the output data structure are compacted and physically ordered, and thus efficient sequential access can be obtained. We test the proposed algorithm with both real-world and synthetic datasets. Experimental results show that our algorithm outperforms the baseline insertion method dramatically when the dataset is large enough and is almost always superior to the basic sort-and-insert algorithm.},
	author = {Ma, Dongzhe and Feng, Jianhua},
	year = {2014},
	doi = {10.1007/978-3-319-08010-9_8},
	note = {Pages: 55-66
Publication Title: Web-Age Information Management},
}

@article{Miguel2008,
	title = {Managing very-large distributed datasets},
	volume = {5331 LNCS},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-88871-0_54},
	doi = {10.1007/978-3-540-88871-0_54},
	abstract = {In this paper, we introduce a system for handling very large datasets, which need to be stored across multiple computing sites. Data distribution introduces complex management issues, particularly as computing sites may make use of different storage systems with different internal organizations. The motivation for our work is the ATLAS Experiment for the Large Hadron Collider (LHC) at CERN, where the authors are involved in developing the data management middleware. This middleware, called DQ2, is charged with shipping petabytes of data every month to research centers and universities worldwide and has achieved aggregate throughputs in excess of 1.5 Gbytes/sec over the wide-area network. We describe DQ2's design and implementation, which builds upon previous work on distributed file systems, peer-to-peer systems and Data Grids. We discuss its fault tolerance and scalability properties and briefly describe results from its daily usage for the ATLAS Experiment.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Branco, Miguel and Zaluska, Ed and De Roure, David and Salgado, Pedro and Garonne, Vincent and Lassnig, Mario and Rocha, Ricardo},
	year = {2008},
	note = {ISBN: 3540888705},
	keywords = {Data Grids, Data Management, Datasets, Distributed Systems, Grid Computing, ★},
	pages = {775--792},
}

@misc{Stefan2017,
	title = {Cache-{Sensitive} {Skip} {List}: {Efficient} {Range} {Queries} on modern {CPUs}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-56111-0_1},
	abstract = {Due to ever falling prices and advancements in chip technolo-gies, many of today's databases can be entirely kept in main memory. However, reusing existing disk-based index structures for managing data in memory leads to suboptimal performance due to inefficient cache usage and negligence of the capabilities of modern CPUs. Accordingly, a num-ber of main-memory optimized index structures have been proposed, yet most of them focus entirely on single-key lookups, neglecting the equally important range queries. We present Cache-Sensitive Skip Lists (CSSL) as a novel index structure that is optimized for range queries and ex-ploits modern CPUs. CSSL is based on a cache-friendly data layout and traversal algorithm that minimizes cache misses, branch mispredictions, and allows to exploit SIMD instructions for search. In our experiments, CSSL's range query performance surpasses all competitors significantly. Even for lookups, it is only surpassed by the recently presented ART in-dex structure. We therefore see CSSL as a serious alternative for mixed key/range workloads on main-memory databases.},
	author = {Sprenger, Stefan and Zeuch, Steffen and Leser, Ulf},
	year = {2017},
	doi = {10.1007/978-3-319-56111-0_1},
	note = {Publication Title: Data Management on New Hardware},
	keywords = {Index Structures, Main-Memory Databases, Scientific Computing},
}

@inproceedings{Paulo2015,
	title = {Using {Information} {Visualization} to support {Open} {Data} {Integration}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-25936-9_1},
	doi = {10.1007/978-3-319-25936-9},
	abstract = {Data integration has always been a major problem in computer sciences. The more heterogeneous, large and distributed the data sources become, the more difficult the data integration process is. Nowadays, more and more information is being made available on the Web. This is especially the case in the Open Data (OD) movement. Large quantities of datasets are published and accessible. Besides size, heterogeneity grows as well: Datasets exist e.g. in different formats and shapes (tabular files, plain-text files and so on). The ability to efficiently interpret and integrate such datasets is of paramount importance for their potential users. Information Visualization may be an important tool to support this OD integration process. This article presents problems which can be encountered in the data integration process, and, more specifically, in the OD integration process. It also describes how Information Visualization can support OD integration process and make it more effective, friendlier, and faster.},
	booktitle = {Data {Management} {Technologies} and {Applications}},
	author = {Carvalho, Paulo and Hitzelberger, Patrik and Otjacques, Benot and Bouali, Fatma and Venturini, Gilles},
	year = {2015},
	note = {ISSN: 18650929},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Evaluation Research},
	pages = {1--15},
}

@article{Corina2007,
	title = {A typology of course of motion in simulated environments based on {Bézier} curve analysis},
	volume = {13},
	url = {http://link.springer.com/article/10.1007/s10115-007-0065-7},
	doi = {10.1007/s10115-007-0065-7},
	journal = {Knowledge and Information Systems},
	author = {Schmidt, Corina SasNikita},
	year = {2007},
}

@article{Peter2010,
	title = {Special issue: best papers of {VLDB} 2008},
	volume = {19},
	issn = {1066-8888},
	url = {http://link.springer.com/10.1007/s00778-009-0173-y},
	doi = {10.1007/s00778-009-0173-y},
	number = {1},
	journal = {The VLDB Journal},
	author = {Buneman, Peter and Markl, Volker and Ooi, Beng Chin and Ross, Kenneth},
	year = {2010},
	pages = {1--2},
}

@misc{Yiwei2007,
	title = {A {Toolkit} to {Support} {Dynamic} {Social} {Network} {Visualization}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76414-4_50},
	author = {Leng, Yiwei CaoRalf KlammaMarc SpaniolYan},
	year = {2007},
	doi = {10.1007/978-3-540-76414-4_50},
	note = {Publication Title: Advances in Visual Information Systems},
}

@article{HuiNengjun2015,
	title = {{FASTDB}: {An} array database system for efficient storing and analyzing massive scientific data},
	volume = {9532},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-27161-3_55},
	doi = {10.1007/978-3-319-27161-3_55},
	abstract = {Co-clustering has been extensively used in varied applications because of its potential to discover latent local patterns that are otherwise unapparent by usual unsupervised algorithms such as k-means. Recently, a unified view of co-clustering algorithms, called Bregman co-clustering (BCC), provides a general framework that even contains several existing co-clustering algorithms, thus we expect to have more applications of this framework to varied data types. However, the amount of data collected from real-life application domains easily grows too big to fit in the main memory of a single processor machine. Accordingly, enhancing the scalability of BCC can be a critical challenge in practice. To address this and eventually enhance its potential for rapid deployment to wider applications with larger data, we parallelize all the twelve co-clustering algorithms in the BCC framework using message passing interface (MPI). In addition, we validate their scalability on eleven synthetic datasets as well as one real-life dataset, where we demonstrate their speedup performance in terms of varied parameter settings.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Li, Hui and Qiu, Nengjun and Chen, Mei and Li, Hongyuan and Dai, Zhenyu and Zhu, Ming and Huang, Menglin},
	year = {2015},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9783319271606},
	keywords = {Massive data, Performance measurement, Scientific Computing},
	pages = {606--616},
}

@inproceedings{Francesco2007,
	title = {On {Interactive} {Pattern} {Mining} from {Relational} {Databases}},
	volume = {4747},
	isbn = {978-3-540-75548-7},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-75549-4_4},
	doi = {10.1007/978-3-540-75549-4_4},
	abstract = {In this paper we present ConQueSt, a constraint based querying system devised with the aim of supporting the intrinsically exploratory (i.e., human-guided, interactive, iterative) nature of pattern discovery. Following the inductive database vision, our framework provides users with an expressive constraint based query language which allows the discovery process to be effectively driven toward potentially interesting patterns. Such constraints are also exploited to reduce the cost of pattern mining computation. We implemented a comprehensive mining system that can access real world relational databases from which extract data. After a preprocessing step, mining queries are answered by an efficient pattern mining engine which entails several data and search space reduction techniques. Resulting patterns are then presented to the user, and possibly stored in the database. New user-defined constraints can be easily added to the system in order to target the particular ap-plication considered.},
	booktitle = {Knowledge {Discovery} in {Inductive} {Databases}},
	author = {Bonchi, Francesco and Giannotti, Fosca and Lucchese, Claudio and Orlando, Salvatore and Perego, Raffaele and Trasarti, Roberto},
	year = {2007},
	note = {ISSN: 03029743},
	keywords = {RInteractive: Yes, cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {42--62},
}

@article{Fernando2012,
	title = {Application of {Array}-oriented {Scientific} {Data} {Formats} ({NetCDF}) to genotype data, {GWASpi} as an example},
	volume = {6620 LNBI},
	issn = {03029743},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-28062-7_2},
	doi = {10.1007/978-3-642-28062-7_2},
	abstract = {Over the last three decades, the power, resolution and sophistication of scientific experiments has vastly increased, allowing the generation of vast volumes of biological data that need to be stored and processed. Array-oriented Scientific Data Formats are part of an effort by diverse scientific communities to solve the increasing problems of data storage and manipulations. Genome-wide Association Studies (GWAS) based on Single Nucleotide Polymorphism (SNP) arrays are one of the technologies that produce large volumes of data, particularly information on genomic variability. Due to the complexity of the methods and software packages available, each with its particular and intricate formats and work-flows, the analysis of GWAS confronts scientists with a complex hardware and software problematic. To help easing these issues, we have introduced the use of Array-oriented Scientific Data Format databases (NetCDF) in the GWASpi application, a user-friendly, multi-platform, desktop-able software for the management and analysis of GWAS data. The achieved leap of performance has permitted to leverage the most out of commonly available desktop hardware, on which GWASpi now enables ”start- to-end” GWAS management, from raw data to end results and charts. Not only NetCDF allows storing the data efficiently, but it reduces the time needed to achieve the basic results of a GWAS in up to two orders of magnitude. Additionally, the same principles can be used to store and analyze variability data generated by means of ultrasequencing technologies. Available at http://www.gwaspi.org.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Muñiz Fernandez, Fernando and Carreño Torres, Angel and Morcillo-Suarez, Carlos and Navarro, Arcadi},
	year = {2012},
	note = {ISBN: 9783642280610},
	keywords = {GWAS, Genome wide association studies, HDF, NetCDF, SNP},
	pages = {8--20},
}

@article{Themis2016,
	title = {Big sequence management: {A} glimpse of the past, the present, and the future},
	volume = {9587},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-49192-8_6},
	doi = {10.1007/978-3-662-49192-8_6},
	abstract = {There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of sequences, or data series. Examples of such applications come from biology, astronomy, entomology, the web, and other domains. It is not unusual for these applications to involve numbers of data series in the order of hundreds of millions to billions, which are often times not analyzed in their full detail due to their sheer size. In this work, we describe recent efforts in designing techniques for indexing and mining truly massive collections of data series that will enable scientists to easily analyze their data. We show that the main bottleneck in mining such massive datasets is the time taken to build the index, and we thus introduce solutions to this problem. Furthermore, we dis-cuss novel techniques that adaptively create data series indexes, allowing users to correctly answer queries before the indexing task is finished. We also show how our methods allow mining on datasets that would otherwise be completely untenable, including the first published experiments using one billion data se-ries. Finally, we present our vision for the future in big sequence management research.},
	journal = {SOFSEM 2016: Theory and Practice of Computer Science},
	author = {Palpanas, Themis},
	year = {2016},
	note = {ISBN: 9783662491911},
	keywords = {Data analysis, Data indexing, Data management, Data series, cluster:Time Series, layer:Database Layer, supercluster:Indexes, type:Validation Research},
	pages = {63--80},
}

@inproceedings{Parke2016,
	title = {Interactive {Visualization} of {Big} {Data}},
	url = {http://link.springer.com/10.1007/978-3-319-34099-9_1},
	doi = {10.1007/978-3-319-34099-9_1},
	abstract = {Data becomes too big to see. Yet visualization is a central way people understand data. We need to learn new ways to accommodate data visualization that scales up and out for large data to enable people to explore visually their data interactively in real-time as a means to understanding it. The five V’s of big data—value, volume, variety, velocity, and veracity—each highlights the challenges of this endeavor. We present these challenges and a system, Skydive, that we are developing to meet them. Skydive presents an approach that tightly couples a database back-end with a visualization front-end for scaling up and out. We show how hierarchical aggregation can be used to drive this, and the powerful types of interactive visual presentations that can be supported. We are preparing for the day soon when visualization becomes the sixth V of big data.},
	booktitle = {Beyond {Databases}, {Architectures} and {Structures}. {Advanced} {Technologies} for {Data} {Mining} and {Knowledge} {Discovery}},
	author = {Godfrey, Parke and Gryz, Jarek and Lasek, Piotr and Razavi, Nasim},
	year = {2016},
	keywords = {cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {3--22},
}

@article{Sadra2014,
	title = {A trust-based service suggestion system using human plausible reasoning},
	volume = {41},
	url = {http://link.springer.com/article/10.1007/s10489-013-0495-8},
	doi = {10.1007/s10489-013-0495-8},
	journal = {Applied Intelligence},
	author = {Sadaoui, Sadra AbedinzadehSamira},
	year = {2014},
}

@article{Fred2005,
	title = {Ontology {Issues} and {Applications} {Guest} {Editors}’ {Introduction}},
	volume = {11},
	url = {http://link.springer.com/article/10.1007/BF03192372},
	doi = {10.1007/BF03192372},
	journal = {Journal of the Brazilian Computer Society},
	author = {Noy, Fred FreitasHeiner StuckenschmidtNatalya F},
	year = {2005},
}

@inproceedings{Todd2012,
	title = {Towards a scalable, performance-oriented {OLAP} storage engine},
	volume = {7239 LNCS},
	isbn = {978-3-642-29034-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-29035-0_13},
	doi = {10.1007/978-3-642-29035-0_13},
	abstract = {Over the past generation, data warehousing and OLAP applications have become the cornerstone of contemporary decision support environments. Typically, OLAP servers are implemented on top of either proprietary array-based storage engines (MOLAP) or as extensions to conventional relational DBMSs (ROLAP). While MOLAP systems do indeed provide impressive performance on common analytics queries, they tend to have limited scalability. Conversely, ROLAP’s table oriented model scales quite nicely, but offers mediocre performance at best relative to the MOLAP systems. In this paper, we describe a storage and indexing framework that aims to provide both MOLAP like performance and ROLAP like scalability by essentially combining some of the best features of both. Based upon a combination of R-trees and bitmap indexes, the storage engine has been integrated with a robust OLAP query engine prototype that is able to fully exploit the efficiency of the proposed storage model. Experimental results demonstrate that not only does the framework improve upon more naive approaches, but that it does indeed offer the potential to optimize both query performance and scalability.},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	author = {Eavis, Todd and Taleb, Ahmad},
	year = {2012},
	note = {Issue: PART 2
ISSN: 03029743},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {185--202},
}

@inproceedings{Hai-Tao2015,
	title = {{RDQS}: {A} {Relevant} and {Diverse} {Query} {Suggestion} {Generation} {Framework}},
	url = {http://link.springer.com/10.1007/978-3-319-25255-1_48},
	doi = {10.1007/978-3-319-25255-1_48},
	abstract = {Traditional query suggestion methods mainly leverage click-through information to find related queries as recommendations, without considering the semantic relateness between queries. In addition, few studies use click-through distribution in diversifying query suggestions. To address these issues, we propose a novel and effective framework to generate relevant and diversified query suggestions. We combine query semantics and click-through information together to generate query suggestion candidates which are highly relevant to original query , we use click-through distribution to diversify the candidates. We evaluate our method on a large-scale search log dataset of a commercial engine, experimental results indicate that our framework has significantly improved the relevance and diversity of suggested queries by comparing to four baseline methods.},
	booktitle = {Web {Technologies} and {Applications}},
	author = {Zheng, Hai-Tao and Zhang, Yi-Chi},
	year = {2015},
	keywords = {cluster:Assisted Query Formulation, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {586--597},
}

@misc{Abraham2017,
	title = {A {Twitter} {Analysis} of an {Integrated} {E}-{Activism} {Campaign}: \#{FeesMustFall} - {A} {South} {African} {Case} {Study}},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-58559-8_31},
	author = {van der Vyver, Abraham G},
	year = {2017},
	doi = {10.1007/978-3-319-58559-8_31},
	note = {Publication Title: Social Computing and Social Media. Human Behavior},
}

@misc{Binh2005,
	title = {A {Unified} {Context} {Model}: {Bringing} {Probabilistic} {Models} to {Context} {Ontology}},
	url = {http://link.springer.com/chapter/10.1007/11596042_59},
	author = {Lee, Binh An TruongYoungKoo LeeSung Young},
	year = {2005},
	doi = {10.1007/11596042_59},
	note = {Publication Title: Embedded and Ubiquitous Computing – EUC 2005 Workshops},
}

@article{Ronny1995,
	title = {A toolkit for appraising the long-term usability of a text editor},
	volume = {4},
	url = {http://link.springer.com/article/10.1007/BF00402716},
	doi = {10.1007/BF00402716},
	journal = {Software Quality Journal},
	author = {Thomas, Ronny CookJudy KayGreg RyanRichard C},
	year = {1995},
}

@article{Andrea2011,
	title = {Inductive database languages: {Requirements} and examples},
	volume = {26},
	issn = {02191377},
	url = {http://link.springer.com/article/10.1007/s10115-009-0281-4},
	doi = {10.1007/s10115-009-0281-4},
	abstract = {Inductive databases (IDBs) represent a database perspective on Knowledge discovery in databases (KDD). In an IDB, the KDD application can express both queries capable of accessing and manipulating data, and queries capable of generating, manipulating, and applying patterns allowing to formalize the notion of mining process. The feature that makes them different from other data mining applications is exactly the idea of looking at the support for knowledge discovery as an extension of the query process. This paper draws a list of desirable properties to be taken into account in the definition of an IDB framework. They involve several dimensions, such as the expressiveness of the language in representing data and models, the closure principle, the capability to provide a support for an efficient algorithm programming. These requirements are a basis for a comparative study that highlights strengths and weaknesses of existing IDB approaches. The paper focuses on the SQL-based ATLaS language/system, on the logic-based LDL++ language/system, and on the XML-based KDDML language/system.},
	number = {3},
	journal = {Knowledge and Information Systems},
	author = {Romei, Andrea and Turini, Franco},
	year = {2011},
	keywords = {Association rules, Data Mining, Inductive database, Knowledge discovery, Query language},
	pages = {351--384},
}

@article{Jürgen2011,
	title = {Comparing data summaries for processing live queries over {Linked} {Data}},
	volume = {14},
	issn = {1386145X},
	url = {http://link.springer.com/article/10.1007/s11280-010-0107-z},
	doi = {10.1007/s11280-010-0107-z},
	abstract = {A growing amount of Linked Data-graph-structured data accessible at sources distributed across the Web-enables advanced data integration and decision-making applications. Typical systems operating on Linked Data collect (crawl) and pre-process (index) large amounts of data, and evaluate queries against a centralised repository. Given that crawling and indexing are time-consuming operations, the data in the centralised index may be out of date at query execution time. An ideal query answering system for querying Linked Data live should return current answers in a reasonable amount of time, even on corpora as large as the Web. In such a live query system source selection-determining which sources contribute answers to a query-is a crucial step. In this article we propose to use lightweight data summaries for determining relevant sources during query evaluation. We compare several data structures and hash functions with respect to their suitability for building such summaries, stressing benefits for queries that contain joins and require ranking of results and sources. We elaborate on join variants, join ordering and ranking. We analyse the different approaches theoretically and provide results of an extensive experimental evaluation. 2011 Springer Science+Business Media, LLC.},
	number = {5},
	journal = {World Wide Web},
	author = {Umbrich, Jürgen and Hose, Katja and Karnstedt, Marcel and Harth, Andreas and Polleres, Axel},
	year = {2011},
	note = {ISBN: 1386145X (ISSN)},
	keywords = {Linked Data, RDF querying, index structures},
	pages = {495--544},
}

@article{Hao2014,
	title = {Topical presentation of search results on database},
	volume = {8422 LNCS},
	issn = {16113349},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-05813-9_23},
	doi = {10.1007/978-3-319-05813-9_23},
	abstract = {Clustering and faceting are two ways of presenting search results in database. Clustering shows the summary of the answer space by grouping similar results. However, clusters are not self-explanatory, thus users cannot clearly identify what can be found inside each cluster. On the other hand, faceting groups results by labelling, but there might be too many facets that overwhelm users. In this paper, we propose a novel approach, topical presentation, to better present the search results. We reckon that an effective presentation technique should be able to cluster results into reasonable number of groups with intelligible meaning, and provide as much information as possible on the first screen. We define and study the presentation properties first, and then propose efficient algorithms to provide real time presentation. Extensive experiments on real datasets show the effectiveness and efficiency of the proposed method. This work was supported in part by NSFC grants (61170007, 60673133, 61033010, 61103009), the Key Project of Shanghai Municipal Science and Technology Commission (Scientific Innovation Act Plan, Grant No.13511504804), and ARC discovery project DP110102407.},
	number = {PART 2},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Hu, Hao and Zhang, Mingxi and He, Zhenying and Wang, Peng and Wang, Wei and Liu, Chengfei},
	year = {2014},
	note = {ISBN: 9783319058122},
	pages = {343--360},
}

@misc{Shanliang2016,
	title = {A {Two}-{Stage} {Decision} {Model} {Based} on {Rough} {Set} and {Fuzzy} {Reasoning} with {Application} to {Missile} {Selection} for {Aerial} {Targets}},
	url = {http://link.springer.com/chapter/10.1007/978-981-10-2666-9_36},
	author = {Huang, Shanliang YangChuncai WangMei YangGe LiKedi},
	year = {2016},
	doi = {10.1007/978-981-10-2666-9_36},
	note = {Publication Title: Theory, Methodology, Tools and Applications for Modeling and Simulation of Complex Systems},
}

@article{Wolfgang2012,
	title = {Special section on large-scale analytics},
	volume = {21},
	issn = {10668888},
	url = {http://link.springer.com/article/10.1007/s00778-012-0291-9},
	doi = {10.1007/s00778-012-0291-9},
	abstract = {Big Data is no longer exclusively the domain of big organizations. Companies, collaborations, and organizations of all types and sizes are increasingly faced with the need to analyze and make sense of large and growing collections of data. Solving the challenge of large-scale analytics requires innovation across the spectrum of data management: Large volumes of data have to be acquired, processed, stored, and eventually reclaimed. Complex statistical procedures must be applied to those large data sets. Transactional guarantees are required to provide a consistent picture with operational systems. Metadata must be maintained to provide the context of the underlying raw data for later analysis. These challenges must be faced independently and together in order to establish a scalable, affordable, and flexible large-scale analytics infrastructure. This special section focuses on conceptual and systems-architecture issues in this emerging area. The three selected papers present recent efforts that push the envelope of novel schemes for large-scale analytics and provide a deeper understanding and assessment of the current state of the art. The first paper of the special section focuses on optimizing the well-known MapReduce processing paradigm. The proposed approach exploits the fact that MapReduce clusters support multiple, concurrently running jobs often accessing the same set of data. The paper entitled “On the optimization of schedules for MapReduce workloads in the presence of shared scans” improves traditional MapReduce processing with two main contributions. First, the authors introduce the technique of cyclic piggybacking to implement shared scans and reduce to overall access cost. Then, given the ability to share scans, the paper addresses the optimization of job scheduling to exploit the shared scans in an optimal fashion. The paper then presents the circumflex scheduler, a generalized version of the flex scheduler allowing a wide variety of different cost metrics to be used. Insights into the implementation of these techniques and results of simulation and real benchmark experiments are given as well. The second paper addresses one of the hot topics in database-centric research in the context of the MapReduce data processing paradigm by trying to bridge the gap between both worlds. In their article entitled “SCOPE: parallel databases meet MapReduce,” the authors carefully layout the design principles and implementation aspects of the scope system. The general goal of the Scope system is to provide an efficient and flexible platform for very large-scale/massive data analytics by providing an easy to program interface supported by sophisticated optimization to achieve high performance and reliability. The paper outlines the compilation of a query from SQL-like query specification via the optimizer, which turns the query into a data-flow graph for the distributed query engine. The paper also outlines the core principles of the execution environment, discussing the role of a job manager orchestrating the cluster resources and generating the schedule for the different jobs to provide fault tolerant behavior. In summary, the paper gives a superb view into the details of a large-scale platform used daily for various analytics and data mining activities at Microsoft. The third paper in this special section, entitled “GBASE: an efficient analysis platform for large graphs,” tackles some key challenges in the context of very large graphs. Graph-structured data are commonplace in domains such as social network analytics, logistics, or even security. In contrast to classical table-like structures, graph structures are more entity-focused, implying that individual nodes may have different types and may have specific relationships to other nodes. Running large analytical queries on top of graphs impacts the requirements for different types of operators and storage structures. This paper outlines the approach taken within the GBASE project. The authors carefully layout storage and compression structures used and present primitive graph operations allowing complex analytical queries through composition. Extensive experimental results are given to underline the efficiency of the approach taken. Taken together, these papers provide a strong point of reference for future research and teaching on this crucial topic. We thank the authors of all of the papers submitted to this special section as well as the diligent reviewers who provided perspective, advice, and keen assessments of the submissions in the context of this fast-moving field. We look forward to continue the innovation and leadership of the VLDB community in the “Big Data” analytics space.},
	number = {5},
	journal = {VLDB Journal},
	author = {Lehner, Wolfgang and Franklin, Michael J.},
	year = {2012},
	note = {ISBN: 0077801202919},
	keywords = {★},
	pages = {587--588},
}

@article{Hoang2012,
	title = {{ROARS}: {A} robust object archival system for data intensive scientific computing},
	volume = {30},
	issn = {09268782},
	url = {http://link.springer.com/article/10.1007/s10619-012-7103-5},
	doi = {10.1007/s10619-012-7103-5},
	abstract = {As scientific research becomes more data intensive, there is an increasing need for scalable, reliable, and high performance storage systems. Such data repositories must provide both data archival services and rich metadata, and cleanly integrate with large scale computing resources. ROARS is a hybrid approach to distributed storage that provides both large, robust, scalable storage and efficient rich metadata queries for scientific applications. In this paper, we present the design and implementation of ROARS, focusing primarily on the challenge of maintaining data integrity across long time scales. We evaluate the performance of ROARS on a storage cluster, comparing to the Hadoop distributed file system and a centralized file server. We observe that ROARS has read and write performance that scales with the number of storage nodes, and integrity checking that scales with the size of the largest node. We demonstrate the ability of ROARS to function correctly through multiple system failures and reconfigurations. ROARS has been in production use for over three years as the primary data repository for a biometrics research lab at the University of Notre Dame. © Springer Science+Business Media, LLC 2012.},
	number = {5-6},
	journal = {Distributed and Parallel Databases},
	author = {Bui, Hoang and Bui, Peter and Flynn, Patrick and Thain, Douglas},
	year = {2012},
	note = {ISBN: 1061901271035},
	keywords = {Distributed storage, Distributed system},
	pages = {325--350},
}

@article{Domenico2015,
	title = {A reverse engineering process for inferring data models from spreadsheet-based information systems: {An} automotive industrial experience},
	volume = {178},
	issn = {18650929},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-25936-9_9},
	doi = {10.1007/978-3-319-25936-9_9},
	abstract = {© Springer International Publishing Switzerland 2015.Nowadays Spreadsheet-based Information Systems are widely used in industries to support different phases of their production processes. The intensive employment of Spreadsheets in industry is mainly due to their ease of use that allows the development of Information Systems even by not experienced programmers. The development of such systems is further aided by integrated scripting languages (e.g. Visual Basic for Applications, Libre Office Basic, JavaScript, etc.) that offer features for the implementation of Rapid Application Development processes. Although Spreadsheet-based Information Systems can be developed with a very short time to market, they are usually poorly documented or in some case not documented at all. As a consequence, they are very difficult to be comprehended, maintained or migrated towards other architectures, such as Database Oriented Information Systems or Web Applications. The abstraction of a data model from the source spreadsheet files represents a fundamental activity of the migration process towards different architectures. In our work we present an heuristic- based reverse engineering process for inferring a data model from an Excel based information system. The process is fully automatic and it is based on seven sequential steps. Both the applicability and the effectiveness of the proposed process have been assessed by an experiment we conducted in the automotive industrial context. The process was successfully used to obtain the UML class diagrams representing the conceptual data models of three different Spreadsheet-based Information Systems. The paper presents the results of the experiment and the lessons we learned from it.},
	journal = {Communications in Computer and Information Science},
	author = {Amalfitano, D. and Fasolino, A.R. and Tramontana, P. and De Simone, V. and Di Mare, G. and Scala, S.},
	year = {2015},
}

@inproceedings{Qiming2012,
	title = {R-proxy framework for in-{DB} data-parallel analytics},
	volume = {7447 LNCS},
	isbn = {978-3-642-32596-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-32597-7_24},
	doi = {10.1007/978-3-642-32597-7_24},
	abstract = {R is a powerful programming environment for data analysis. However, when dealing with big data in R, a kind of main-memory based functional programming environment, the data movement and memory swapping become the major performance bottleneck. Therefore, executing a big-data-intensive R program could be many orders of magnitude less efficient than processing the SQL query directly inside the database for dealing with the same analytic task. Although there exists a number of "parallel-R" solutions, pushing R operations down to the parallel database layer, while retaining the natural R interface and the virtual R analytics flow, remains a very competitive alternative. This has motivated us to develop the R-Vertica framework to scale-out R applications through in-DB, data-parallel analytics. In order to extend the R programming environment to the space of parallel query processing transparently to the R users, we introduce the notion of R Proxy - the R object with instance maintained in the parallel database as partitioned data sets, and schema (header) retained in the memory-based R environment. A function (such as aggregation) applied to a proxy is pushed down to the parallel database layer as SQL queries or procedures, with the query results automatically returned and converted to R objects. By providing the transparent 2-way mappings between several major types of R objects and database tables or query results, the R environment and the underlying parallel database are seamlessly integrated. The R object proxies may be created from database table schemas, in-DB operations, or the operations for persisting R objects to the database. The instances of the R proxies can be retrieved into regular R objects using SQL queries. With this framework, an R application is expressed as the analytics flow with the R objects bearing small data and the R proxies representing, but not bearing, big data. The big data are manipulated, or flow, underneath the in-memory R environment in terms of In-DB and data-parallel operations. We have implemented the proposed approach and used it to integrate several large-scale R applications with the multi-node Vertica parallel database system. Our experience illustrates the unique feature and efficiency of this R-Vertica framework. © 2012 Springer-Verlag.},
	booktitle = {Database and {Expert} {Systems} {Applications}},
	author = {Chen, Qiming and Hsu, Meichun and Wu, Ren and Shan, Jerry},
	year = {2012},
	note = {Issue: PART 2
ISSN: 03029743},
	keywords = {cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {266--280},
}

@article{doi:10.1142/S0129626413500023,
	title = {Parallel {Construction} of {Data} {Cubes} on {Multi}-{Core} {Multi}-{Disk} {Platforms}},
	volume = {23},
	issn = {0129-6264},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0129626413500023},
	doi = {10.1142/S0129626413500023},
	abstract = {On-line Analytical Processing (OLAP) has become one of the most powerful and prominent technologies for knowledge discovery in VLDB (Very Large Database) environments. Central to the OLAP paradigm is the data cube, a multi dimensional hierarchy of aggregate values that provides a rich analytical model for decision support. Various sequential algorithms for the efficient generation of the data cube have appeared in the literature. However, given the size of contemporary data warehousing repositories, multi-processor solutions are crucial for the massive computational demands of current and future OLAP systems. In this paper we discuss the development of MCMD-CUBE, a new parallel data cube construction method for multi-core processors with multiple disks. We present experimental results for a Sandy Bridge multi-core processor with four parallel disks. Our experiments indicate that MCMD-CUBE achieves very close to linear speedup. A critical part of our MCMD-CUBE method is parallel sorting. We developed a new parallel sorting method termed MCMD-SORT for multi-core processors with multiple disks which outperforms other previous methods. Read More: http://www.worldscientific.com/doi/abs/10.1142/S0129626413500023},
	number = {01},
	journal = {Parallel Processing Letters},
	author = {DEHNE, FRANK and ZABOLI, HAMIDREZA},
	year = {2013},
	pages = {1350002},
}

@article{doi:10.1142/S0218213001000684,
	title = {{THE} {APPLICATION} {OF} {MACHINE} {LEARNING} {TOOLS} {TO} {THE} {VALIDATION} {OF} {AN} {AIR} {TRAFFIC} {CONTROL} {DOMAIN} {THEORY}},
	volume = {10},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213001000684},
	doi = {10.1142/S0218213001000684},
	number = {04},
	journal = {International Journal on Artificial Intelligence Tools},
	author = {WEST, M M and McCLUSKEY, T L},
	year = {2001},
	pages = {613--637},
}

@article{doi:10.1142/S0218843016500106,
	title = {A {Schema}-{Based} {Approach} to {Enable} {Data} {Integration} on the {Fly}},
	volume = {26},
	issn = {0218-8430},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218843016500106},
	doi = {10.1142/S0218843016500106},
	abstract = {On-the-fly data integration, i.e. at query time, happens mostly in tightly coupled, homogeneous environments where the partitioning of the data can be controlled or is known in advance. During the process of data fusion, the information is homogenized and data inconsistencies are hidden from the application. Beyond this, we propose in this paper the Nexus metadata model and a processing approach that support on-the-fly data integration in a loosely coupled federation of autonomous data providers, thereby advancing the status quo in terms of flexibility and expressive power. It is able to represent data and schema inconsistencies like multi-valued attributes and multi-typed objects. In an open environment, this best suites the application needs where the data processing infrastructure is not able to decide which attribute value is correct. The Nexus metadata model provides the foundation for integration schemata that are specific to a given application domain. The corresponding processing model provides four complementary query semantics in order to account for the subtleties of multi-valued and missing attributes. In this paper we show that this query semantics is sound, easy to implement, and it builds upon existing query processing techniques. Thus the Nexus metadata model provides a unique level of flexibility for on-the-fly data integration. Read More: http://www.worldscientific.com/doi/abs/10.1142/S0218843016500106},
	number = {01},
	journal = {International Journal of Cooperative Information Systems},
	author = {Nicklas, Daniela and Schwarz, Thomas and Mitschang, Bernhard},
	year = {2017},
	keywords = {cluster:Flexible Engines, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution},
	pages = {1650010},
}

@article{Taniar20151508,
	title = {A taxonomy for region queries in spatial databases},
	volume = {81},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000014001895},
	doi = {https://doi.org/10.1016/j.jcss.2014.12.025},
	abstract = {Abstract In spatial databases, there are two basic types of queries, namely nearest neighbour queries (kNN) and range queries. Spatial range queries are not only finding objects of interest within a certain range or radius, but feature a wide spectrum: from finding objects of interest to forming the range (or region). Therefore, in this paper, we coin a term “Region Queries” to indicate a broad category of spatial range queries. It is imperative to understand the full capabilities of region queries, before starting to work on processing and optimising such queries. The aim of this paper is to show a complete picture of region queries. In this study, we present taxonomy of region queries, comprising of three categories: (i) finding objects of interest, (ii) forming regions, and (iii) determining centroids. These three query types form a comprehensive view of what region queries are about.},
	number = {8},
	journal = {Journal of Computer and System Sciences},
	author = {Taniar, David and Rahayu, Wenny},
	year = {2015},
	keywords = {Query processing, Range queries, Region queries, Spatial databases, Spatial queries, cluster:Spatial Query, layer:Database Layer, supercluster:Indexes, type:Evaluation Research},
	pages = {1508--1531},
}

@misc{Making2010,
	title = {Making {Noise} with {OpenAL}},
	url = {http://link.springer.com/chapter/10.1007/978-1-4302-2600-0_10},
	year = {2010},
	doi = {10.1007/978-1-4302-2600-0_10},
	note = {Publication Title: Beginning iPhone Games Development},
}

@article{Kacprzyk2015300,
	title = {Fuzziness in database management systems : {Half} a century of developments and future prospects},
	volume = {281},
	issn = {0165-0114},
	url = {http://www.sciencedirect.com/science/article/pii/S0165011415003000},
	doi = {10.1016/j.fss.2015.06.011},
	abstract = {Abstract This comprehensive, bird's view research note combines the state of the art, a brief presentation of the history and some original solutions, and position like views of some prospective future developments of one of the most relevant and interesting areas related to the use of fuzzy logic in database management systems, notably in its querying component, and – to some extent – in a broader issue of data and information management. We briefly summarize the roots of those new applications of fuzzy logic, more relevant proposals and development in the context of fuzzification of the basic relational database model, and then some of its further generalizations. We particularly focus on fuzzy querying as a human consistent and friendly way of retrieving information due to real human intentions and preferences expressed in natural language represented via fuzzy logic and possibility theory. We mention some extensions, notably fuzzy queries with linguistic quantifiers, and point their close relation to linguistic summaries. As for newer, prospective developments, we mainly focus on bipolar queries that can accomodate the users' intentions and preferences involving some sort of a required and desired, mandatory and optional, etc. conditions. We show various ways of handling such queries. We conclude with some brief position statements of our view on relevant and promising directions, and challenges.},
	journal = {Fuzzy Sets and Systems},
	author = {Kacprzyk, Janusz and Zadro, Sławomir},
	year = {2015},
	keywords = {database management systems, database querying, dbms, for a new generation, fuzzy logic, huge quantities, more effective and efficient, new, of database systems incorporating, of various forms and, or decades, over the last years, possibility theory},
	pages = {300--307},
}

@inproceedings{6916467,
	title = {Speed {Up} {Distance}-{Based} {Similarity} {Query} {Using} {Multiple} {Threads}},
	url = {http://ieeexplore.ieee.org/document/6916467/?reload=true},
	doi = {10.1109/PAAP.2014.54},
	abstract = {Metric-space indexing, also known as distance-based indexing, is a universal indexing to support similarity queries. It only requires that the similarity of data be defined by a metric distance function. To achieve the great universalness, metric-space indexing does not take use of the domain information of data, and is thus outperformed by many domain-specific methods. In this paper, to speed up metric-space similarity query, we first assign one thread for each query in the multi-query case to increase the throughput. Then, for a single query, we assign one thread for each search path from the root of the index tree to decrease the responding time. Last but not least, we implement an in-memory buffer to break the bottleneck of the disk access to the index file. Experimental results show that our efforts result in good speed up and parallel efficiency.},
	booktitle = {2014 {Sixth} {International} {Symposium} on {Parallel} {Architectures}, {Algorithms} and {Programming}},
	author = {Lei, F and Wu, W and Li, Q and Zhang, H and Li, P and Luo, Q and Mao, R},
	month = jul,
	year = {2014},
	note = {ISSN: 2168-3034},
	keywords = {cluster:Spatial Query, database indexing, layer:Database Layer, query processing, supercluster:Indexes, type:Validation Research},
	pages = {215--219},
}

@article{Campos2013346,
	title = {Splat-based surface reconstruction from defect-laden point sets},
	volume = {75},
	issn = {1524-0703},
	url = {http://www.sciencedirect.com/science/article/pii/S1524070313000258},
	doi = {https://doi.org/10.1016/j.gmod.2013.08.001},
	abstract = {Abstract We introduce a method for surface reconstruction from point sets that is able to cope with noise and outliers. First, a splat-based representation is computed from the point set. A robust local 3D RANSAC-based procedure is used to filter the point set for outliers, then a local jet surface – a low-degree surface approximation – is fitted to the inliers. Second, we extract the reconstructed surface in the form of a surface triangle mesh through Delaunay refinement. The Delaunay refinement meshing approach requires computing intersections between line segment queries and the surface to be meshed. In the present case, intersection queries are solved from the set of splats through a 1D \{RANSAC\} procedure.},
	number = {6},
	journal = {Graphical Models},
	author = {Campos, Ricard and Garcia, Rafael and Alliez, Pierre and Yvinec, Mariette},
	year = {2013},
	keywords = {Delaunay refinement, RANSAC fitting},
	pages = {346--361},
}

@inproceedings{Han2017,
	title = {A distributed in-situ analysis method for large-scale scientific data},
	isbn = {978-1-5090-3015-6},
	url = {http://ieeexplore.ieee.org/document/7881718/},
	doi = {10.1109/BIGCOMP.2017.7881718},
	abstract = {Recently, a massive amount of data is generated in a wide range of scientific applications such as NASA's satellite, the large hadron collider, and large synoptic survey telescope. Most of scientific data follows the array model, and there are various kinds of standard array formats such as HDF, NetCDF, MDSplus, and ROOT. SciDB is the most well-known DBMS that stores the array-based scientific data and processes queries on it. SciDB is a distributed DBMS, and so, is scalable in terms of query performance. However, it has a severe drawback that takes a huge amount of time for loading a massive amount of scientific data into DBMS. That is, it is not scalable in terms of data loading. To overcome that problem, we propose a distributed in-situ analysis method that allows processing queries on raw scientific data in a distributed manner without explicit data loading. In detail, we propose the in-situ scan operator that scans necessary data of the array format and passes it to upper operators of the pipeline of a query plan. It also performs repartitioning during in-situ scanning, which is required for correct query results. Through experiments using real datasets, we have shown that the SciDB system using our method significantly outperforms the original SciDB system by orders of magnitude in terms of the performance of the first query.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing}, {BigComp} 2017},
	author = {Han, Donghyoung and Nam, Yoon Min and Kim, Min Soo},
	month = feb,
	year = {2017},
	keywords = {Data analysis, RDistributed: Yes, RInteractive: Yes, RRaw: Yes, cluster:Adaptive Loading, distributed databases, layer:Database Layer, natural science, supercluster:Data Storage, type:Proposal of Solution, ★},
	pages = {69--75},
}

@inproceedings{7036684,
	title = {Information retrieval using dynamic indexing},
	url = {http://ieeexplore.ieee.org/document/7036684/},
	doi = {10.1109/INFOS.2014.7036684},
	abstract = {Since the demand for information retrieval increases quickly, indexing structures became an important issue to support fast information retrieval. According to the work in this paper, a new data structure called Dynamic Ordered Multi-field Index (DOMI) for information retrieval has been introduced. It is based on radix trees organized in segments in addition to a hash table to point to the roots of each segment, where each segment is dedicated to store the values of a single field. The hash table is used to access the needed segments directly without traversing the upper segments. So, DOMI improves look-up performance for queries addressing to a single field. In the case of multiple queries addressing, each segment of the radix tree is traversed sequentially without visiting the unrelated branches. The use of segmentation for the proposed DOMI provides flexibility for minimizing communication overhead in the distributed system. Every field in the radix tree is represented by one segment, where each segment can be stored as one block. Moreover, the proposed DOMI consumes less space comparing to indexes which are built using B or B+ trees. Hence, it is more suitable for intensive-data such as Big Data.},
	booktitle = {2014 9th {International} {Conference} on {Informatics} and {Systems}},
	author = {Mohammed, S I and Omara, F A and Sharaf, H M},
	month = dec,
	year = {2014},
	keywords = {Big Data, cluster:Adaptive Indexing, data structures, indexing, layer:Database Layer, query processing, supercluster:Indexes, type:Proposal of Solution},
	pages = {PDC--93--PDC--101},
}

@inproceedings{6781382,
	title = {Achieving query optimization using sparsity management in {OLAP} system},
	isbn = {978-1-4799-2900-9},
	url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781382},
	doi = {10.1109/ICICICT.2014.6781382},
	abstract = {Data Warehouses are increasing their data volume at an accelerated rate; high disk space consumption; slow query response time and complex database administration are common problems in these environments. The lack of a proper data model and an adequate architecture specifically targeted towards these environments are the root causes of these problems. Inefficient management of stored data includes duplicate values at column level and poor management of data sparsity which derives from a low data density, and affects the final size of Data Warehouses. It finds that Relational technology and the Relational Model are not the best techniques for managing duplicates and data sparsity. The novelty of this research is to compare some data models considering their data density and their data sparsity management to optimize Data Warehouse environments. In this research paper various techniques for query performance optimization have been explored and a close association of its conceptual aspects with Oracle Warehouse Builder is mapped. © 2014 IEEE.},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Issues} and {Challenges} in {Intelligent} {Computing} {Techniques}, {ICICT} 2014},
	author = {Kumar, Arvind and Singh, Deepti and Sharma, Vineet},
	month = feb,
	year = {2014},
	keywords = {Query Performance, Sparsity, Storage Structure, Two-Tier Structure},
	pages = {797--801},
}

@inproceedings{6062917,
	title = {A {Tree}-{Based} {Local} {Repairing} {Approach} for {Increasing} {Lifetime} of {Query} {Driven} {WSN}},
	doi = {10.1109/CSE.2011.87},
	abstract = {Query driven Broadcast through wireless sensor nodes also leads to the domain of event driven converge cast. A query-response based application in Wireless Sensor Networks(WSN) demands the correct delivery of data message at each sensor node. A Breadth-First Search(BFS) tree rooted at the base station offers shortest path traversal for each data message which utilizes the sensor resources efficiently. Resource constrained sensor nodes are highly prone to sudden crash. So the application demands a quick and smart approach to repair the tree when a node dies. In this paper a novel scheme has been proposed to locally repair the tree with constant round of message transmissions. Each node piggybacks a few bytes of extra information along with each query and response messages. Based on these piggybacked values each node calculates its alternate parent. When a parent node fails, its children can contact their respective alternate parents immediately to establish an alternate path to the root. Reduced communication cost in terms of extra message transmissions saves battery power at each node. Efficient query-response message handler ensures the correct delivery of messages. Fast repairing offers good Quality of Service(QoS). Simulation result shows that no message is lost except the one holding by the crashed node.},
	booktitle = {2011 14th {IEEE} {International} {Conference} on {Computational} {Science} and {Engineering}},
	author = {Chakraborty, S and Chakraborty, S and Nandi, S and Karmakar, S},
	month = aug,
	year = {2011},
	pages = {475--482},
}

@misc{What’s2006,
	title = {What’s {Next} for the {Google} {Maps} {API}?},
	url = {http://link.springer.com/chapter/10.1007/978-1-4302-0224-0_8},
	year = {2006},
	doi = {10.1007/978-1-4302-0224-0_8},
	note = {Publication Title: Beginning Google Maps Applications with PHP and Ajax},
}

@inproceedings{5352770,
	title = {Optimization of object-oriented queries addressing large and small collections},
	isbn = {978-1-4244-5314-6},
	url = {http://ieeexplore.ieee.org/document/5352770/},
	doi = {10.1109/IMCSIT.2009.5352770},
	abstract = {When a query jointly addresses very large and very small collections it may happen that an iteration caused by a query operator is driven by a large collection and in each cycle it evaluates a subquery that depends on an element of a small collection. For each such element the result returned by the subquery is the same. In effect, such a subquery is unnecessarily evaluated many times. The optimization rewrites such a query to reverse the situation: the loop is to be performed on a small collection and inside each its cycle a subquery addressing a large collection is evaluated. We illustrate the method on comprehensive examples and then present the general rewriting rule. The research follows the Stack-Based Approach to query languages having roots in the semantics of programming languages. The optimization method consists in analyzing of scoping and binding rules for names occurring in queries.},
	booktitle = {2009 {International} {Multiconference} on {Computer} {Science} and {Information} {Technology}},
	author = {Bleja, Michal and Stencel, Krzysztof and Subieta, Kazimierz},
	month = oct,
	year = {2009},
	note = {ISSN: 2157-5525},
	keywords = {object-oriented methods, programming language seman},
	pages = {643--650},
}

@inproceedings{4287923,
	title = {A {Unified}-{Index} {Based} {Distributed} {Specification} for {Heterogeneous} {Components} {Management}},
	volume = {3},
	doi = {10.1109/SNPD.2007.109},
	abstract = {With the development of service-oriented architecture (SOA) and component-based software engineering (CBSE), the number of various heterogeneous service components increases rapidly. They are stored in traditional repositories of different organization and can be independently accessed by users. However, it brings difficulty in component discovery and reuse. The utility of massive component resource can not be remarkably improved. In this paper, a unified-index (UI) based distributed specification for heterogeneous components management is introduced. It focuses on creating and maintaining a Unified-Index tree on the UI repository server, which has a management field grouped by some traditional repositories. Each UI repository server maps the components of its children traditional repositories into UI format and stores them in local UI repository. It also actively monitors the adding of new components and removing of invalid ones. Every query of component discovery will be diffused to all of the UI repository servers with transmitting of the root server. So in this desired architecture, users can consistently search and access all the heterogeneous components for further assembly and application, satisfying various personalized and comprehensive user requirements.},
	booktitle = {Eighth {ACIS} {International} {Conference} on {Software} {Engineering}, {Artificial} {Intelligence}, {Networking}, and {Parallel}/{Distributed} {Computing} ({SNPD} 2007)},
	author = {Zhong, M and Zhang, Y and Tian, P and Fang, C and Zhou, Y},
	month = jul,
	year = {2007},
	keywords = {distributed processing;formal specification;object},
	pages = {599--604},
}

@inproceedings{4677352,
	title = {Model-driven {Visual} {Analytics}},
	url = {http://ieeexplore.ieee.org/document/4677352/},
	doi = {10.1109/VAST.2008.4677352},
	abstract = {We describe a visual analytics (VA) infrastructure, rooted on techniques in machine learning and logic-based deductive reasoning that will assist analysts to make sense of large, complex data sets by facilitating the generation and validation of models representing relationships in the data. We use logic programming (LP) as the underlying computing machinery to encode the relations as rules and facts and compute with them. A unique aspect of our approach is that the LP rules are automatically learned, using Inductive Logic Programming, from examples of data that the analyst deems interesting when viewing the data in the high-dimensional visualization interface. Using this system, analysts will be able to construct models of arbitrary relationships in the data, explore the data for scenarios that fit the model, refine the model if necessary, and query the model to automatically analyze incoming (future) data exhibiting the encoded relationships. In other words it will support both model-driven data exploration, as well as data-driven model evolution. More importantly, by basing the construction of models on techniques from machine learning and logic-based deduction, the VA process will be both flexible in terms of modeling arbitrary, user-driven relationships in the data as well as readily scale across different data domains.},
	booktitle = {2008 {IEEE} {Symposium} on {Visual} {Analytics} {Science} and {Technology}},
	author = {Garg, S and Nam, J E and Ramakrishnan, I V and Mueller, K},
	month = oct,
	year = {2008},
	keywords = {cluster:Visual Optimizations, data visualisation, inductive logic programming, inf, layer:User Interaction, supercluster:Data Visualization, type:Validation Research},
	pages = {19--26},
}

@article{Alkowaileet:2016:LCA:3007263.3007315,
	title = {Large-scale {Complex} {Analytics} on {Semi}-structured {Datasets} using {AsterixDB} and {Spark}},
	volume = {9},
	issn = {21508097},
	url = {http://dx.doi.org/10.14778/3007263.3007315},
	doi = {10.14778/3007263.3007315},
	abstract = {Large quantities of raw data are being generated by many different sources in different formats. Private and public sectors alike ac-claim the valuable information and insights that can be mined from such data to better understand the dynamics of everyday life, such as traffic, worldwide logistics, and social behavior. For this reason, storing, managing, and analyzing " Big Data " at scale is getting a tremendous amount of attention, both in academia and industry. In this paper, we demonstrate the power of a parallel connection that we have built between Apache Spark and Apache AsterixDB (Incubating) to enable complex analytics such as machine learn-ing and graph analysis on data drawn from large semi-structured data collections. The integration of these two systems allows re-searchers and data scientists to leverage AsterixDB capabilities, including fast ingestion and indexing of semi-structured data and efficient answering of geo-spatial and fuzzy text queries. Complex data analytics can then be performed on the resulting AsterixDB query output in order to obtain additional insights by leveraging the power of Spark's machine learning and graph libraries.},
	number = {13},
	journal = {Proceedings of the VLDB Endowment},
	author = {Alkowaileet, Wail Y and Alsubaiee, Sattam and Carey, Michael J and Westmann, Till and Bu, Yingyi},
	month = sep,
	year = {2016},
	note = {Publisher: VLDB Endowment},
	pages = {1585--1588},
}

@inproceedings{Rundensteiner:2007:XQQ:1247480.1247623,
	address = {New York, NY, USA},
	title = {{XmdvtoolQ}:: {Quality}-aware {Interactive} {Data} {Exploration}},
	isbn = {978-1-59593-686-8},
	url = {http://doi.acm.org/10.1145/1247480.1247623},
	doi = {10.1145/1247480.1247623},
	abstract = {In this work, we describe our approach for making the interactive data exploration system, called XmdvTool, quality-aware to assure informed decision-making. XmdvToolQ, makes quality or lack thereof explicit for all stages of the data exploration process from raw data, to abstracted data, to the final visual displays, allowing users to query and navigate through data-, structure- and quality-spaces.},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Rundensteiner, Elke A and Ward, Matthew O and Xie, Zaixian and Cui, Qingguang and Wad, Charudatta V and Yang, Di and Huang, Shiping},
	year = {2007},
	note = {Series Title: SIGMOD '07},
	keywords = {cluster:Novel Query Interfaces, data quality, display quality, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1109--1112},
}

@inproceedings{Sioulas:2016:VSQ:2882903.2914829,
	address = {New York, NY, USA},
	title = {Vectorizing an {In} {Situ} {Query} {Engine}},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2914829},
	doi = {10.1145/2882903.2914829},
	abstract = {Database systems serve a wide range of use cases efficiently, but require data to be loaded and adapted to the system's execution engine. This pre-processing step is a bottleneck to the analysis of the increasingly large and heterogeneous datasets. Therefore, numerous research efforts advocate for querying each dataset in situ,i.e., without pre-loading it in a DBMS. On the other hand, performing analysis over raw data entails numerous overheads because of the potentially inefficient data representations. In this paper, we investigate the effect of vector processing on raw data querying. We enhance the operators of a query engine to use SIMD operations. Specifically, we examine the effect of SIMD on two different cases: the scan operators that perform the CPU-intensive task of input parsing, and the part of the query pipeline that performs a selection and computes an aggregate. We show that a vectorized approach has a lot of potential to improve performance, which nevertheless comes with trade-offs.},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Management} of {Data} - {SIGMOD} '16},
	publisher = {ACM},
	author = {Sioulas, Panagiotis and Ailamaki, Anastasia},
	year = {2016},
	note = {Series Title: SIGMOD '16
ISSN: 07308078},
	keywords = {JIT, in situ query engine, raw data, ★},
	pages = {2261--2262},
}

@inproceedings{Thiagarajan:2008:QCF:1376616.1376696,
	address = {New York, NY, USA},
	title = {Querying continuous functions in a database system},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?doid=1376616.1376696},
	doi = {10.1145/1376616.1376696},
	abstract = {Many scientific, financial, data mining and sensor network applications need to work with continuous, rather than discrete data e.g., temperature as a function of location, or stock prices or vehicle trajectories as a function of time. Querying raw or discrete data is unsatisfactory for these applications -- e.g., in a sensor network, it is necessary to interpolate sensor readings to predict values at locations where sensors are not deployed. In other situations, raw data can be inaccurate owing to measurement errors, and it is useful to fit continuous functions to raw data and query the functions, rather than raw data itself -- e.g., fitting a smooth curve to noisy sensor readings, or a smooth trajectory to GPS data containing gaps or outliers. Existing databases do not support storing or querying continuous functions, short of brute-force discretization of functions into a collection of tuples. We present FunctionDB, a novel database system that treats mathematical functions as first-class citizens that can be queried like traditional relations. The key contribution of FunctionDB is an efficient and accurate algebraic query processor - for the broad class of multi-variable polynomial functions, FunctionDB executes queries directly on the algebraic representation of functions without materializing them into discrete points, using symbolic operations: zero finding, variable substitution, and integration. Even when closed form solutions are intractable, FunctionDB leverages symbolic approximation operations to improve performance. We evaluate FunctionDB on real data sets from a temperature sensor network, and on traffic traces from Boston roads. We show that operating in the functional domain has substantial advantages in terms of accuracy (15-30\%) and up to order of magnitude (10x-100x) performance wins over existing approaches that represent models as discrete collections of points.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Thiagarajan, Arvind and Madden, Samuel},
	year = {2008},
	note = {Series Title: SIGMOD '08
ISSN: 07308078},
	keywords = {cluster:Novel Query Interfaces, erroneous data, functions, imprecise data, layer:User Interaction, model based views, regression, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {791},
}

@inproceedings{Lakshminarasimhan:2013:SSS:2493123.2465527,
	address = {New York, NY, USA},
	title = {Scalable in situ scientific data encoding for analytical query processing},
	isbn = {978-1-4503-1910-2},
	url = {http://doi.acm.org/10.1145/2462902.2465527},
	doi = {10.1145/2462902.2465527},
	abstract = {The process of scientific data analysis in high-performance computing environments has been evolving along with the advancement of computing capabilities. With the onset of exascale computing, the increasing gap between compute performance and I/O bandwidth has rendered the traditional method of post-simulation processing a tedious process. Despite the challenges due to increased data production, there exists an opportunity to benefit from "cheap" computing power to perform query-driven exploration and visualization during simulation time. To accelerate such analyses, applications traditionally augment raw data with large indexes, post-simulation, which are then repeatedly utilized for data exploration. However, the generation of current state-of-the-art indexes involve a compute- and memory-intensive processing, thus rendering them inapplicable in an in situ context. In this paper we propose DIRAQ, a parallel in situ, in network data encoding and reorganization technique that enables the transformation of simulation output into a query-efficient form, with negligible runtime overhead to the simulation run. DIRAQ begins with an effective core-local, precision-based encoding approach, which incorporates an embedded compressed index that is 3 - 6x smaller than current state-of-the-art indexing schemes. DIRAQ then applies an in network index merging strategy, enabling the creation of aggregated indexes ideally suited for spatial-context querying that speed up query responses by up to 10x versus alternative techniques. We also employ a novel aggregation strategy that is topology-, data-, and memory-aware, resulting in efficient I/O and yielding overall end-to-end encoding and I/O time that is less than that required to write the raw data with MPI collective I/O. © 2013 ACM.},
	booktitle = {{HPDC} '13 {Proceedings} of the 22nd international symposium on {High}-performance parallel and distributed computing},
	publisher = {ACM},
	author = {Lakshminarasimhan, Sriram and Boyuka, II, David A. and Pendse, Saurabh V. and Zou, Xiaocheng and Jenkins, John and Vishwanath, Venkatram and Papka, Michael E. and Samatova, Nagiza F.},
	year = {2013},
	note = {Series Title: HPDC '13},
	keywords = {exascale computing, indexing, query processing},
	pages = {1--12},
}

@inproceedings{Tian2014,
	address = {New York, NY, USA},
	title = {{DiNoDB}: {Efficient} {Large}-{Scale} {Raw} {Data} {Analytics}},
	isbn = {978-1-4503-3186-9},
	url = {http://dl.acm.org/citation.cfm?doid=2658840.2658841},
	doi = {10.1145/2658840.2658841},
	abstract = {Modern big data workflows, found in e.g., machine learning use cases, often involve iterations of cycles of batch analytics and interactive analytics on temporary data. Whereas batch analytics solutions for large volumes of raw data are well established (e.g., Hadoop, MapReduce), state-of-the-art interactive analytics solutions (e.g., distributed shared nothing RDBMSs) require data loading and/or transformation phase, which is inherently expensive for temporary data. In this paper, we propose a novel scalable distributed solution for in-situ data analytics, that offers both scalable batch and interactive data analytics on raw data, hence avoiding the loading phase bottleneck of RDBMSs. Our system combines a MapReduce based platform with the recently proposed NoDB paradigm, which optimizes traditional centralized RDBMSs for in-situ queries of raw files. We revisit the NoDB's centralized design and scale it out supporting multiple clients and data processing nodes to produce a new distributed data analytics system we call Distributed NoDB (DiNoDB). DiNoDB leverages MapReduce batch queries to produce critical pieces of metadata (e.g., distributed positional maps and vertical indices) to speed up interactive queries without the overheads of the data loading and data movement phases allowing users to quickly and efficiently exploit their data. Our experimental analysis demonstrates that DiNoDB significantly reduces the data-to-query latency with respect to comparable state-of-the-art distributed query engines, like Shark, Hive and HadoopDB.},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {Bringing} the {Value} of "{Big} {Data}" to {Users} ({Data4U} 2014)},
	publisher = {ACM},
	author = {Tian, Yongchao and Alagiannis, Ioannis and Liarou, Erietta and Ailamaki, Anastasia and Michiardi, Pietro and Vukolić, Marko},
	year = {2014},
	note = {Series Title: Data4U '14},
	keywords = {Distributed database, In situ query, RDistributed: Yes, RRaw: Yes, cluster:Adaptive Indexing, layer:Database Layer, positional map file, supercluster:Data Storage, type:Proposal of Solution, ★},
	pages = {1--6},
}

@article{Silva2017,
	title = {Raw data queries during data-intensive parallel workflow execution},
	issn = {0167739X},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X17300237},
	doi = {10.1016/j.future.2017.01.016},
	abstract = {Computer simulations consume and produce huge amounts of raw data files presented in different formats, e.g., HDF5 in computational fluid dynamics simulations. Users often need to analyze domain-specific data based on related data elements from multiple files during the execution of computer simulations. In a raw data analysis, one should identify regions of interest in the data space and retrieve the content of specific related raw data files. Existing solutions, such as FastBit and RAW, are limited to a single raw data file analysis and can only be used after the execution of computer simulations. Scientific Workflow Management Systems (SWMS) can manage the dataflow of computer simulations and register related raw data files at a provenance database. This paper aims to combine the advantages of a dataflow-aware SWMS and the raw data file analysis techniques to allow for queries on raw data file elements that are related, but reside in separate files. We propose a component-based architecture, named as ARMFUL (Analysis of Raw data from Multiple Files) with raw data extraction and indexing techniques, which allows for a direct access to specific elements or regions of raw data space. ARMFUL innovates by using a SWMS provenance database to add a dataflow access path to raw data files. ARMFUL facilitates the invocation of ad-hoc programs and third party tools (e.g., FastBit tool) for raw data analyses. In our experiments, a real parallel computational fluid dynamics is executed, exploring different alternatives of raw data extraction, indexing and analysis.},
	journal = {Future Generation Computer Systems},
	author = {Silva, Vítor and Leite, José and Camata, José J. and de Oliveira, Daniel and Coutinho, Alvaro L G A and Valduriez, Patrick and Mattoso, Marta},
	year = {2016},
	keywords = {Dataflow, Index raw data, RDistributed: Yes, RRaw: Yes, Raw data analysis, Scientific Computing, cluster:Adaptive Indexing, layer:Database Layer, supercluster:Indexes, type:Proposal of Solution, ★},
}

@article{Aoyama20071,
	title = {{TimeLine} and visualization of multiple-data sets and the visualization querying challenge},
	volume = {18},
	issn = {1045926X},
	url = {http://www.sciencedirect.com/science/article/pii/S1045926X06000395},
	doi = {10.1016/j.jvlc.2005.11.002},
	abstract = {Data in its raw form can potentially contain valuable information, but much of that value is lost if it cannot be presented to a user in a way that is useful and meaningful. Data visualization techniques offer a solution to this issue. Such methods are especially useful in spatial data domains such as medical scan data and geophysical data. However, to properly see trends in data or to relate data from multiple sources, multiple-data set visualization techniques must be used. In research with the time-line paradigm, we have integrated multiple streaming data sources into a single visual interface. Data visualization takes place on several levels, from the visualization of query results in a time-line fashion to using multiple visualization techniques to view, analyze, and compare the data from the results. A significant contribution of this research effort is the extension and combination of existing research efforts into the visualization of multiple-data sets to create new and more flexible techniques. We specifically address visualization issues regarding clarity, speed, and interactivity. The developed visualization tools have also led recently to the visualization querying paradigm and challenge highlighted herein. ?? 2006 Elsevier Ltd. All rights reserved.},
	number = {1},
	journal = {Journal of Visual Languages and Computing},
	author = {Aoyama, David A. and Hsiao, J. T T and Cárdenas, Alfonso F. and Pon, Raymond K.},
	year = {2007},
	keywords = {Data visualization, Multi-platform visualization, Remote processing, Visualization, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {1--21},
}

@inproceedings{Anagnostou2017,
	address = {New York, NY, USA},
	title = {Alpine: {Efficient} {In}-{Situ} {Data} {Exploration} in the {Presence} of {Updates}},
	isbn = {978-1-4503-4197-4},
	url = {http://dl.acm.org/citation.cfm?doid=3035918.3058743},
	doi = {10.1145/3035918.3058743},
	abstract = {The ever growing data collections create the need for brief explorations of the available data to extract relevant information before decision making becomes necessary. In this context of data exploration, current data analysis solutions struggle to quickly pinpoint useful information in data collections. One major reason is that loading data in a DBMS without knowing which part of it will actually be useful is a major bottleneck. To remove this bottleneck, state-of-the art approaches perform queries in situ, thus avoiding the loading overhead. In situ query engines, however, are index-oblivious, and lack sophisticated techniques to reduce the amount of data to be accessed. Furthermore, applications constantly generate fresh data and update the existing raw data files whereas state-of-the art in situ approaches support only append-like workloads. In this demonstration, we showcase the efficiency of adaptive indexing and partitioning techniques for analytical queries in the presence of updates. We demonstrate an online partitioning and indexing tuner for in situ querying which plugs to a query engine and offers support for fast queries over raw data files. We present Alpine, our prototype implementation, which combines the tuner with a query executor incorporating in situ query techniques to provide efficient raw data access. We will visually demonstrate how Alpine incrementally and adaptively builds auxiliary data structures and indexes over raw data files and how it adapts its behavior as a side-effect of updates in the raw data files.},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Anagnostou, Antonios and Olma, Matthaios and Ailamaki, Anastasia},
	year = {2017},
	note = {Series Title: SIGMOD '17},
	keywords = {RRaw: Yes, cluster:Adaptive Indexing, in-situ querying, indexes, layer:Database Layer, partitions, supercluster:Indexes, type:Proposal of Solution, ★},
	pages = {1651--1654},
}

@article{Qian201276,
	title = {A traceability system incorporating {2D} barcode and \{{RFID}\} technology for wheat flour mills},
	volume = {89},
	issn = {0168-1699},
	url = {http://www.sciencedirect.com/science/article/pii/S0168169912002050},
	doi = {https://doi.org/10.1016/j.compag.2012.08.004},
	abstract = {Wheat flour undergoes several processing steps in its transformation from raw wheat in the mill, which differentiates wheat flour from other farm products. At each step, various wheat sources are combined into one batch of wheat flour. This study primarily aimed to develop a Wheat Flour Milling Traceability System (WFMTS), incorporating 2D barcode and radio frequency identification (RFID) technology, and to validate the system in a wheat flour mill in China. We designed the encoding rules for the raw material, processing and traceability batches. Labels with a Quick Response Code (QR Code) were attached to small packages of wheat flour to link them to their processing information, and \{RFID\} tags were affixed to the storage bins to record logistics information. A traceability system was developed based on batch identification and record keeping. The system was applied and supported in a wheat flour mill for one year. The \{WFMTS\} management and traceability capacity was evaluated using a contrast experiment. The experiment was divided into five parts, including raw material data recording, processing data recording, package data recording, logistics data recording and traceability query. The results show that although time consumption using \{WFMTS\} in package data recording was more than that with paper recording, \{WFMTS\} was dominant in its total time consumption: five parts were reduced by 113\%, and the mean accuracy of the five parts increased by 8\%. The \{QR\} Code and \{RFID\} recognition accuracy was evaluated using experiments with different reading distances. The cost and income variations in application \{WFMTS\} were analyzed based on the survey. The results show that the total cost increased by 17.2\% to apply the system. Compared to the cost, the sales income increase was obvious, and it reached 32.5\%. Considering the good evaluation results, the system has good application potential in medium or large wheat mill enterprises.},
	journal = {Computers and Electronics in Agriculture},
	author = {Qian, Jian-Ping and Yang, Xin-Ting and Wu, Xiao-Ming and Zhao, Li and Fan, Bei-Lei and Xing, Bin},
	year = {2012},
	keywords = {RFID, Traceability},
	pages = {76--85},
}

@article{Combi201275,
	title = {Visually defining and querying consistent multi-granular clinical temporal abstractions},
	volume = {54},
	issn = {09333657},
	url = {http://www.sciencedirect.com/science/article/pii/S0933365711001424},
	doi = {10.1016/j.artmed.2011.10.004},
	abstract = {Objective: The main goal of this work is to propose a framework for the visual specification and query of consistent multi-granular clinical temporal abstractions. We focus on the issue of querying patient clinical information by visually defining and composing temporal abstractions, i.e., high level patterns derived from several time-stamped raw data. In particular, we focus on the visual specification of consistent temporal abstractions with different granularities and on the visual composition of different temporal abstractions for querying clinical databases. Background: Temporal abstractions on clinical data provide a concise and high-level description of temporal raw data, and a suitable way to support decision making. Granularities define partitions on the time line and allow one to represent time and, thus, temporal clinical information at different levels of detail, according to the requirements coming from the represented clinical domain. The visual representation of temporal information has been considered since several years in clinical domains. Proposed visualization techniques must be easy and quick to understand, and could benefit from visual metaphors that do not lead to ambiguous interpretations. Recently, physical metaphors such as strips, springs, weights, and wires have been proposed and evaluated on clinical users for the specification of temporal clinical abstractions. Visual approaches to boolean queries have been considered in the last years and confirmed that the visual support to the specification of complex boolean queries is both an important and difficult research topic. Methodology: We propose and describe a visual language for the definition of temporal abstractions based on a set of intuitive metaphors (striped wall, plastered wall, brick wall), allowing the clinician to use different granularities. A new algorithm, underlying the visual language, allows the physician to specify only consistent abstractions, i.e., abstractions not containing contradictory conditions on the component abstractions. Moreover, we propose a visual query language where different temporal abstractions can be composed to build complex queries: temporal abstractions are visually connected through the usual logical connectives AND, OR, and NOT. Results: The proposed visual language allows one to simply define temporal abstractions by using intuitive metaphors, and to specify temporal intervals related to abstractions by using different temporal granularities. The physician can interact with the designed and implemented tool by point-and-click selections, and can visually compose queries involving several temporal abstractions. The evaluation of the proposed granularity-related metaphors consisted in two parts: (i) solving 30 interpretation exercises by choosing the correct interpretation of a given screenshot representing a possible scenario, and (ii) solving a complex exercise, by visually specifying through the interface a scenario described only in natural language. The exercises were done by 13 subjects. The percentage of correct answers to the interpretation exercises were slightly different with respect to the considered metaphors (54.4 - striped wall, 73.3 - plastered wall, 61 - brick wall, and 61 - no wall), but post hoc statistical analysis on means confirmed that differences were not statistically significant. The result of the user's satisfaction questionnaire related to the evaluation of the proposed granularity-related metaphors ratified that there are no preferences for one of them. The evaluation of the proposed logical notation consisted in two parts: (i) solving five interpretation exercises provided by a screenshot representing a possible scenario and by three different possible interpretations, of which only one was correct, and (ii) solving five exercises, by visually defining through the interface a scenario described only in natural language. Exercises had an increasing difficulty. The evaluation involved a total of 31 subjects. Results related to this evaluation phase confirmed us about the soundness of the proposed solution even in comparison with a well known proposal based on a tabular query form (the only significant difference is that our proposal requires more time for the training phase: 21. min versus 14. min). Discussion and conclusions: In this work we have considered the issue of visually composing and querying temporal clinical patient data. In this context we have proposed a visual framework for the specification of consistent temporal abstractions with different granularities and for the visual composition of different temporal abstractions to build (possibly) complex queries on clinical databases. A new algorithm has been proposed to check the consistency of the specified granular abstraction. From the evaluation of the proposed metaphors and interfaces and from the comparison of the visual query language with a well known visual method for boolean queries, the soundness of the overall system has been confirmed; moreover, pros and cons and possible improvements emerged from the comparison of different visual metaphors and solutions. ?? 2011 Elsevier B.V.},
	number = {2},
	journal = {Artificial Intelligence in Medicine},
	author = {Combi, Carlo and Oliboni, Barbara},
	year = {2012},
	pmid = {22177662},
	note = {ISBN: 0933-3657},
	keywords = {Hemodialysis data, Metaphors, Temporal abstractions, Temporal clinical data, Temporal granularities, UserStudy:yes, Visualization, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {75--101},
}

@article{Afrati2008462,
	title = {Adaptive-sampling algorithms for answering aggregation queries on {Web} sites},
	volume = {64},
	issn = {0169-023X},
	url = {http://www.sciencedirect.com/science/article/pii/S0169023X07001814},
	doi = {https://doi.org/10.1016/j.datak.2007.09.014},
	abstract = {Many Web sites publish their data in a hierarchical structure. For instance, Amazon.com organizes its pages on books as a hierarchy, in which each leaf node corresponds to a collection of pages of books in the same class (e.g., books on Data Mining). Users can easily browse this class by following a path from the root to the corresponding leaf node, such as “Computers \&amp; Internet – Databases – Storage – Data Mining”. Business applications often require to submit aggregation queries on such data, such as “finding the average price of books on Data Mining”. On the other hand, it is computationally expensive to compute the exact answer to such a query due to the large amount of data, its dynamicity, and limited Web-access resources. In this paper, we study how to answer such aggregation queries approximately with quality guarantees using sampling. We study how to use adaptive-sampling techniques that allocate the resources adaptively based on partial samples retrieved from different nodes in the hierarchy. Based on statistical methods, we study how to estimate the quality of the answer using the sample. Our experimental study using real and synthetic data sets validates the proposed techniques.},
	number = {2},
	journal = {Data \& Knowledge Engineering},
	author = {Afrati, Foto N and Lekeas, Paraskevas V and Li, Chen},
	year = {2008},
	keywords = {cluster:Sampling, layer:Database Layer, supercluster:Data Storage, type:Validation Research},
	pages = {462--490},
}

@article{Günther1990313,
	title = {The arc tree: {An} approximation scheme to represent arbitrary curved shapes},
	volume = {51},
	issn = {0734-189X},
	url = {http://www.sciencedirect.com/science/article/pii/0734189X9090006H},
	doi = {https://doi.org/10.1016/0734-189X(90)90006-H},
	abstract = {This paper introduces the arc tree, a hierarchical data structure to represent arbitrary curved shapes. The arc tree is a balanced binary tree that represents a curve of length l such that any subtree whose root is on the kth tree level is representing a subcurve of length l2k. Each tree level is associated with an approximation of the curve; lower levels correspond to approximations of higher resolution. Based on this hierarchy of detail, queries such as point search or intersection detection and computation can be solved in a hierarchical manner. Algorithms start out near the root of the tree and try to solve the queries at a very coarse resolution. If that is not possible, the resolution is increased where necessary. We describe and analyze several such algorithms to compute a variety of set and search operators. Various related approximation schemes to represent curved shapes are also discussed.},
	number = {3},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Günther, Oliver and Wong, Eugene},
	year = {1990},
	pages = {313--337},
}

@article{Madria2007131,
	title = {Efficient processing of {XPath} queries using indexes},
	volume = {32},
	issn = {0306-4379},
	url = {http://www.sciencedirect.com/science/article/pii/S0306437905000669},
	doi = {https://doi.org/10.1016/j.is.2005.06.003},
	abstract = {A number of indexing techniques have been proposed in recent times for optimizing the queries on \{XML\} and other semi-structured data models. Most of the semi-structured models use tree-like structures and query languages (XPath, XQuery, etc.) which make use of regular path expressions to optimize the query processing. In this paper, we propose two algorithms called Entry-point algorithm (EPA) and Two-point Entry algorithms that exploit different types of indices to efficiently process \{XPath\} queries. We discuss and compare two approaches namely, Root-first and Bottom-first in implementing the EPA. We present the experimental results of the algorithms using \{XML\} benchmark queries and data and compare the results with that of traditional methods of query processing with and without the use of indexes, and ToXin indexing approach. Our algorithms show improved performance results than the traditional methods and Toxin indexing approach.},
	number = {1},
	journal = {Information Systems},
	author = {Madria, Sanjay and Chen, Yan and Passi, Kalpdrum and Bhowmick, Sourav},
	year = {2007},
	keywords = {Indexing, Query processing},
	pages = {131--159},
}

@inproceedings{Cheng:2014:PID:2588555.2593673,
	address = {New York, NY, USA},
	title = {Parallel in-situ data processing with speculative loading},
	isbn = {978-1-4503-2376-5},
	url = {http://dl.acm.org/citation.cfm?doid=2588555.2593673},
	doi = {10.1145/2588555.2593673},
	abstract = {Traditional databases incur a significant data-to-query delay due to the requirement to load data inside the system before querying. Since this is not acceptable in many domains generating massive amounts of raw data, e.g., genomics, databases are entirely discarded. External tables, on the other hand, provide instant SQL querying over raw files. Their performance across a query workload is limited though by the speed of repeated full scans, tokenizing, and parsing of the entire file. In this paper, we propose SCANRAW, a novel database physical operator for in-situ processing over raw files that integrates data loading and external tables seamlessly while preserving their advantages: optimal performance across a query workload and zero time-to-query. Our major contribution is a parallel super-scalar pipeline implementation that allows SCANRAW to take advantage of the current many- and multi-core processors by overlapping the execution of independent stages. Moreover, SCANRAW overlaps query processing with loading by speculatively using the additional I/O bandwidth arising during the conversion process for storing data into the database such that subsequent queries execute faster. As a result, SCANRAW makes optimal use of the available system resources -- CPU cycles and I/O bandwidth -- by switching dynamically between tasks to ensure that optimal performance is achieved. We implement SCANRAW in a state-of-the-art database system and evaluate its performance across a variety of synthetic and real-world datasets. Our results show that SCANRAW with speculative loading achieves optimal performance for a query sequence at any point in the processing. Moreover, SCANRAW maximizes resource utilization for the entire workload execution while speculatively loading data and without interfering with normal query processing.},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} international conference on {Management} of data - {SIGMOD} '14},
	publisher = {ACM},
	author = {Cheng, Yu and Rusu, Florin},
	year = {2014},
	note = {Series Title: SIGMOD '14
ISSN: 07308078},
	keywords = {data loading, external tables, raw file processing},
	pages = {1287--1298},
}

@article{Jugel:2016:VAV:2884416.2884424,
	title = {{VDDA}: automatic visualization-driven data aggregation in relational databases},
	volume = {25},
	issn = {0949877X},
	url = {http://dx.doi.org/10.1007/s00778-015-0396-z},
	doi = {10.1007/s00778-015-0396-z},
	abstract = {Contemporary RDBMS-based systems for visualization of high-volume numerical data have difficulty to cope with the hard latency requirements and high ingestion rates of interactive visualizations. Existing solutions for lowering the volume of large data sets disregard the spatial properties of visualizations, resulting in visualization errors. In this work, we introduce VDDA, a visualization-driven data aggregation that models visual aggregation at the pixel level as data aggregation at the query level. Based on the M4 aggregation for producing pixel-perfect line charts from highly reduced data subsets, we define a complete set of data reduction operators that simulate the overplotting behavior of the most frequently used chart types. Relying only on the relational algebra and the common data aggregation functions, our approach is generic and applicable to any visualization system that consumes data stored in relational databases. We demonstrate our visualization-driven data aggregation using real-world data sets from high-tech manufacturing, stock markets, and sports analytics, reducing data volumes by up to two orders of magnitude, while preserving pixel-perfect visualizations, as producible from the raw data.},
	number = {1},
	journal = {The VLDB Journal},
	author = {Jugel, Uwe and Jerzak, Zbigniew and Hackenbroich, Gregor and Markl, Volker},
	month = feb,
	year = {2016},
	note = {Publisher: Springer-Verlag New York, Inc.
Place: Secaucus, NJ, USA},
	keywords = {Data aggregation, Data visualization, Dimensionality reduction, Line rasterization, Overplotting, Relational databases, Visualization, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Proposal of Solution},
	pages = {53--77},
}

@article{Cheng2015,
	title = {{SCANRAW}: {A} {Database} {Meta}-{Operator} for {Parallel} {In}-{Situ} {Processing} and {Loading}},
	volume = {40},
	issn = {03625915},
	url = {http://dl.acm.org/citation.cfm?doid=2838914.2818181},
	doi = {10.1145/2818181},
	abstract = {Traditional databases incur a significant data-to-query delay due to the requirement to load data inside the system before querying. Since this is not acceptable in many domains generating massive amounts of raw data (e.g., genomics), databases are entirely discarded. External tables, on the other hand, provide instant SQL querying over raw files. Their performance across a query workload is limited though by the speed of repeated full scans, tokenizing, and parsing of the entire file. In this article, we propose SCANRAW, a novel database meta-operator for in-situ processing over raw files that integrates data loading and external tables seamlessly, while preserving their advantages: optimal performance across a query workload and zero time-to-query. We decompose loading and external table processing into atomic stages in order to identify common functionality. We analyze alternative implementations and discuss possible optimizations for each stage. Our major contribution is a parallel superscalar pipeline implementation that allows SCANRAW to take advantage of the current many- and multicore processors by overlapping the execution of independent stages. Moreover, SCANRAW overlaps query processing with loading by speculatively using the additional I/O bandwidth arising during the conversion process for storing data into the database, such that subsequent queries execute faster. As a result, SCANRAW makes intelligent use of the available system resources—CPU cycles and I/O bandwidth—by switching dynamically between tasks to ensure that optimal performance is achieved. We implement SCANRAW in a state-of-the-art database system and evaluate its performance across a variety of synthetic and real-world datasets. Our results show that SCANRAW with speculative loading achieves the best-possible performance for a query sequence at any point in the processing. Moreover, SCANRAW maximizes resource utilization for the entire workload execution while speculatively loading data and without interfering with normal query processing.},
	number = {3},
	journal = {ACM Transactions on Database Systems},
	author = {Cheng, Yu and Rusu, Florin},
	month = oct,
	year = {2015},
	note = {Publisher: ACM
Place: New York, NY, USA},
	keywords = {Data access operator, RRaw: Yes, cluster:Adaptive Loading, data loading, database operator, external table, layer:Database Layer, supercluster:Data Storage, type:Proposal of Solution, ★},
	pages = {1--45},
}

@misc{Introduction2010,
	title = {Introduction to {Core} {Audio}},
	url = {http://link.springer.com/chapter/10.1007/978-1-4302-2600-0_9},
	year = {2010},
	doi = {10.1007/978-1-4302-2600-0_9},
	note = {Publication Title: Beginning iPhone Games Development},
}

@article{SPE:SPE944,
	title = {{XML} data mining},
	volume = {40},
	issn = {00380644},
	url = {http://dx.doi.org/10.1002/spe.944},
	doi = {10.1002/spe.944},
	abstract = {With the spreading of XML sources, mining XML data can be an important objective in the near future. This paper presents a project focussed on designing a general-purpose query language in support of mining XML data. In our framework, raw data, mining models and domain knowledge are represented by way of XML documents and stored inside native XML databases. Data mining (DM) tasks are expressed in an extension of XQuery. Special attention is given to the frequent pattern discovery problem, and a way of exploiting domain-dependent optimizations and efficient data structures as deeper as possible in the extraction process is presented. We report the results of a first bunch of experiments, showing that a good trade-off between expressiveness and efficiency in XML DM is not a chimera. Copyright © 2009 John Wiley \& Sons, Ltd.},
	number = {2},
	journal = {Software - Practice and Experience},
	author = {Romei, Andrea and Turini, Franco},
	year = {2010},
	note = {Publisher: John Wiley \& Sons, Ltd.
ISBN: 9781613503560},
	keywords = {Data mining, Inductive database, Knowledge discovery, Query language},
	pages = {101--130},
}

@article{doi:10.1142/S0218843015500021,
	title = {A {Distributed} {Infrastructure} for {Earth}-{Science} {Big} {Data} {Retrieval}},
	volume = {24},
	issn = {0218-8430},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218843015500021},
	doi = {10.1142/S0218843015500021},
	abstract = {Earth-Science data are composite, multi-dimensional and of significant size, and as such, continue to pose a number of ongoing problems regarding their management. With new and diverse information sources emerging as well as rates of generated data continuously increasing, a persistent challenge becomes more pressing: To make the information existing in multiple heterogeneous resources readily available. The widespread use of the XML data-exchange format has enabled the rapid accumulation of semi-structured metadata for Earth-Science data. In this paper, we exploit this popular use of XML and present the means for querying metadata emanating from multiple sources in a succinct and effective way. Thereby, we release the user from the very tedious and time consuming task of examining individual XML descriptions one by one. Our approach, termed Meta-Array Data Search (MAD Search), brings together diverse data sources while enhancing the user-friendliness of the underlying information sources. We gather metad...},
	number = {02},
	journal = {International Journal of Cooperative Information Systems},
	author = {Liakos, Panagiotis and Koltsida, Panagiota and Kakaletris, George and Baumann, Peter and Ioannidis, Yannis and Delis, Alex},
	year = {2015},
	keywords = {RDistributed: Yes, cluster:Novel Query Interfaces, layer:User Interaction, supercluster:Exploration Interfaces, type:Proposal of Solution},
	pages = {1550002},
}

@article{doi:10.1142/S1793351X16400110,
	title = {Visual reasoning indexing and retrieval using in-memory computing},
	volume = {10},
	issn = {1793-351X},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S1793351X16400110},
	doi = {10.1109/BigMM.2016.83},
	abstract = {Research has shown that visual information of multimedia is critical in highly-skilled application, such as biomedicine and life sciences, and a certain visual reasoning process is essential for meaningful search in a timely manner. Relevant image characteristics are learned and verified with accumulated experiences during the reasoning processes. However, such type of process is highly dynamic and elusive to computationally quantify and therefore challenging to analyze, let alone to make the knowledge shareable across users. In this paper we study real-time human visual reasoning processes with the aid of gaze tracking devices. Temporal and spatial representations are proposed for gaze modeling, and a visual reasoning retrieval system utilizing in-memory computing such as Apache Spark is designed for real-time search. Simulated data derived from human subject experiments show that the system has a reasonably high accuracy and provides predictive estimations for hardware requirements versus data sizes for exhaustive searches.},
	number = {03},
	journal = {International Journal of Semantic Computing},
	author = {Cao, Hongfei and Li, Yu and Allen, Carla M. and Phinney, Michael A. and Shyu, Chi Ren},
	year = {2016},
	note = {ISBN: 9781509021789},
	keywords = {Graphic-based comparison, In-memory computing, UserStudy:yes, Visualization, cluster:Visual Optimizations, layer:User Interaction, supercluster:Data Visualization, type:Evaluation Research},
	pages = {17--24},
}

@article{doi:10.1142/S0218488512500328,
	title = {Probabilistic {Querying} {Over} {Uncertain} {Data} {Streams}},
	volume = {20},
	issn = {0218-4885},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218488512500328},
	doi = {10.1142/S0218488512500328},
	abstract = {Inherent imprecision of data in many applications motivates us to support uncertainty as a first-class concept. Data stream and probabilistic data have been recently considered noticeably in isolation. However, there are many applications including sensor data management systems and object monitoring systems which need both issues in tandem. Our main contribution is designing a probabilistic data stream management system, called Sarcheshmeh, for continuous querying over probabilistic data streams. Sarcheshmeh supports uncertainty from input data to final query results. In this paper, after reviewing requirements and applications of probabilistic data streams, we present our new data model for probabilistic data streams and define our main logical operators formally. Then, we present query language and physical operators. In addition, we introduce the architecture of Sarcheshmeh and also describe some major challenges like memory management and our floating precision mechanism toward designing a more robust system. Finally, we report evaluation of our system and the effect of floating precision on the tradeoff between accuracy and efficiency. Read More: http://www.worldscientific.com/doi/abs/10.1142/S0218488512500328},
	number = {05},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {DEZFULI, MOHAMMAD G. and HAGHJOO, MOSTAFA S.},
	year = {2012},
	keywords = {Probabilistic data stream, probabilistic queries},
	pages = {701--728},
}

@article{doi:10.1142/S0218843015500033,
	title = {Evaluating a {Stream} of {Relational} {K} {NN} {Queries} by a {Knowledge} {Base}},
	volume = {24},
	issn = {0218-8430},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218843015500033},
	doi = {10.1142/S0218843015500033},
	abstract = {In relational databases and their applications, there are opportunities for evaluating a stream of KNN queries submitted one by one at different times. For this issue, we propose a new method with learning-based techniques, region clustering methods and caching mechanisms. This method uses a knowledge base to store related information of some past KNN queries, groups the search regions of the past queries into larger regions, and retrieves the tuples from the larger regions. To answer a newly submitted query, our strategy tries to obtain a majority or all of the results from the previously retrieved tuples cached in main memory. Thus, this method seeks to minimize the response time by reducing the search region or avoiding the accesses to the underlying database. Meanwhile, our method remains effective for high-dimensional data. Extensive experiments are carried out to measure the performance of this new strategy and the results indicate that it is significantly better than the state-of-the-art naïve methods of evaluating a stream of KNN queries for both low-dimensional (2, 3 and 4) and high-dimensional (25, 50 and 104) data. Read More: http://www.worldscientific.com/doi/abs/10.1142/S0218843015500033},
	number = {02},
	journal = {International Journal of Cooperative Information Systems},
	author = {Zhu, Liang and Song, Xin and Liu, Chunnian},
	year = {2015},
	keywords = {cluster:Data Prefetching, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {1550003},
}

@article{Sassi2010,
	title = {About {Database} {Summarization}},
	volume = {18},
	issn = {0218-4885},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218488510006453},
	doi = {10.1142/S0218488510006453},
	abstract = {The summarization system takes a Database (DB) table as input and produces a reduced version of this table through both a rewriting and a generalization process. The resulting table provides records with less precision than the original but it is very informative of the actual DB content. This reduced form can be used as input for advanced Data Mining processes. Several approaches of DB summarization have been proposed in the literature. The most recent is the SaintEtiQ summarization model, proposed initially by Raschia.1 Based on a hierarchical conceptual clustering algorithm, SaintEtiQ builds a summary hierarchy from DB records. In this paper, we propose to extend this DB summarization model by introducing some optimization processes including: (i) minimization of the expert risks domain, (iii) building of the summary hierarchy from DB records, and (iv) cooperation with the user by giving him summaries in different hierarchy levels. Read More: http://www.worldscientific.com/doi/abs/10.1142/S0218488510006453},
	number = {02},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Sassi, Minyar and TOUZI, AMEL GRISSA and OUNELLI, HABIB and AISSA, INES},
	year = {2010},
	keywords = {cluster:Query Approximation, layer:Middleware, supercluster:Interactive Performance Optimizations, type:Proposal of Solution},
	pages = {133--151},
}

@article{doi:10.1142/S0219622008003204,
	title = {A {Descriptive} {Framework} for the {Field} of {Data} {Mining} and {Knowledge} {Discovery}},
	volume = {07},
	issn = {0219-6220},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0219622008003204},
	doi = {10.1142/S0219622008003204},
	abstract = {Despite the rapid development, the field of data mining and knowledge discovery (DMKD) is still vaguely defined and lack of integrated descriptions. This situation causes difficulties in teaching, learning, research, and application. This paper surveys a large collection of DMKD literature to provide a comprehensive picture of current DMKD research and classify these research activities into high-level categories using grounded theory approach; it also evaluates the longitudinal changes of DMKD research activities during the last decade.},
	number = {04},
	journal = {International Journal of Information Technology \& Decision Making},
	author = {PENG, YI and KOU, GANG and SHI, YONG and CHEN, ZHENGXIN},
	year = {2008},
	pages = {639--682},
}

@article{Janyst2011,
	title = {Exabyte {Scale} {Storage} at {CERN}},
	volume = {331},
	issn = {1742-6596},
	url = {http://stacks.iop.org/1742-6596/331/i=5/a=052015},
	abstract = {The future of data management for LHC at CERN brings new requirements for scalability and a change
of scheduling and data handling compared to the HSM mass storage system in use today. A forecast for
disk based storage volume at CERN in 2015 is on the Exabyte scale with hundreds of millions of
files.

A new CERN storage architecture is represented as a storage cluster with an analysis, archive and
tape pool with container based data movements and decoupled namespaces.

Main assets of a new system is high-availability and life cycle management for large storage
installations. Today this is one of the major issues at the CERN computer centre with more than
1,000 disk servers and continuous hardware replacement. Another key point is distributed meta data
handling with in-memory caching and persistent key-value stores to reduce latencies and operational
complexity.

Focus of this paper will be on the analysis pool implementation providing low-latency, nonsequential
file access and a hierarchical namespace. A summary of performance indicators and first operational
experiences will be reported.},
	number = {5},
	journal = {Journal of Physics: Conference Series},
	author = {Janyst, Andreas J Peters {and} Lukasz},
	year = {2011},
	pages = {52015},
}

@article{Brun1997,
	title = {{ROOT} - {An} object oriented data analysis framework},
	volume = {389},
	issn = {01689002},
	doi = {10.1016/S0168-9002(97)00048-X},
	abstract = {The ROOT system in an Object Oriented framework for large scale data analysis. ROOT written in C++, contains, among others, an efficient hierarchical OO database, a C++ interpreter, advanced statistical analysis (multi-dimensional histogramming, fitting, minimization, cluster finding algorithms) and visualization tools. The user interacts with ROOT via a graphical user interface, the command line or batch scripts. The command and scripting language is C++ (using the interpreter) and large scripts can be compiled and dynamically linked in. The OO database design has been optimized for parallel access (reading as well as writing) by multiple processes.},
	number = {1-2},
	journal = {Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
	author = {Brun, Rene and Rademakers, Fons},
	year = {1997},
	note = {ISBN: 0168-9002},
	pages = {81--86},
}

@article{Kitchenham2007,
	title = {Guidelines for performing {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	volume = {2},
	abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
	journal = {Engineering},
	author = {Kitchenham, Barbara and Charters, S},
	year = {2007},
	pages = {1051},
}

@article{Stonebraker2009,
	title = {Requirements for {Science} {Data} {Bases} and {SciDB}},
	doi = {10.1.1.145.1567},
	abstract = {For the past year, we have been assembling requirements from a collection of scientific data base users from astronomy, particle physics, fusion, remote sensing, oceanography, and biology. The intent has been to specify a common set of requirements for a new science data base system, which we call SciDB. In addition, we have discovered that very complex business analytics share most of the same requirements as big science. We have also constructed a partnership of companies to fund the development of SciDB, including eBay, the Large Synoptic Survey Telescope (LSST), Microsoft, the Stanford Linear Accelerator Center (SLAC) and Vertica. Lastly, we have identified two lighthouse customers (LSST and eBay) who will run the initial system, once it is constructed. In this paper, we report on the requirements we have identified and briefly sketch some of the SciDB design.},
	journal = {4th Biennial Conference on Innovative Data Systems Research CIDR’09},
	author = {Stonebraker, Michael},
	year = {2009},
	pages = {173184},
}

@article{Karpathiotakis2015,
	title = {Just-{In}-{Time} {Data} {Virtualization}: {Lightweight} {Data} {Management} with {ViD}},
	abstract = {As the size of data and its heterogeneity increase, tradi-tional database system architecture becomes an obstacle to data analysis. Integrating and ingesting (loading) data into databases is quickly becoming a bottleneck in face of mas-sive data as well as increasingly heterogeneous data for-mats. Still, state-of-the-art approaches typically rely on copying and transforming data into one (or few) repositories. Queries, on the other hand, are often ad-hoc and supported by pre-cooked operators which are not adaptive enough to optimize access to data. As data formats and queries in-creasingly vary, there is a need to depart from the current status quo of static query processing primitives and build dynamic, fully adaptive architectures. We build ViDa, a system which reads data in its raw for-mat and processes queries using adaptive, just-in-time op-erators. Our key insight is use of virtualization, i.e., ab-stracting data and manipulating it regardless of its original format, and dynamic generation of operators. ViDa's query engine is generated just-in-time; its caches and its query op-erators adapt to the current query and the workload, while also treating raw datasets as its native storage structures. Finally, ViDa features a language expressive enough to sup-port heterogeneous data models, and to which existing lan-guages can be translated. Users therefore have the power to choose the language best suited for an analysis.},
	journal = {Proceedings of 7th Biennial Conference on Innovative Data Systems Research},
	author = {Karpathiotakis, Manos and Alagiannis, Ioannis and Heinis, Thomas and Branco, Miguel and Ailamaki, Anastasia},
	year = {2015},
}

@inproceedings{Palomo-Duarte2010,
	title = {Takuan: {A} tool for {WS}-{BPEL} composition testing using dynamic invariant generation},
	volume = {6189 LNCS},
	isbn = {3-642-13910-8},
	doi = {10.1007/978-3-642-13911-6_45},
	abstract = {WS-BPEL eases programming in the large by composing web services, but poses new challenges to classical white-box testing techniques. These have to be updated to take context into account and cope with its specific instructions for web service management. Takuan is an open-source system that dynamically generates invariants reflecting the internal logic of a WS-BPEL composition. After several improvements and the development of a graphical interface, we consider Takuan to be a mature tool that can help find both bugs in the WS-BPEL composition and missing test cases in the test suite.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {Palomo-Duarte, Manuel and García-Domínguez, Antonio and Medina-Bulo, Inmaculada and Alvarez-Ayllón, Alejandro and Santacruz, Javier},
	year = {2010},
	note = {ISSN: 03029743},
	keywords = {dynamic invariant generation},
	pages = {531--534},
}

@article{Furano2012,
	title = {Dynamic federations: {Storage} aggregation using open tools and protocols},
	volume = {396},
	issn = {17426588 17426596},
	doi = {10.1088/1742-6596/396/3/032042},
	abstract = {A number of storage elements now offer standard protocol interfaces like NFS 4.1/pNFS and WebDAV, for access to their data repositories, in line with the standardization effort of the European Middleware Initiative (EMI). Also the LCG FileCatalogue (LFC) can offer such features. Here we report on work that seeks to exploit the federation potential of these protocols and build a system that offers a unique view of the storage and metadata ensemble and the possibility of integration of other compatible resources such as those from cloud providers. The challenge, here undertaken by the providers of dCache and DPM, and pragmatically open to other Grid and Cloud storage solutions, is to build such a system while being able to accommodate name translations from existing catalogues (e.g. LFCs), experiment-based metadata catalogues, or stateless algorithmic name translations, also known as "trivial file catalogues". Such so-called storage federations of standard protocols-based storage elements give a unique view of their content, thus promoting simplicity in accessing the data they contain and offering new possibilities for resilience and data placement strategies. The goal is to consider HTTP and NFS4.1-based storage elements and metadata catalogues and make them able to cooperate through an architecture that properly feeds the redirection mechanisms that they are based upon, thus giving the functionalities of a "loosely coupled" storage federation. One of the key requirements is to use standard clients (provided by OS'es or open source distributions, e.g. Web browsers) to access an already aggregated system; this approach is quite different from aggregating the repositories at the client side through some wrapper API, like for instance GFAL, or by developing new custom clients. Other technical challenges that will determine the success of this initiative include performance, latency and scalability, and the ability to create worldwide storage federations that are able to redirect clients to repositories that they can efficiently access, for instance trying to choose the endpoints that are closer or applying other criteria. We believe that the features of a loosely coupled federation of open-protocols-based storage elements will open many possibilities of evolving the current computing models without disrupting them, and, at the same time, will be able to operate with the existing infrastructures, follow their evolution path and add storage centers that can be acquired as a third-party service. © Published under licence by IOP Publishing Ltd.},
	number = {PART 3},
	journal = {Journal of Physics: Conference Series},
	author = {Furano, F. and Brito Da Rocha, R. and Devresse, A. and Keeble, O. and Álvarez Ayllón, A. and Fuhrmann, P.},
	year = {2012},
}
